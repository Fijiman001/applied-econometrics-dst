{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63b56250-c732-445f-828e-5c7c41036a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58841afb-7da4-43e2-9d3c-73d6c7b8cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([\n",
    "    {'i': 1, 'f': 1.1, 's': 'abc'},\n",
    "    {'i': 1, 's': 'abc'},\n",
    "])\n",
    "df2 = pd.DataFrame([\n",
    "    {'i': 2, 'f': 1.1, 's': 'abcd'},\n",
    "    {'i': 2, 's': 'abcd'},\n",
    "])\n",
    "csv1 = df1.to_csv(index=False)\n",
    "csv2 = df2.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9b7dd70-905a-456e-8d8d-e2eb0d374cfc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pq\u001b[38;5;241m.\u001b[39mParquetFile(source_path) \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m f_in\u001b[38;5;241m.\u001b[39miter_batches(use_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m         \u001b[43mpq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_to_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDUID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lower memory\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/TSE/AppliedEconometrics/repo/venv/lib/python3.11/site-packages/pyarrow/parquet/core.py:3200\u001b[0m, in \u001b[0;36mwrite_to_dataset\u001b[0;34m(table, root_path, partition_cols, partition_filename_cb, filesystem, use_legacy_dataset, schema, partitioning, basename_template, use_threads, file_visitor, existing_data_behavior, **kwargs)\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   3197\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(path)\n\u001b[0;32m-> 3200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_to_dataset\u001b[39m(table, root_path, partition_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3201\u001b[0m                      partition_filename_cb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filesystem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3202\u001b[0m                      use_legacy_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3203\u001b[0m                      partitioning\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, basename_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3204\u001b[0m                      use_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, file_visitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3205\u001b[0m                      existing_data_behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3206\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around dataset.write_dataset (when use_legacy_dataset=False) or\u001b[39;00m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;124;03m    parquet.write_table (when use_legacy_dataset=True) for writing a Table to\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;124;03m    Parquet format by partitions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;124;03m    ['dataset_name_4/...-0.parquet']\u001b[39;00m\n\u001b[1;32m   3343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3344\u001b[0m     \u001b[38;5;66;03m# Choose the implementation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data'\n",
    "source_path = os.path.join(data_dir, 'debug/DISPATCHLOAD.parquet')\n",
    "dest_path = os.path.join(data_dir, 'debug/DISPATCHLOAD-pq/')\n",
    "with pq.ParquetFile(source_path) as f_in:\n",
    "    \n",
    "    for batch in f_in.iter_batches(use_threads = False):\n",
    "        pq.write_to_dataset(\n",
    "            table = batch,\n",
    "            root_path = dest_path,\n",
    "            partitioning = ['DUID'],\n",
    "            use_threads = False, # lower memory\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9220a-ce54-4586-b440-dbd760c4c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data'\n",
    "source_path = os.path.join(data_dir, 'debug/DISPATCHLOAD.parquet')\n",
    "dest_path = os.path.join(data_dir, 'debug/DISPATCHLOAD-pq-2/')\n",
    "with pq.ParquetFile(source_path) as f_in:\n",
    "    batches = f_in.iter_batches(use_threads = False)\n",
    "    table = pa.Table.from_batches(batches, schema=f_in.schema_arrow)\n",
    "    pq.write_to_dataset(\n",
    "        table = f_in,\n",
    "        root_path = dest_path,\n",
    "        partitioning = ['DUID'],\n",
    "        schema = f_in.schema_arrow,\n",
    "        use_threads = False, # lower memory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c09766-d28d-418b-b1a8-38ffd42954f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pyarrow.parquet.ParquetWriter(parquet_file, output_schema) as writer:\n",
    "        for csv_file in os.listdir(csv_folder):\n",
    "            try:\n",
    "                csv_path = os.path.join(csv_folder, csv_file)\n",
    "                csv_reader = pyarrow.csv.open_csv(csv_path, \n",
    "                                                  convert_options=pyarrow.csv.ConvertOptions(\n",
    "                                                      column_types=input_schema, \n",
    "                                                      include_missing_columns=True, \n",
    "                                                      include_columns=column_names))\n",
    "    \n",
    "                for batch in csv_reader:\n",
    "                    check_for_pause()\n",
    "                    columns = []\n",
    "                    if datetime_columns == []:\n",
    "                        # table has no datetime columns\n",
    "                        # just write straight into the parquet file\n",
    "                        writer.write(batch)\n",
    "                    else:\n",
    "                        # parse some columns from string to datetime\n",
    "                        \n",
    "                        for (i, column_name) in enumerate(column_names):\n",
    "                            assert isinstance(column_name, str), f\"column_name is type {type(column_name)}\"\n",
    "                            column_data = batch[column_name]\n",
    "                            if column_name in datetime_columns:\n",
    "                                # pyarrow assumes the timesstamp is UTC by default\n",
    "                                # the schema specifies the timezone as UTC+10 (which we want)\n",
    "                                # but it adds 10h when doing the conversion\n",
    "                                # so let's subtract 10h\n",
    "                                # This is checked with a unit test later\n",
    "                                column_data = pc.strptime(column_data, format=\"%Y/%m/%d %H:%M:%S\", unit=\"s\", error_is_null=True)\n",
    "                                column_data = pc.add(column_data, -dt.timedelta(hours=TIMEZONE_OFFSET))\n",
    "                            columns.append(column_data)\n",
    "                        \n",
    "                        updated_table = pyarrow.Table.from_arrays(\n",
    "                            columns, \n",
    "                            schema=output_schema\n",
    "                        )\n",
    "                        writer.write(updated_table)\n",
    "            except Exception:\n",
    "                # additional logging\n",
    "                logger.error(f\"Error with file {csv_path}\")\n",
    "                raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
