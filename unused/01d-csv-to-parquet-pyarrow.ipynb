{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc77c51-01c0-4027-9fb9-c46b76caae3a",
   "metadata": {},
   "source": [
    "# CSV to parquet\n",
    "\n",
    "We use pyarrow here.\n",
    "\n",
    "We also rename column names to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a050fd1-0264-4b95-a491-64c4368d70ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/matthew/Documents/TSE/AppliedEconometrics/repo/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import importlib\n",
    "from itertools import zip_longest\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pyarrow is like pandas, but works for datasets too big for memory.\n",
    "import pyarrow\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46927f8e-2e66-4f42-bd02-dbc157da6846",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d51f76b-9013-4c91-a6b6-7a1b29a05b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data/'\n",
    "laptop_data_dir = '/home/matthew/data/'\n",
    "\n",
    "# output of the previous script\n",
    "source_dir = os.path.join(laptop_data_dir, '01-C-split-mapped-csv')\n",
    "\n",
    "\n",
    "# the parquet files go here\n",
    "dest_dir = os.path.join(laptop_data_dir, '01-D-parquet-pyarrow')\n",
    "\n",
    "# once files are processed, we move them here\n",
    "# if move_when_done\n",
    "archive_dir = os.path.join(laptop_data_dir, '01-D-split-mapped-csv-done')\n",
    "move_when_done = True\n",
    "\n",
    "schema_path = os.path.join(repo_data_dir, 'schemas.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb649f5-17eb-4011-addb-fe205c7f931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_col_name = 'SCHEMA_VERSION'\n",
    "top_timestamp_col_name = 'TOP_TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c083668-1c31-4dbf-9f87-0ef4fc52104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(os.path.join(repo_data_dir, 'logs.txt'))\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c42fe6-66f9-4c5b-9f76-9e8fa290e7a2",
   "metadata": {},
   "source": [
    "## Prepare Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b429d297-81f5-42fa-be5d-d2caaf7b7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(schema_path, 'r') as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ade408-cf3a-4cd8-b3b7-4f4fad68f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEMO's schemas have Oracle SQL types\n",
    "# map those to types arrow can use\n",
    "# e.g. DATE -> pl.datatypes.Date\n",
    "# NUMBER(2,0) -> pl.Int16\n",
    "# NUMBER(15,5) -> pl.Float64\n",
    "# VARCHAR2(10) -> pl.String\n",
    "# if date_as_str, return string instead of datetime\n",
    "# (because pyarrow can't read datetimes when parsing from CSV)\n",
    "def aemo_type_to_arrow_type(t: str, date_as_str=False) -> pa.DataType:\n",
    "    t = t.upper()\n",
    "    if re.match(r\"VARCHAR(2)?\\(\\d+\\)\", t):\n",
    "        return pa.string()\n",
    "    if re.match(r\"CHAR\\((\\d+)\\)\", t):\n",
    "        # single character\n",
    "        # arrow has no dedicated type for that\n",
    "        # so use string\n",
    "        # (could use categorical?)\n",
    "        return pa.string()\n",
    "    elif t.startswith(\"NUMBER\"):\n",
    "        match = re.match(r\"NUMBER ?\\((\\d+), ?(\\d+)\\)\", t)\n",
    "        if match:\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = int(match.group(2))\n",
    "        else:\n",
    "            # e.g. NUMBER(2)\n",
    "            match = re.match(r\"NUMBER ?\\((\\d+)\", t)\n",
    "            assert match, f\"Unsure how to cast {t} to arrow type\"\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = 0\n",
    "            \n",
    "        if decimal_digits == 0:\n",
    "            # integer\n",
    "            # we assume signed (can't tell unsigned from the schema)\n",
    "            # but how many bits?\n",
    "            max_val = 10**whole_digits\n",
    "\n",
    "            if 2**(8-1) > max_val:\n",
    "                return pa.int8()\n",
    "            elif 2**(16-1) > max_val:\n",
    "                return pa.int16()\n",
    "            elif 2**(32-1) > max_val:\n",
    "                return pa.int32()\n",
    "            else:\n",
    "                return pa.int64()\n",
    "        else:\n",
    "            # we could use pa.decimal128(whole_digits, decimal_digits)\n",
    "            # but we don't need that much accuracy\n",
    "            return pa.float64()\n",
    "    elif (t == 'DATE') or re.match(r\"TIMESTAMP\\((\\d)\\)\", t):\n",
    "        # watch out, when AEMO say \"date\" they mean \"datetime\"\n",
    "        # for both dates and datetimes they say \"date\",\n",
    "        # but both have a time component. (For actual dates, it's always midnight.)\n",
    "        # and some dates go out as far as 9999-12-31 23:59:59.999\n",
    "        # (and some dates are 9999-12-31 23:59:59.997)\n",
    "        if date_as_str:\n",
    "            return pa.string()\n",
    "        else:\n",
    "            # pyarrow doesn't support parsing into a given timezone when reading from CSV\n",
    "            # it does for batched chunks of CSV, but we want to stream to avoid using up all memory\n",
    "            # so we'll treat these as timezone unaware datetimes\n",
    "            #return pa.timestamp('s', tz='Australia/Brisbane')\n",
    "            return pa.timestamp('s')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsure how to convert AEMO type {t} to arrow type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b43934cb-abff-4005-8db7-a2581a4334c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_dir(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319ef855-fee7-4b16-b5f1-2cd8ab9c09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPATCHLOAD is quite large, and we only need some columns for our analysis. If we drop the columns we don't need, the whole thing will be far faster.\n",
    "# include ones we *might* use\n",
    "def get_cols_to_skip(table):\n",
    "    cols = []\n",
    "    if table == 'DISPATCHLOAD':\n",
    "        for c in schemas['DISPATCHLOAD'].keys():\n",
    "            if any(c.upper().startswith(prefix) for prefix in ['RAISE', 'LOWER', 'VIOLATION', 'MARGINAL']):\n",
    "                cols.append(c)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f623a1-fe3f-46a7-8371-b0b8d761351f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find tables we have no schema for\n",
    "[t for t in os.listdir(source_dir) if t not in schemas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fb731-08e2-42f2-a936-8610e91bb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# takes in the name of a folder of CSVs\n",
    "# converts them all to a single parquet file\n",
    "# for `table`, the files are like\n",
    "# source_dir / table / SCHEMA_VERSION=2 / TOP_TIMESTAMP=2019_03_02_00_45_12 /  something.CSV.gz\n",
    "# renames columns to lowercase names\n",
    "def convert_csv_parquet(table):\n",
    "    table_dir = os.path.join(source_dir, table)\n",
    "    parquet_file = os.path.join(dest_dir, table + '.parquet')\n",
    "\n",
    "    logger.info(f\"Preparing to process {table} from {table_dir} to {parquet_file}\")\n",
    "\n",
    "    input_schema = {}\n",
    "    output_schema = {}\n",
    "    datetime_columns = []\n",
    "    expected_source_columns = list(schemas[table]['columns'].keys())\n",
    "    columns_to_skip = get_cols_to_skip(table)\n",
    "    source_columns_to_read = [c for c in expected_source_columns if c not in columns_to_skip]\n",
    "    for (c,s) in schemas[table]['columns'].items():\n",
    "        t = s['AEMO_type']\n",
    "        input_schema[c] = aemo_type_to_arrow_type(t, date_as_str=True)\n",
    "        if c not in columns_to_skip:\n",
    "            output_schema[c.lower()] = aemo_type_to_arrow_type(t, date_as_str=False)\n",
    "            if isinstance(output_schema[c.lower()], pa.TimestampType):\n",
    "                datetime_columns.append(c)\n",
    "    output_schema.update({\n",
    "        version_col_name.lower(): pa.uint8(),\n",
    "        # leave as string, don't bother converting\n",
    "        # we only use this as a sort key later for deduplication\n",
    "        top_timestamp_col_name.lower(): pa.string(), \n",
    "    })\n",
    "\n",
    "    output_columns = list(output_schema.keys())\n",
    "    \n",
    "    input_schema = pyarrow.schema(input_schema)\n",
    "    output_schema = pyarrow.schema(output_schema)\n",
    "\n",
    "    logger.info(f\"Starting to process {table} from {table_dir} to {parquet_file}\")\n",
    "\n",
    "    # this will overwrite an existing file (from a previous run)    \n",
    "    with pyarrow.parquet.ParquetWriter(\n",
    "        where=parquet_file, \n",
    "        schema=output_schema, \n",
    "        dictionary_pagesize_limit=1,\n",
    "        use_dictionary=False, #[c for c in schemas[table]['primary_keys'] if 'DATE' not in c.upper()],\n",
    "        write_statistics=False) as writer:\n",
    "        for schema_subdir in os.listdir(table_dir):\n",
    "            match = re.match(rf\"{version_col_name}=(\\d+)\", schema_subdir)\n",
    "            assert match, f\"Unable to extract schema version from {os.path.join(table_dir, schema_subdir)}\"\n",
    "            schema_version = int(match.group(1))\n",
    "\n",
    "            for top_timestamp_subdir in os.listdir(os.path.join(table_dir, schema_subdir)):\n",
    "                match = re.match(rf\"{top_timestamp_col_name}=([\\d_]+)\", top_timestamp_subdir)\n",
    "                assert match, f\"Unable to extract top_timestamp from {os.path.join(table_dir, schema_subdir, top_timestamp_subdir)}\"\n",
    "                top_timestamp = match.group(1)\n",
    "\n",
    "                for csv_file in os.listdir(os.path.join(table_dir, schema_subdir, top_timestamp_subdir)):\n",
    "                    try:\n",
    "                        \n",
    "                        csv_path = os.path.join(table_dir, schema_subdir, top_timestamp_subdir, csv_file)\n",
    "                        logger.info(f\"Processing {csv_path}\")\n",
    "                        with gzip.open(csv_path, 'rt') as f:\n",
    "                            first_line = f.readline()\n",
    "                            assert isinstance(first_line, str), f\"file openned in wrong mode. {type(first_line)=}\"\n",
    "                            actual_source_columns = first_line.strip().split(',')\n",
    "\n",
    "                        unexpected_source_columns = [c for c in actual_source_columns if c not in expected_source_columns]\n",
    "                        if unexpected_source_columns:\n",
    "                            logger.warning(f\"Found unexpected columns {unexpected_source_columns} in {csv_file}\")\n",
    "                        \n",
    "                        csv_reader = pyarrow.csv.open_csv(csv_path, \n",
    "                                                          convert_options=pyarrow.csv.ConvertOptions(\n",
    "                                                              column_types=input_schema, \n",
    "                                                              include_missing_columns=True, \n",
    "                                                              include_columns=expected_source_columns)\n",
    "                                                         )\n",
    "            \n",
    "                        for batch in csv_reader:\n",
    "                            columns = []\n",
    "                            \n",
    "                            for (i, column_name) in enumerate(expected_source_columns):\n",
    "                                assert isinstance(column_name, str), f\"column_name is type {type(column_name)}\"\n",
    "                                column_data = batch[column_name]\n",
    "                                if column_name in datetime_columns:\n",
    "                                    # pyarrow assumes the timesstamp is UTC by default\n",
    "                                    # the schema specifies the timezone as UTC+10 (which we want)\n",
    "                                    # but it adds 10h when doing the conversion\n",
    "                                    # so let's subtract 10h\n",
    "                                    # This is checked with a unit test later\n",
    "                                    column_data = pc.strptime(column_data, format=\"%Y/%m/%d %H:%M:%S\", unit=\"s\", error_is_null=True)\n",
    "                                    column_data = pc.assume_timezone(column_data, timezone='Australia/Brisbane')\n",
    "                                    #column_data = pc.add(column_data, -dt.timedelta(hours=TIMEZONE_OFFSET))\n",
    "                                columns.append(column_data)\n",
    "    \n",
    "                            # now add the metadata from folder names as columns\n",
    "                            # pa.array(repeat(x), type=pa.string(), size=batch.num_rows)\n",
    "                            columns.append(pa.Array.from_pandas(np.repeat(schema_version, batch.num_rows)))\n",
    "    \n",
    "                            # convert top timestamp to datetime too\n",
    "                            tt = pa.array(np.repeat(top_timestamp, batch.num_rows), type=pa.string(), size=batch.num_rows)\n",
    "                            tt = pc.strptime(tt, format=\"%Y_%m_%d_%H_%M_%S\", unit=\"s\", error_is_null=True)\n",
    "                            tt = pc.assume_timezone(tt, timezone='Australia/Brisbane')\n",
    "                            columns.append(tt)\n",
    "\n",
    "                            for (oc, ic) in zip_longest(output_schema.names[:-2], expected_source_columns):\n",
    "                                assert oc == ic.lower(), f\"{oc=} {ic=}\"\n",
    "                            \n",
    "                            updated_table = pyarrow.Table.from_arrays(\n",
    "                                columns, \n",
    "                                schema=output_schema\n",
    "                            )\n",
    "                            writer.write(updated_table)\n",
    "                    except pa.ArrowInvalid as ex:\n",
    "                        logger.error(f\"Issue with {csv_path}\")\n",
    "                        logger.info(f\"INPUT SCHEMA:\\n{input_schema}\")\n",
    "                        logger.info(f\"OUTPUT SCHEMA:\\n{output_schema}\")\n",
    "                        raise\n",
    "        logger.info(f\"Closing parquet for {table}\")\n",
    "    logger.info(f\"Closed parquet for {table}\")\n",
    "    if move_when_done:\n",
    "        # move away,\n",
    "        # so if the next table fails, and we re-run the script\n",
    "        # we don't waste time re-doing this one\n",
    "        table_archive_dir = os.path.join(archive_dir, table)\n",
    "        logger.info(f\"Finished with {table}, moving {table_dir} to {table_archive_dir}\")\n",
    "        utils.create_dir(archive_dir)\n",
    "        os.rename(table_dir, table_archive_dir)\n",
    "\n",
    "tables = [t for t in schemas if os.path.exists(os.path.join(source_dir, t))]\n",
    "tables[::-1]\n",
    "logger.reset()\n",
    "logger.info(f\"Tables listed\")\n",
    "\n",
    "logger.flush = True\n",
    "\n",
    "# no multiprocessing\n",
    "# because some tables require almost all memory to process\n",
    "# (even when streaming)\n",
    "for table in tqdm(tables):\n",
    "    convert_csv_parquet(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d5f88-e465-46c1-a8ea-77ff178b7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this next\n",
    "# https://arrow.apache.org/docs/python/dataset.html#configuring-rows-per-group-during-a-write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb53b0-0287-4cb7-91a2-2f45e2af39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "table = \"DUDETAIL\"\n",
    "source_dir = f\"/home/matthew/data/01-C-split-mapped-done/{table}\"\n",
    "part = ds.partitioning(\n",
    "    pa.schema([(\"SCHEMA_VERSION\", pa.int8()), (\"TOP_TIMESTAMP\", pa.string()),\n",
    "    flavor=\"hive\"\n",
    ")\n",
    "schema = {c: aemo_type_to_arrow_type(t['AEMO_type'], date_as_str=True) for (c,t) in schemas[table]['columns']}\n",
    "dataset = ds.dataset(\n",
    "    source=source_dir, \n",
    "    format=\"csv\",\n",
    "    partitioning=part,\n",
    "    schema=pyarrow.schema(schema)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
