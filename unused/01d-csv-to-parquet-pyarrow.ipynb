{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc77c51-01c0-4027-9fb9-c46b76caae3a",
   "metadata": {},
   "source": [
    "# CSV to parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609abb78-d83d-4ed5-abc1-364c0252ebbd",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "If you don't have these libraries installed, run `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52747299-874d-4489-8817-6be9929b5521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a050fd1-0264-4b95-a491-64c4368d70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, current_process\n",
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pyarrow is like pandas, but works for datasets too big for memory.\n",
    "import pyarrow\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46927f8e-2e66-4f42-bd02-dbc157da6846",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51f76b-9013-4c91-a6b6-7a1b29a05b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_one = '/media/matthew/Tux/AppliedEconometrics/data'\n",
    "disk_two = '/media/matthew/nemweb/AppliedEconometrics/data'\n",
    "repo_data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data/'\n",
    "laptop_data_dir = '/home/matthew/data/'\n",
    "\n",
    "source_dir = os.path.join(laptop_data_dir, '01-E-split-mapped-csv')\n",
    "dest_dir = os.path.join(laptop_data_dir, '01-F-parquet-duplicated')\n",
    "\n",
    "schema_path = os.path.join(repo_data_dir, 'schemas.json')\n",
    "\n",
    "# process files with multiprocessing, to be faster.\n",
    "# If set to False, will use a for loop, which gives clearer traceback error messages.\n",
    "use_multiprocessing = True\n",
    "\n",
    "if leave_unused_cpu:\n",
    "    num_processes = os.cpu_count() - 2 # *2 because we assume hyperthreading\n",
    "else:\n",
    "    num_processes = os.cpu_count()\n",
    "\n",
    "# set to True to leave one unused CPU when multiprocessing\n",
    "# so that you can still do other stuff on your laptop without your internet browser or whatever being laggy\n",
    "leave_unused_cpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb649f5-17eb-4011-addb-fe205c7f931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_col_name = 'SCHEMA_VERSION'\n",
    "top_timestamp_col_name = 'TOP_TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c083668-1c31-4dbf-9f87-0ef4fc52104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(os.path.join(repo_data_dir, 'logs.txt'))\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c42fe6-66f9-4c5b-9f76-9e8fa290e7a2",
   "metadata": {},
   "source": [
    "## Prepare Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429d297-81f5-42fa-be5d-d2caaf7b7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(schema_path, 'r') as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ade408-cf3a-4cd8-b3b7-4f4fad68f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEMO's schemas have Oracle SQL types\n",
    "# map those to types arrow can use\n",
    "# e.g. DATE -> pl.datatypes.Date\n",
    "# NUMBER(2,0) -> pl.Int16\n",
    "# NUMBER(15,5) -> pl.Float64\n",
    "# VARCHAR2(10) -> pl.String\n",
    "# if date_as_str, return string instead of datetime\n",
    "# (because pyarrow can't read datetimes when parsing from CSV)\n",
    "def aemo_type_to_arrow_type(t: str, date_as_str=False) -> pa.DataType:\n",
    "    t = t.upper()\n",
    "    if re.match(r\"VARCHAR(2)?\\(\\d+\\)\", t):\n",
    "        return pa.string()\n",
    "    if re.match(r\"CHAR\\((\\d+)\\)\", t):\n",
    "        # single character\n",
    "        # arrow has no dedicated type for that\n",
    "        # so use string\n",
    "        # (could use categorical?)\n",
    "        return pa.string()\n",
    "    elif t.startswith(\"NUMBER\"):\n",
    "        match = re.match(r\"NUMBER ?\\((\\d+), ?(\\d+)\\)\", t)\n",
    "        if match:\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = int(match.group(2))\n",
    "        else:\n",
    "            # e.g. NUMBER(2)\n",
    "            match = re.match(r\"NUMBER ?\\((\\d+)\", t)\n",
    "            assert match, f\"Unsure how to cast {t} to arrow type\"\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = 0\n",
    "            \n",
    "        if decimal_digits == 0:\n",
    "            # integer\n",
    "            # we assume signed (can't tell unsigned from the schema)\n",
    "            # but how many bits?\n",
    "            max_val = 10**whole_digits\n",
    "\n",
    "            if 2**(8-1) > max_val:\n",
    "                return pa.int8()\n",
    "            elif 2**(16-1) > max_val:\n",
    "                return pa.int16()\n",
    "            elif 2**(32-1) > max_val:\n",
    "                return pa.int32()\n",
    "            else:\n",
    "                return pa.int64()\n",
    "        else:\n",
    "            # we could use pa.decimal128(whole_digits, decimal_digits)\n",
    "            # but we don't need that much accuracy\n",
    "            return pa.float64()\n",
    "    elif (t == 'DATE') or re.match(r\"TIMESTAMP\\((\\d)\\)\", t):\n",
    "        # watch out, when AEMO say \"date\" they mean \"datetime\"\n",
    "        # for both dates and datetimes they say \"date\",\n",
    "        # but both have a time component. (For actual dates, it's always midnight.)\n",
    "        # and some dates go out as far as 9999-12-31 23:59:59.999\n",
    "        # (and some dates are 9999-12-31 23:59:59.997)\n",
    "        if date_as_str:\n",
    "            return pa.string()\n",
    "        else:\n",
    "            return pa.timestamp('s', tz='Australia/Brisbane')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsure how to convert AEMO type {t} to arrow type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43934cb-abff-4005-8db7-a2581a4334c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dest_dir):\n",
    "    os.makedirs(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ef855-fee7-4b16-b5f1-2cd8ab9c09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPATCHLOAD is quite large, and we only need some columns for our analysis. If we drop the columns we don't need, the whole thing will be far faster.\n",
    "# include ones we *might* use\n",
    "cols = list(schemas['DISPATCHLOAD'].keys())\n",
    "for c in cols:\n",
    "    if any(c.upper().startswith(prefix) for prefix in ['RAISE', 'LOWER', 'VIOLATION', 'MARGINAL']):\n",
    "        print(f\"Deleting column {c} from DISPATCHLOAD\")\n",
    "        del schemas['DISPATCHLOAD'][c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe872c5a-e391-491e-a0cf-964e75ac5f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: document the datetime from string handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f623a1-fe3f-46a7-8371-b0b8d761351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find tables we have no schema for\n",
    "[t for t in os.listdir(source_dir) if t not in schemas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fb731-08e2-42f2-a936-8610e91bb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the name of a folder of CSVs\n",
    "# converts them all to a single parquet file\n",
    "# for `table`, the files are like\n",
    "# source_dir / table / SCHEMA_VERSION=2 / TOP_TIMESTAMP=2019_03_02_00_45_12 /  something.CSV.gz\n",
    "def convert_csv_parquet(table):\n",
    "    table_dir = os.path.join(source_dir, table)\n",
    "    parquet_file = os.path.join(dest_dir, table + '.parquet')\n",
    "\n",
    "    input_schema = {}\n",
    "    output_schema = {}\n",
    "    datetime_columns = []\n",
    "    column_names = [] # source columns\n",
    "    for (c,s) in schemas[table]['columns'].items():\n",
    "        t = s['AEMO_type']\n",
    "        input_schema[c] = aemo_type_to_arrow_type(t, date_as_str=True)\n",
    "        output_schema[c] = aemo_type_to_arrow_type(t, date_as_str=False)\n",
    "        if isinstance(output_schema[c], pa.TimestampType):\n",
    "            datetime_columns.append(c)\n",
    "        column_names.append(c)\n",
    "    output_schema.update({\n",
    "        version_col_name: pa.uint8(),\n",
    "        # leave as string, don't bother converting\n",
    "        # we only use this as a sort key later for deduplication\n",
    "        top_timestamp_col_name: pa.string(), \n",
    "    })\n",
    "    \n",
    "    input_schema = pyarrow.schema(input_schema)\n",
    "    output_schema = pyarrow.schema(output_schema)\n",
    "\n",
    "    # this will overwrite an existing file (from a previous run)    \n",
    "    with pyarrow.parquet.ParquetWriter(parquet_file, output_schema) as writer:\n",
    "        for schema_subdir in os.listdir(table_dir):\n",
    "            match = re.match(rf\"{version_col_name}=(\\d+)\", schema_subdir)\n",
    "            assert match, f\"Unable to extract schema version from {os.path.join(table_dir, schema_subdir)}\"\n",
    "            schema_version = int(match.group(1))\n",
    "\n",
    "            for top_timestamp_subdir in os.listdir(os.path.join(table_dir, schema_subdir)):\n",
    "                match = re.match(rf\"{top_timestamp_col_name}=([\\d_]+)\", top_timestamp_subdir)\n",
    "                assert match, f\"Unable to extract top_timestabl from {os.path.join(table_dir, schema_subdir, top_timestamp_subdir)}\"\n",
    "                top_timestamp = match.group(1)\n",
    "\n",
    "                for csv_file in os.listdir(os.path.join(table_dir, schema_subdir, top_timestamp_subdir)):\n",
    "                    csv_path = os.path.join(table_dir, schema_subdir, top_timestamp_subdir, csv_file)\n",
    "                    csv_reader = pyarrow.csv.open_csv(csv_path, \n",
    "                                                      convert_options=pyarrow.csv.ConvertOptions(\n",
    "                                                      column_types=input_schema, \n",
    "                                                      include_missing_columns=True, \n",
    "                                                      include_columns=column_names))\n",
    "        \n",
    "                    for batch in csv_reader:\n",
    "                        columns = []\n",
    "                        \n",
    "                        for (i, column_name) in enumerate(column_names):\n",
    "                            assert isinstance(column_name, str), f\"column_name is type {type(column_name)}\"\n",
    "                            column_data = batch[column_name]\n",
    "                            if column_name in datetime_columns:\n",
    "                                # pyarrow assumes the timesstamp is UTC by default\n",
    "                                # the schema specifies the timezone as UTC+10 (which we want)\n",
    "                                # but it adds 10h when doing the conversion\n",
    "                                # so let's subtract 10h\n",
    "                                # This is checked with a unit test later\n",
    "                                column_data = pc.strptime(column_data, format=\"%Y/%m/%d %H:%M:%S\", unit=\"s\", error_is_null=True)\n",
    "                                column_data = pc.assume_timezone(column_data, timezone='Australia/Brisbane')\n",
    "                                #column_data = pc.add(column_data, -dt.timedelta(hours=TIMEZONE_OFFSET))\n",
    "                            columns.append(column_data)\n",
    "\n",
    "                        # now add the metadata from folder names as columns\n",
    "                        # pa.array(repeat(x), type=pa.string(), size=batch.num_rows)\n",
    "                        columns.append(pa.Array.from_pandas(np.repeat(schema_version, batch.num_rows)))\n",
    "\n",
    "                        # convert top timestamp to datetime too\n",
    "                        tt = pa.array(np.repeat(top_timestamp, batch.num_rows), type=pa.string(), size=batch.num_rows)\n",
    "                        tt = pc.strptime(tt, format=\"%Y_%m_%d_%H_%M_%S\", unit=\"s\", error_is_null=True)\n",
    "                        tt = pc.assume_timezone(tt, timezone='Australia/Brisbane')\n",
    "                        columns.append(tt)\n",
    "                        \n",
    "                        updated_table = pyarrow.Table.from_arrays(\n",
    "                            columns, \n",
    "                            schema=output_schema\n",
    "                        )\n",
    "                        writer.write(updated_table)\n",
    "\n",
    "tables = [t for t in schemas if os.path.exists(os.path.join(source_dir, t))]\n",
    "print(f\"Tables listed\")\n",
    "if use_multiprocessing and False:\n",
    "    if leave_unused_cpu:\n",
    "        num_processes = os.cpu_count() - 2 # *2 because we assume hyperthreading\n",
    "    else:\n",
    "        num_processes = os.cpu_count()\n",
    "    with Pool(num_processes) as p:\n",
    "        list(tqdm(p.imap(convert_csv_parquet, tables), total=len(tables)))\n",
    "else:\n",
    "    for table in tqdm(tables):\n",
    "        convert_csv_parquet(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
