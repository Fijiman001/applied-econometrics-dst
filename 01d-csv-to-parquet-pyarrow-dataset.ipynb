{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc77c51-01c0-4027-9fb9-c46b76caae3a",
   "metadata": {},
   "source": [
    "# CSV to parquet\n",
    "\n",
    "This script takes a few hours to run.\n",
    "\n",
    "We have many CSV files, mostly small ones, for each AEMO 'table' (dataframe). This script merges lots of small CSVs, and creates a one large parquet file (per table). \"Compacting\" into fewer, larger files improves performance later.\n",
    "\n",
    "To see the advantages of parquet over csv, read [this](https://r4ds.hadley.nz/arrow#advantages-of-parquet).\n",
    "The main benefits are:\n",
    "\n",
    "* performance - e.g. if we only care about 2 out of 20 columns, we skip over 90% of data when reading from disk.\n",
    "* type safety - the file keeps track of what's a float vs datetime etc. So we don't have to tell the code what each datatype is after this script.\n",
    "\n",
    "We use pyarrow here, not Pandas. This is partly for performance reasons. Also because Pandas can't handle empty values for some datatypes.\n",
    "\n",
    "When running, don't forget to change `base_data_dir`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a050fd1-0264-4b95-a491-64c4368d70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import importlib\n",
    "import shutil\n",
    "from random import shuffle\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pyarrow is like pandas, but works for datasets too big for memory.\n",
    "import pyarrow\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.csv\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46927f8e-2e66-4f42-bd02-dbc157da6846",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51f76b-9013-4c91-a6b6-7a1b29a05b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = 'data'\n",
    "\n",
    "# output of the previous script\n",
    "source_dir = os.path.join(base_data_dir, '01-C-split-mapped-csv')\n",
    "\n",
    "# the parquet files go here\n",
    "dest_dir = os.path.join(base_data_dir, '01-D-parquet-pyarrow-dataset')\n",
    "\n",
    "# once files are processed, we move them here\n",
    "# if move_when_done\n",
    "archive_dir = os.path.join(base_data_dir, '01-D-split-mapped-csv-done')\n",
    "move_when_done = False\n",
    "\n",
    "schema_path = os.path.join(base_data_dir, '01-aemo-schemas.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb649f5-17eb-4011-addb-fe205c7f931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_col_name = 'SCHEMA_VERSION'\n",
    "top_timestamp_col_name = 'TOP_TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c083668-1c31-4dbf-9f87-0ef4fc52104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(os.path.join(base_data_dir, 'logs.txt'))\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c42fe6-66f9-4c5b-9f76-9e8fa290e7a2",
   "metadata": {},
   "source": [
    "## Prepare Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429d297-81f5-42fa-be5d-d2caaf7b7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(schema_path, 'r') as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ade408-cf3a-4cd8-b3b7-4f4fad68f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEMO's schemas have Oracle SQL types\n",
    "# map those to types arrow can use\n",
    "# e.g. DATE -> pl.datatypes.Date\n",
    "# NUMBER(2,0) -> pl.Int16\n",
    "# NUMBER(15,5) -> pl.Float64\n",
    "# VARCHAR2(10) -> pl.String\n",
    "# if date_as_str, return string instead of datetime\n",
    "# (because pyarrow can't read datetimes when parsing from CSV)\n",
    "def aemo_type_to_arrow_type(t: str, date_as_str=False) -> pa.DataType:\n",
    "    t = t.upper()\n",
    "    if re.match(r\"VARCHAR(2)?\\(\\d+\\)\", t):\n",
    "        return pa.string()\n",
    "    if re.match(r\"CHAR\\((\\d+)\\)\", t):\n",
    "        # single character\n",
    "        # arrow has no dedicated type for that\n",
    "        # so use string\n",
    "        # (could use categorical?)\n",
    "        return pa.string()\n",
    "    elif t.startswith(\"NUMBER\"):\n",
    "        match = re.match(r\"NUMBER ?\\((\\d+), ?(\\d+)\\)\", t)\n",
    "        if match:\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = int(match.group(2))\n",
    "        else:\n",
    "            # e.g. NUMBER(2)\n",
    "            match = re.match(r\"NUMBER ?\\((\\d+)\", t)\n",
    "            assert match, f\"Unsure how to cast {t} to arrow type\"\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = 0\n",
    "            \n",
    "        if decimal_digits == 0:\n",
    "            # integer\n",
    "            # we assume signed (can't tell unsigned from the schema)\n",
    "            # but how many bits?\n",
    "            max_val = 10**whole_digits\n",
    "\n",
    "            if 2**(8-1) > max_val:\n",
    "                return pa.int8()\n",
    "            elif 2**(16-1) > max_val:\n",
    "                return pa.int16()\n",
    "            elif 2**(32-1) > max_val:\n",
    "                return pa.int32()\n",
    "            else:\n",
    "                return pa.int64()\n",
    "        else:\n",
    "            # we could use pa.decimal128(whole_digits, decimal_digits)\n",
    "            # but we don't need that much accuracy\n",
    "            return pa.float64()\n",
    "    elif (t == 'DATE'):\n",
    "        # watch out, when AEMO say \"date\" they mean \"datetime\"\n",
    "        # for both dates and datetimes they say \"date\",\n",
    "        # but both have a time component. (For actual dates, it's always midnight.)\n",
    "        # and some dates go out as far as 9999-12-31 23:59:59.999\n",
    "        # (and some dates are 9999-12-31 23:59:59.997)\n",
    "        if date_as_str:\n",
    "            return pa.string()\n",
    "        else:\n",
    "            # no timezone here\n",
    "            # pyarrow can't assume timezone when reading from CSV\n",
    "            # we treat them is timezone unaware for now\n",
    "            return pa.timestamp('s')\n",
    "            #return pa.timestamp('s', tz='Australia/Brisbane')\n",
    "    elif re.match(r\"TIMESTAMP\\((\\d)\\)\", t):\n",
    "        # this is the same as DATE, but with a microsecond component\n",
    "        if date_as_str:\n",
    "            return pa.string()\n",
    "        else:\n",
    "            # https://github.com/apache/arrow/issues/39839\n",
    "            # bug with pyarrow. It can't handle millisecond components\n",
    "            # even with .%f\n",
    "            # But this millisecond granularity type is quite rare\n",
    "            # I don't think it happens in any of the tables we care about.\n",
    "            return pa.string()\n",
    "            #return pa.timestamp('ms')\n",
    "    else:\n",
    "        raise ValueError(f\"Unsure how to convert AEMO type {t} to arrow type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa194bb-6f60-4446-80c4-9351fc62b934",
   "metadata": {},
   "source": [
    "Note that if a table has columns specified in `columns_to_drop`, those will be already omitted from the source CSV. But this script will add them back in as NA/NULL. This doesn't waste space, because parquet compresses data well. This was a deliberate decision because as we changed how we handle `columns_to_drop`, if they are in the source file, then pyarrow will try to read the data of those columns to infer datatype. That's slow, and can result in errors. (e.g. it can't reconcile int and float.)\n",
    "So we don't exclude `columns_to_drop` when generating `csv_schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fb731-08e2-42f2-a936-8610e91bb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the name of a folder of CSVs\n",
    "# converts them all to a single parquet file\n",
    "# for `table`, the files are like\n",
    "# source_dir / table / SCHEMA_VERSION=2 / TOP_TIMESTAMP=2019_03_02_00_45_12 /  something.CSV.gz\n",
    "def convert_csv_parquet(table):\n",
    "    table_source_dir = os.path.join(source_dir, table)\n",
    "    table_dest_dir = os.path.join(dest_dir, table)\n",
    "\n",
    "    logger.info(f\"Preparing to process {table} from {table_source_dir} to {table_dest_dir}\")\n",
    "\n",
    "    csv_schema = {c: aemo_type_to_arrow_type(t['AEMO_type'], date_as_str=False) for (c,t) in schemas[table]['columns'].items()}\n",
    "    partition_schema = {\n",
    "        \"SCHEMA_VERSION\": pa.int8(), \n",
    "        \"TOP_TIMESTAMP\": pa.string(),\n",
    "    }\n",
    "    schema = dict(csv_schema, **partition_schema)\n",
    "    dataset = ds.dataset(\n",
    "        source=table_source_dir, \n",
    "        format=ds.CsvFileFormat(\n",
    "            convert_options=pyarrow.csv.ConvertOptions(\n",
    "                timestamp_parsers=[\n",
    "                    \"%Y/%m/%d %H:%M:%S\",\n",
    "                    \"%Y/%m/%d %H:%M:%S.%f\",\n",
    "                ]\n",
    "            )\n",
    "        ), # really .csv.gz, but pyarrow will figure that out\n",
    "        partitioning=ds.partitioning(\n",
    "            pa.schema(partition_schema),\n",
    "            flavor=\"hive\"\n",
    "        ),\n",
    "        schema=pyarrow.schema(schema)\n",
    "    )\n",
    "\n",
    "    shutil.rmtree(table_dest_dir, ignore_errors=True)\n",
    "    ds.write_dataset(\n",
    "        data=dataset, \n",
    "        base_dir=table_dest_dir, \n",
    "        format=\"parquet\", \n",
    "        min_rows_per_group=1024*1024,\n",
    "        existing_data_behavior=\"delete_matching\"\n",
    "    )\n",
    "    logger.info(f\"Finished writing {table} to {table_dest_dir}\")\n",
    "    if move_when_done:\n",
    "        # move away,\n",
    "        # so if the next table fails, and we re-run the script\n",
    "        # we don't waste time re-doing this one\n",
    "        table_archive_dir = os.path.join(archive_dir, table)\n",
    "        logger.info(f\"Finished with {table}, moving {table_source_dir} to {table_archive_dir}\")\n",
    "        utils.create_dir(archive_dir)\n",
    "        os.rename(table_source_dir, table_archive_dir)\n",
    "\n",
    "tables = [t for t in schemas if os.path.exists(os.path.join(source_dir, t))]\n",
    "shuffle(tables)\n",
    "logger.info(f\"{len(tables)} Tables listed\")\n",
    "logger.flush = True\n",
    "\n",
    "# no multiprocessing\n",
    "# because max memory usage peaks very high\n",
    "for table in tqdm(tables):\n",
    "    gc.collect()\n",
    "    convert_csv_parquet(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10deb2-b5ea-4467-8ac8-86b1c5c487c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in schemas if os.path.exists(os.path.join(source_dir, t))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77318b-20a7-4d4c-892c-c203cbf9a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed769183-1985-450a-abb9-da846304ce40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
