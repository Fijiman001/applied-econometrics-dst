
> # constants ---------------------------------------------------------------
> 
> Sys.setenv(TZ='UTC')

> # directories
> data_dir <- here::here("data")

> source_dir <-  file.path(data_dir, '01-D-parquet-pyarrow-dataset')

> source_dispatchload_dir <-  file.path(source_dir, 'DISPATCHLOAD')

> dispatchload_partitioned_dir <- file.path(data_dir, '01-E-DISPATCHLOAD-partitioned-by-month-raw')

> month_dir <-  file.path(data_dir, '01-E-DISPATCHLOAD-partitioned-by-region-month')

> duid_standing_path <- file.path(data_dir, '01-E-duid-standing.parquet')

> import_export_path <- file.path(data_dir, '01-E-import-export.parquet')

> start_year <- 2009

> end_year <- 2023

> # time constants
> # data granularity is 5 minutes
> # so this many hours per interval
> h_per_interval <- 5/60

> # minutes per half hour
> min_per_hh <- 30

> # initial repartition -----------------------------------------------------
> 
> 
> # Let's take the one large parquet file
> # and write one file p .... [TRUNCATED] 
[1] "Repartitioning year=2009"
[1] "Repartitioning year=2010"
[1] "Repartitioning year=2011"
[1] "Repartitioning year=2012"
[1] "Repartitioning year=2013"
[1] "Repartitioning year=2014"
[1] "Repartitioning year=2015"
[1] "Repartitioning year=2016"
[1] "Repartitioning year=2017"
[1] "Repartitioning year=2018"
[1] "Repartitioning year=2019"
[1] "Repartitioning year=2020"
[1] "Repartitioning year=2021"
[1] "Repartitioning year=2022"
[1] "Repartitioning year=2023"

> # reference tables --------------------------------------------------------
> 
> region_duid <- open_dataset(file.path(source_dir, 'DUDETAILSUMMARY' .... [TRUNCATED] 

> # load DUALLOC, deduplicate
> duid_gensetid <- open_dataset(file.path(source_dir, 'DUALLOC')) |>
+   arrange(DUID, GENSETID, desc(VERSIONNO), desc(E .... [TRUNCATED] 

> # load genunits, deduplicate
> genunits <- open_dataset(file.path(source_dir, 'GENUNITS')) |>
+   filter(GENSETTYPE == 'GENERATOR') |>
+   arrange(G .... [TRUNCATED] 

> duid_standing <- genunits |>
+   inner_join(duid_gensetid, by='GENSETID') |>
+   summarise(
+     CO2E_EMISSIONS_FACTOR=weighted.mean(CO2E_EMISSIONS .... [TRUNCATED] 

> duid_standing |>
+   write_parquet(duid_standing_path)

> # main per-month processing ---------------------------------------------------------
> 
> # For each month: (i.e. small enough to fit into memory)
 .... [TRUNCATED] 
[1] "2009 1"
[1] "No data"
[1] "2009 2"
[1] "No data"
[1] "2009 3"
[1] "No data"
[1] "2009 4"
[1] "No data"
[1] "2009 5"
[1] "No data"
[1] "2009 6"
[1] "No data"
[1] "2009 7"
[1] "2009 8"
[1] "2009 9"
[1] "2009 10"
[1] "2009 11"
[1] "2009 12"
[1] "2010 1"
[1] "2010 2"
[1] "2010 3"
[1] "2010 4"
[1] "2010 5"
[1] "2010 6"
[1] "2010 7"
[1] "2010 8"
[1] "2010 9"
[1] "2010 10"
[1] "2010 11"
[1] "2010 12"
[1] "2011 1"
[1] "2011 2"
[1] "2011 3"
[1] "2011 4"
[1] "2011 5"
[1] "2011 6"
[1] "2011 7"
[1] "2011 8"
[1] "2011 9"
[1] "2011 10"
[1] "2011 11"
[1] "2011 12"
[1] "2012 1"
[1] "2012 2"
[1] "2012 3"
[1] "2012 4"
[1] "2012 5"
[1] "2012 6"
[1] "2012 7"
[1] "2012 8"
[1] "2012 9"
[1] "2012 10"
[1] "2012 11"
[1] "2012 12"
[1] "2013 1"
[1] "2013 2"
[1] "2013 3"
[1] "2013 4"
[1] "2013 5"
[1] "2013 6"
[1] "2013 7"
[1] "2013 8"
[1] "2013 9"
[1] "2013 10"
[1] "2013 11"
[1] "2013 12"
[1] "2014 1"
[1] "2014 2"
[1] "2014 3"
[1] "2014 4"
[1] "2014 5"
[1] "2014 6"
[1] "2014 7"
[1] "2014 8"
[1] "2014 9"
[1] "2014 10"
[1] "2014 11"
[1] "2014 12"
[1] "2015 1"
[1] "2015 2"
[1] "2015 3"
[1] "2015 4"
[1] "2015 5"
[1] "2015 6"
[1] "2015 7"
[1] "2015 8"
[1] "2015 9"
[1] "2015 10"
[1] "2015 11"
[1] "2015 12"
[1] "2016 1"
[1] "2016 2"
[1] "2016 3"
[1] "2016 4"
[1] "2016 5"
[1] "2016 6"
[1] "2016 7"
[1] "2016 8"
[1] "2016 9"
[1] "2016 10"
[1] "2016 11"
[1] "2016 12"
[1] "2017 1"
[1] "2017 2"
[1] "2017 3"
[1] "2017 4"
[1] "2017 5"
[1] "2017 6"
[1] "2017 7"
[1] "2017 8"
[1] "2017 9"
[1] "2017 10"
[1] "2017 11"
[1] "2017 12"
[1] "2018 1"
[1] "2018 2"
[1] "2018 3"
[1] "2018 4"
[1] "2018 5"
[1] "2018 6"
[1] "2018 7"
[1] "2018 8"
[1] "2018 9"
[1] "2018 10"
[1] "2018 11"
[1] "2018 12"
[1] "2019 1"
[1] "2019 2"
[1] "2019 3"
[1] "2019 4"
[1] "2019 5"
[1] "2019 6"
[1] "2019 7"
[1] "2019 8"
[1] "2019 9"
[1] "2019 10"
[1] "2019 11"
[1] "2019 12"
[1] "2020 1"
[1] "2020 2"
[1] "2020 3"
[1] "2020 4"
[1] "2020 5"
[1] "2020 6"
[1] "2020 7"
[1] "2020 8"
[1] "2020 9"
[1] "2020 10"
[1] "2020 11"
[1] "2020 12"
[1] "2021 1"
[1] "2021 2"
[1] "2021 3"
[1] "2021 4"
[1] "2021 5"
[1] "2021 6"
[1] "2021 7"
[1] "2021 8"
[1] "2021 9"
[1] "2021 10"
[1] "2021 11"
[1] "2021 12"
[1] "2022 1"
[1] "2022 2"
[1] "2022 3"
[1] "2022 4"
[1] "2022 5"
[1] "2022 6"
[1] "2022 7"
[1] "2022 8"
[1] "2022 9"
[1] "2022 10"
[1] "2022 11"
[1] "2022 12"
[1] "2023 1"
[1] "2023 2"
[1] "2023 3"
[1] "2023 4"
[1] "2023 5"
[1] "2023 6"
[1] "2023 7"
[1] "2023 8"
[1] "2023 9"
[1] "2023 10"
[1] "2023 11"
[1] "2023 12"

> # AEMO data join
> #
> # We handled the big AEMO data in the previous script.
> # That last script was slow and required lots of memory.
> # This on .... [TRUNCATED] 

> library(tidyverse)

> library(R.utils)

> library(ids)

> library(duckdb)

> library(janitor)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(NULL) # unset from previous runs
