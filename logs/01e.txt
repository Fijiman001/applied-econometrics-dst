
> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m2.04GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m4.46GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m49.62MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.09GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m36.51GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m15.34GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [===========================--] [32m74.16GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [==============================] [32m1.33GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m57.08GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m48.38GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m26.54GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m23.90GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m17.26GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [=============================] [32m9.82GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m21.90GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m27.96GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 
[1] "/home/matthew/applied_repo/logs/01e.txt"
                                                     
> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m2.04GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m4.46GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m49.62MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.09GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m36.51GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m15.34GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [===========================--] [32m74.16GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [==============================] [32m1.33GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m57.08GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m48.38GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m26.54GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m23.90GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m17.26GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [=============================] [32m9.82GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m21.90GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m27.96GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 
[1] "/home/matthew/applied_repo/logs/01e.txt"

> # This file merges all our datasets together
> # AEMO data (all AEMO data was joined together in previous scripts)
> # Population data (for per-capi .... [TRUNCATED] 

> library(arrow)

> library(zoo)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(here::here("logs/10.txt"), split=TRUE)

> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m1.60GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m3.16GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m44.60MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.50GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m49.70GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m16.91GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [=======================------] [32m62.22GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [============================] [32m569.11MB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m63.09GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m66.83GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m25.42GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m16.07GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m22.65GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [============================] [32m11.44GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m18.18GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m33.37GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> # This file merges all our datasets together
> # AEMO data (all AEMO data was joined together in previous scripts)
> # Population data (for per-capi .... [TRUNCATED] 

> library(arrow)

> library(zoo)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(here::here("logs/10.txt"), split=TRUE)

> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m1.96GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m3.77GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m46.36MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m1.90GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m39.29GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m17.71GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [============================] [32m110.12GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1mindexing[0m [34mweather_hobart.csv[0m [==============================] [32m9.54GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m57.08GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m43.41GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m23.74GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m21.62GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m17.26GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [============================] [32m11.44GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m25.61GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m33.37GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> df <- df |>
+   left_join(midday_emissions, by=c("regionid", "date_local")) |>
+   mutate(
+     energy_wh_per_capita_vs_midday=(energy_kwh_per_capi .... [TRUNCATED] 

> # tidy up -----------------------------------------------------------------
> 
> 
> # our weather data, AEMO data etc
> # has slightly different end .... [TRUNCATED] 

> # Save output -------------------------------------------------------------
> # CSV for stata
> # parquet for the next R script
> 
> 
> write_csv(df .... [TRUNCATED] 
[1mwrote[0m [32m5.81MB[0m in [36m 0s[0m, [32m338.64GB/s[0m[1mwrote[0m [32m87.15MB[0m in [36m 0s[0m, [32m413.89MB/s[0m[1mwrote[0m [32m92.97MB[0m in [36m 0s[0m, [32m416.15MB/s[0m[1mwrote[0m [32m98.79MB[0m in [36m 0s[0m, [32m401.37MB/s[0m[1mwrote[0m [32m104.59MB[0m in [36m 0s[0m, [32m408.34MB/s[0m[1mwrote[0m [32m110.38MB[0m in [36m 0s[0m, [32m383.23MB/s[0m[1mwrote[0m [32m116.14MB[0m in [36m 0s[0m, [32m389.52MB/s[0m[1mwrote[0m [32m121.95MB[0m in [36m 0s[0m, [32m377.42MB/s[0m[1mwrote[0m [32m127.81MB[0m in [36m 0s[0m, [32m383.35MB/s[0m[1mwrote[0m [32m133.64MB[0m in [36m 0s[0m, [32m370.11MB/s[0m[1mwrote[0m [32m139.45MB[0m in [36m 0s[0m, [32m375.23MB/s[0m[1mwrote[0m [32m145.25MB[0m in [36m 0s[0m, [32m368.53MB/s[0m[1mwrote[0m [32m151.04MB[0m in [36m 0s[0m, [32m373.24MB/s[0m[1mwrote[0m [32m156.83MB[0m in [36m 0s[0m, [32m370.82MB/s[0m[1mwrote[0m [32m162.64MB[0m in [36m 0s[0m, [32m373.75MB/s[0m[1mwrote[0m [32m168.37MB[0m in [36m 0s[0m, [32m370.24MB/s[0m[1mwrote[0m [32m174.05MB[0m in [36m 0s[0m, [32m363.84MB/s[0m[1mwrote[0m [32m179.76MB[0m in [36m 0s[0m, [32m362.41MB/s[0m[1mwrote[0m [32m185.56MB[0m in [36m 1s[0m, [32m364.74MB/s[0m[1mwrote[0m [32m191.35MB[0m in [36m 1s[0m, [32m361.90MB/s[0m[1mwrote[0m [32m197.16MB[0m in [36m 1s[0m, [32m365.20MB/s[0m[1mwrote[0m [32m202.92MB[0m in [36m 1s[0m, [32m357.36MB/s[0m[1mwrote[0m [32m208.68MB[0m in [36m 1s[0m, [32m361.06MB/s[0m[1mwrote[0m [32m214.38MB[0m in [36m 1s[0m, [32m357.24MB/s[0m[1mwrote[0m [32m220.09MB[0m in [36m 1s[0m, [32m360.90MB/s[0m[1mwrote[0m [32m225.87MB[0m in [36m 1s[0m, [32m358.90MB/s[0m[1mwrote[0m [32m231.55MB[0m in [36m 1s[0m, [32m357.79MB/s[0m[1mwrote[0m [32m237.26MB[0m in [36m 1s[0m, [32m358.86MB/s[0m[1mwrote[0m [32m242.98MB[0m in [36m 1s[0m, [32m358.42MB/s[0m[1mwrote[0m [32m248.77MB[0m in [36m 1s[0m, [32m358.56MB/s[0m[1mwrote[0m [32m254.47MB[0m in [36m 1s[0m, [32m360.51MB/s[0m[1mwrote[0m [32m260.18MB[0m in [36m 1s[0m, [32m349.90MB/s[0m[1mwrote[0m [32m265.95MB[0m in [36m 1s[0m, [32m347.44MB/s[0m[1mwrote[0m [32m271.75MB[0m in [36m 1s[0m, [32m345.43MB/s[0m[1mwrote[0m [32m277.54MB[0m in [36m 1s[0m, [32m339.58MB/s[0m[1mwrote[0m [32m283.27MB[0m in [36m 1s[0m, [32m340.42MB/s[0m[1mwrote[0m [32m289.25MB[0m in [36m 1s[0m, [32m337.62MB/s[0m[1mwrote[0m [32m295.28MB[0m in [36m 1s[0m, [32m337.00MB/s[0m[1mwrote[0m [32m301.29MB[0m in [36m 1s[0m, [32m332.85MB/s[0m[1mwrote[0m [32m307.38MB[0m in [36m 1s[0m, [32m335.86MB/s[0m[1mwrote[0m [32m313.46MB[0m in [36m 1s[0m, [32m332.96MB/s[0m[1mwrote[0m [32m319.56MB[0m in [36m 1s[0m, [32m335.19MB/s[0m[1mwrote[0m [32m325.58MB[0m in [36m 1s[0m, [32m333.89MB/s[0m[1mwrote[0m [32m331.64MB[0m in [36m 1s[0m, [32m334.92MB/s[0m[1mwrote[0m [32m337.70MB[0m in [36m 1s[0m, [32m333.36MB/s[0m[1mwrote[0m [32m343.81MB[0m in [36m 1s[0m, [32m335.91MB/s[0m[1mwrote[0m [32m349.90MB[0m in [36m 1s[0m, [32m333.20MB/s[0m[1mwrote[0m [32m355.94MB[0m in [36m 1s[0m, [32m335.61MB/s[0m[1mwrote[0m [32m361.93MB[0m in [36m 1s[0m, [32m333.37MB/s[0m[1mwrote[0m [32m367.91MB[0m in [36m 1s[0m, [32m335.66MB/s[0m[1mwrote[0m [32m373.95MB[0m in [36m 1s[0m, [32m333.13MB/s[0m[1mwrote[0m [32m380.00MB[0m in [36m 2s[0m, [32m201.21MB/s[0m[1mwrote[0m [32m386.06MB[0m in [36m 2s[0m, [32m201.61MB/s[0m[1mwrote[0m [32m392.14MB[0m in [36m 2s[0m, [32m203.63MB/s[0m[1mwrote[0m [32m398.20MB[0m in [36m 2s[0m, [32m203.95MB/s[0m[1mwrote[0m [32m404.26MB[0m in [36m 2s[0m, [32m205.37MB/s[0m[1mwrote[0m [32m410.33MB[0m in [36m 2s[0m, [32m206.89MB/s[0m[1mwrote[0m [32m416.37MB[0m in [36m 2s[0m, [32m208.24MB/s[0m[1mwrote[0m [32m422.43MB[0m in [36m 2s[0m, [32m208.52MB/s[0m[1mwrote[0m [32m428.47MB[0m in [36m 2s[0m, [32m210.15MB/s[0m[1mwrote[0m [32m434.47MB[0m in [36m 2s[0m, [32m210.78MB/s[0m[1mwrote[0m [32m440.45MB[0m in [36m 2s[0m, [32m212.62MB/s[0m[1mwrote[0m [32m446.56MB[0m in [36m 2s[0m, [32m212.63MB/s[0m[1mwrote[0m [32m452.68MB[0m in [36m 2s[0m, [32m214.14MB/s[0m[1mwrote[0m [32m458.75MB[0m in [36m 2s[0m, [32m215.54MB/s[0m[1mwrote[0m [32m464.81MB[0m in [36m 2s[0m, [32m215.77MB/s[0m[1mwrote[0m [32m470.90MB[0m in [36m 2s[0m, [32m217.25MB/s[0m[1mwrote[0m [32m476.89MB[0m in [36m 2s[0m, [32m218.21MB/s[0m[1mwrote[0m [32m482.87MB[0m in [36m 2s[0m, [32m219.66MB/s[0m[1mwrote[0m [32m488.85MB[0m in [36m 2s[0m, [32m220.10MB/s[0m[1mwrote[0m [32m494.91MB[0m in [36m 2s[0m, [32m220.95MB/s[0m[1mwrote[0m [32m501.07MB[0m in [36m 2s[0m, [32m221.87MB/s[0m[1mwrote[0m [32m507.19MB[0m in [36m 2s[0m, [32m223.16MB/s[0m[1mwrote[0m [32m513.33MB[0m in [36m 2s[0m, [32m223.33MB/s[0m[1mwrote[0m [32m519.42MB[0m in [36m 2s[0m, [32m224.94MB/s[0m[1mwrote[0m [32m525.48MB[0m in [36m 2s[0m, [32m224.85MB/s[0m[1mwrote[0m [32m531.53MB[0m in [36m 2s[0m, [32m226.43MB/s[0m[1mwrote[0m [32m537.54MB[0m in [36m 2s[0m, [32m226.51MB/s[0m[1mwrote[0m [32m543.59MB[0m in [36m 2s[0m, [32m227.71MB/s[0m[1mwrote[0m [32m549.70MB[0m in [36m 2s[0m, [32m228.47MB/s[0m[1mwrote[0m [32m555.73MB[0m in [36m 2s[0m, [32m228.82MB/s[0m[1mwrote[0m [32m561.73MB[0m in [36m 2s[0m, [32m230.25MB/s[0m[1mwrote[0m [32m567.75MB[0m in [36m 2s[0m, [32m230.52MB/s[0m[1mwrote[0m [32m573.74MB[0m in [36m 2s[0m, [32m231.96MB/s[0m[1mwrote[0m [32m579.76MB[0m in [36m 3s[0m, [32m231.86MB/s[0m[1mwrote[0m [32m585.84MB[0m in [36m 3s[0m, [32m233.01MB/s[0m[1mwrote[0m [32m591.91MB[0m in [36m 3s[0m, [32m234.16MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 3s[0m, [32m395.53GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
# A tibble: 5 × 4
  regionid temperature population solar_exposure
  <chr>          <dbl>      <dbl>          <dbl>
1 NSW1               0          0              0
2 QLD1               0          0              0
3 SA1                0          0              0
4 TAS1               0          0              0
5 VIC1               0          0              0
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m2.07GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m4.46GB/s[0m, eta: [36m 0s[0m                                                                                                                  # A tibble: 1 × 1
       x
   <dbl>
1 0.0341
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m52.59MB/s[0m, eta: [36m 0s[0m                                                                                                                  # A tibble: 1 × 1
       x
   <dbl>
1 0.0341
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.20GB/s[0m, eta: [36m 0s[0m                                                                                                                  # A tibble: 5 × 2
  regionid      x
  <chr>     <dbl>
1 NSW1     0.0344
2 QLD1     0.0330
3 SA1      0.0355
4 TAS1     0.0317
5 VIC1     0.0359

> # This file merges all our datasets together
> # AEMO data (all AEMO data was joined together in previous scripts)
> # Population data (for per-capi .... [TRUNCATED] 

> library(arrow)

> library(zoo)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(here::here("logs/10.txt"), split=TRUE)

> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m1.48GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m3.77GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m49.62MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.50GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m46.88GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m20.96GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [=========================----] [32m75.01GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [============================] [32m933.70MB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m49.95GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m63.49GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m17.87GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m22.99GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m17.26GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [=============================] [32m8.95GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m22.73GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m36.70GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> df <- df |>
+   left_join(midday_emissions, by=c("regionid", "date_local")) |>
+   mutate(
+     energy_wh_per_capita_vs_midday=(energy_kwh_per_capi .... [TRUNCATED] 

> # tidy up -----------------------------------------------------------------
> 
> 
> # our weather data, AEMO data etc
> # has slightly different end .... [TRUNCATED] 

> # check data has no unexpected holes
> df |>
+   summarise(
+     temperature=mean(is.na(temperature)),
+     population=mean(is.na(population)),
+  .... [TRUNCATED] 
# A tibble: 1 × 3
  temperature population solar_exposure
        <dbl>      <dbl>          <dbl>
1           0          0              0

> # Save output -------------------------------------------------------------
> # CSV for stata
> # parquet for the next R script
> 
> 
> write_csv(df .... [TRUNCATED] 
[1mwrote[0m [32m5.83MB[0m in [36m 0s[0m, [32m344.16GB/s[0m[1mwrote[0m [32m81.47MB[0m in [36m 0s[0m, [32m392.69MB/s[0m[1mwrote[0m [32m87.34MB[0m in [36m 0s[0m, [32m391.94MB/s[0m[1mwrote[0m [32m93.18MB[0m in [36m 0s[0m, [32m394.78MB/s[0m[1mwrote[0m [32m99.00MB[0m in [36m 0s[0m, [32m379.10MB/s[0m[1mwrote[0m [32m104.81MB[0m in [36m 0s[0m, [32m385.95MB/s[0m[1mwrote[0m [32m110.62MB[0m in [36m 0s[0m, [32m374.95MB/s[0m[1mwrote[0m [32m116.39MB[0m in [36m 0s[0m, [32m377.10MB/s[0m[1mwrote[0m [32m122.22MB[0m in [36m 0s[0m, [32m375.60MB/s[0m[1mwrote[0m [32m128.08MB[0m in [36m 1s[0m, [32m201.47MB/s[0m[1mwrote[0m [32m133.93MB[0m in [36m 1s[0m, [32m201.38MB/s[0m[1mwrote[0m [32m139.75MB[0m in [36m 1s[0m, [32m206.68MB/s[0m[1mwrote[0m [32m145.56MB[0m in [36m 1s[0m, [32m206.61MB/s[0m[1mwrote[0m [32m151.37MB[0m in [36m 1s[0m, [32m211.88MB/s[0m[1mwrote[0m [32m157.17MB[0m in [36m 1s[0m, [32m212.96MB/s[0m[1mwrote[0m [32m162.99MB[0m in [36m 1s[0m, [32m216.62MB/s[0m[1mwrote[0m [32m168.73MB[0m in [36m 1s[0m, [32m217.83MB/s[0m[1mwrote[0m [32m174.43MB[0m in [36m 1s[0m, [32m222.33MB/s[0m[1mwrote[0m [32m180.15MB[0m in [36m 1s[0m, [32m222.79MB/s[0m[1mwrote[0m [32m185.96MB[0m in [36m 1s[0m, [32m227.11MB/s[0m[1mwrote[0m [32m191.77MB[0m in [36m 1s[0m, [32m227.02MB/s[0m[1mwrote[0m [32m197.59MB[0m in [36m 1s[0m, [32m231.10MB/s[0m[1mwrote[0m [32m203.36MB[0m in [36m 1s[0m, [32m230.73MB/s[0m[1mwrote[0m [32m209.14MB[0m in [36m 1s[0m, [32m234.64MB/s[0m[1mwrote[0m [32m214.85MB[0m in [36m 1s[0m, [32m235.60MB/s[0m[1mwrote[0m [32m220.57MB[0m in [36m 1s[0m, [32m239.32MB/s[0m[1mwrote[0m [32m226.36MB[0m in [36m 1s[0m, [32m238.47MB/s[0m[1mwrote[0m [32m232.05MB[0m in [36m 1s[0m, [32m241.96MB/s[0m[1mwrote[0m [32m237.77MB[0m in [36m 1s[0m, [32m242.66MB/s[0m[1mwrote[0m [32m243.51MB[0m in [36m 1s[0m, [32m245.78MB/s[0m[1mwrote[0m [32m249.31MB[0m in [36m 1s[0m, [32m245.02MB/s[0m[1mwrote[0m [32m255.03MB[0m in [36m 1s[0m, [32m248.19MB/s[0m[1mwrote[0m [32m260.75MB[0m in [36m 1s[0m, [32m247.73MB/s[0m[1mwrote[0m [32m266.54MB[0m in [36m 1s[0m, [32m250.83MB/s[0m[1mwrote[0m [32m272.34MB[0m in [36m 1s[0m, [32m250.40MB/s[0m[1mwrote[0m [32m278.15MB[0m in [36m 1s[0m, [32m253.46MB/s[0m[1mwrote[0m [32m283.89MB[0m in [36m 1s[0m, [32m253.15MB/s[0m[1mwrote[0m [32m289.88MB[0m in [36m 1s[0m, [32m256.12MB/s[0m[1mwrote[0m [32m295.93MB[0m in [36m 1s[0m, [32m254.97MB/s[0m[1mwrote[0m [32m301.95MB[0m in [36m 1s[0m, [32m257.81MB/s[0m[1mwrote[0m [32m308.05MB[0m in [36m 1s[0m, [32m255.75MB/s[0m[1mwrote[0m [32m314.15MB[0m in [36m 1s[0m, [32m258.56MB/s[0m[1mwrote[0m [32m320.26MB[0m in [36m 1s[0m, [32m257.46MB/s[0m[1mwrote[0m [32m326.29MB[0m in [36m 1s[0m, [32m260.11MB/s[0m[1mwrote[0m [32m332.36MB[0m in [36m 1s[0m, [32m259.17MB/s[0m[1mwrote[0m [32m338.43MB[0m in [36m 1s[0m, [32m261.48MB/s[0m[1mwrote[0m [32m344.55MB[0m in [36m 1s[0m, [32m261.48MB/s[0m[1mwrote[0m [32m350.66MB[0m in [36m 1s[0m, [32m263.18MB/s[0m[1mwrote[0m [32m356.71MB[0m in [36m 1s[0m, [32m264.55MB/s[0m[1mwrote[0m [32m362.71MB[0m in [36m 1s[0m, [32m263.07MB/s[0m[1mwrote[0m [32m368.71MB[0m in [36m 1s[0m, [32m265.24MB/s[0m[1mwrote[0m [32m374.76MB[0m in [36m 1s[0m, [32m265.40MB/s[0m[1mwrote[0m [32m380.83MB[0m in [36m 1s[0m, [32m267.37MB/s[0m[1mwrote[0m [32m386.89MB[0m in [36m 1s[0m, [32m266.59MB/s[0m[1mwrote[0m [32m392.98MB[0m in [36m 1s[0m, [32m268.82MB/s[0m[1mwrote[0m [32m399.05MB[0m in [36m 1s[0m, [32m268.76MB/s[0m[1mwrote[0m [32m405.13MB[0m in [36m 1s[0m, [32m270.24MB/s[0m[1mwrote[0m [32m411.21MB[0m in [36m 2s[0m, [32m269.72MB/s[0m[1mwrote[0m [32m417.27MB[0m in [36m 2s[0m, [32m271.84MB/s[0m[1mwrote[0m [32m423.34MB[0m in [36m 2s[0m, [32m270.31MB/s[0m[1mwrote[0m [32m429.39MB[0m in [36m 2s[0m, [32m191.15MB/s[0m[1mwrote[0m [32m435.40MB[0m in [36m 2s[0m, [32m192.03MB/s[0m[1mwrote[0m [32m441.40MB[0m in [36m 2s[0m, [32m193.29MB/s[0m[1mwrote[0m [32m447.52MB[0m in [36m 2s[0m, [32m194.23MB/s[0m[1mwrote[0m [32m453.65MB[0m in [36m 2s[0m, [32m195.68MB/s[0m[1mwrote[0m [32m459.73MB[0m in [36m 2s[0m, [32m196.33MB/s[0m[1mwrote[0m [32m465.81MB[0m in [36m 2s[0m, [32m197.64MB/s[0m[1mwrote[0m [32m471.91MB[0m in [36m 2s[0m, [32m198.81MB/s[0m[1mwrote[0m [32m477.91MB[0m in [36m 2s[0m, [32m200.13MB/s[0m[1mwrote[0m [32m483.91MB[0m in [36m 2s[0m, [32m200.68MB/s[0m[1mwrote[0m [32m489.90MB[0m in [36m 2s[0m, [32m202.30MB/s[0m[1mwrote[0m [32m495.98MB[0m in [36m 2s[0m, [32m202.27MB/s[0m[1mwrote[0m [32m502.14MB[0m in [36m 2s[0m, [32m203.79MB/s[0m[1mwrote[0m [32m508.28MB[0m in [36m 2s[0m, [32m203.97MB/s[0m[1mwrote[0m [32m514.43MB[0m in [36m 3s[0m, [32m204.08MB/s[0m[1mwrote[0m [32m520.53MB[0m in [36m 3s[0m, [32m203.34MB/s[0m[1mwrote[0m [32m526.61MB[0m in [36m 3s[0m, [32m203.22MB/s[0m[1mwrote[0m [32m532.67MB[0m in [36m 3s[0m, [32m204.71MB/s[0m[1mwrote[0m [32m538.69MB[0m in [36m 3s[0m, [32m204.87MB/s[0m[1mwrote[0m [32m544.75MB[0m in [36m 3s[0m, [32m206.40MB/s[0m[1mwrote[0m [32m550.88MB[0m in [36m 3s[0m, [32m205.61MB/s[0m[1mwrote[0m [32m556.92MB[0m in [36m 3s[0m, [32m207.11MB/s[0m[1mwrote[0m [32m562.93MB[0m in [36m 3s[0m, [32m207.38MB/s[0m[1mwrote[0m [32m568.96MB[0m in [36m 3s[0m, [32m208.85MB/s[0m[1mwrote[0m [32m574.97MB[0m in [36m 3s[0m, [32m209.01MB/s[0m[1mwrote[0m [32m581.00MB[0m in [36m 3s[0m, [32m210.46MB/s[0m[1mwrote[0m [32m587.09MB[0m in [36m 3s[0m, [32m210.68MB/s[0m[1mwrote[0m [32m593.17MB[0m in [36m 3s[0m, [32m212.36MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 3s[0m, [32m357.96GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
                               regionid 
                            0.000000000 
                             date_local 
                            0.000000000 
                             population 
                            0.000000000 
                               dst_date 
                            0.000000000 
                          dst_direction 
                            0.000000000 
                      dst_transition_id 
                            0.000000000 
                 days_before_transition 
                            0.000000000 
                  days_after_transition 
                            0.000000000 
                              dst_start 
                            0.000000000 
                          days_into_dst 
                            0.000000000 
                         public_holiday 
                            0.000000000 
               rooftop_solar_energy_mwh 
                            0.491619939 
                           hh_end_fixed 
                            0.000000000 
                           hh_end_local 
                            0.000000000 
                         hh_start_fixed 
                            0.000000000 
                         hh_start_local 
                            0.000000000 
                             date_fixed 
                            0.000000000 
                   midday_control_local 
                            0.000000000 
                   midday_control_fixed 
                            0.000000000 
               not_midday_control_local 
                            0.000000000 
               not_midday_control_fixed 
                            0.000000000 
                               hr_local 
                            0.000000000 
                               hr_fixed 
                            0.000000000 
                      day_of_week_local 
                            0.000000000 
                      day_of_week_fixed 
                            0.000000000 
                          weekend_local 
                            0.000000000 
                          weekend_fixed 
                            0.000000000 
                       after_transition 
                            0.000000000 
                       dst_now_anywhere 
                            0.000000000 
                       dst_here_anytime 
                            0.000000000 
                           dst_now_here 
                            0.000000000 
           dst_transition_id_and_region 
                            0.000000000 
          days_into_dst_extreme_outlier 
                            0.000000000 
                  days_into_dst_outlier 
                            0.000000000 
                            temperature 
                            0.000000000 
                         solar_exposure 
                            0.000000000 
                          wind_km_per_h 
                            0.002696049 
                      co2_kg_per_capita 
                            0.000000000 
                  energy_kwh_per_capita 
                            0.000000000 
energy_kwh_adj_rooftop_solar_per_capita 
                            0.491619939 
             total_renewables_today_twh 
                            0.000000000 
        total_renewables_today_twh_uigf 
                            0.000000000 
               co2_kg_per_capita_midday 
                            0.000000000 
           energy_kwh_per_capita_midday 
                            0.000000000 
         energy_wh_per_capita_vs_midday 
                            0.000000000 
             co2_g_per_capita_vs_midday 
                            0.000000000 
     regionid date_local population dst_date dst_direction dst_transition_id
[1,]        0          0          0        0             0                 0
     days_before_transition days_after_transition dst_start days_into_dst
[1,]                      0                     0         0             0
     public_holiday rooftop_solar_energy_mwh hh_end_fixed hh_end_local
[1,]              0                0.4916199            0            0
     hh_start_fixed hh_start_local date_fixed midday_control_local
[1,]              0              0          0                    0
     midday_control_fixed not_midday_control_local not_midday_control_fixed
[1,]                    0                        0                        0
     hr_local hr_fixed day_of_week_local day_of_week_fixed weekend_local
[1,]        0        0                 0                 0             0
     weekend_fixed after_transition dst_now_anywhere dst_here_anytime
[1,]             0                0                0                0
     dst_now_here dst_transition_id_and_region days_into_dst_extreme_outlier
[1,]            0                            0                             0
     days_into_dst_outlier temperature solar_exposure wind_km_per_h
[1,]                     0           0              0   0.002696049
     co2_kg_per_capita energy_kwh_per_capita
[1,]                 0                     0
     energy_kwh_adj_rooftop_solar_per_capita total_renewables_today_twh
[1,]                               0.4916199                          0
     total_renewables_today_twh_uigf co2_kg_per_capita_midday
[1,]                               0                        0
     energy_kwh_per_capita_midday energy_wh_per_capita_vs_midday
[1,]                            0                              0
     co2_g_per_capita_vs_midday
[1,]                          0
               rooftop_solar_energy_mwh 
                            0.491619939 
                          wind_km_per_h 
                            0.002696049 
energy_kwh_adj_rooftop_solar_per_capita 
                            0.491619939 
               rooftop_solar_energy_mwh 
                            0.491619939 
                          wind_km_per_h 
                            0.002696049 
energy_kwh_adj_rooftop_solar_per_capita 
                            0.491619939 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
                                   <NA> 
                                     NA 
 [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[37]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
wind_km_per_h 
  0.002696049 

> # This file merges all our datasets together
> # AEMO data (all AEMO data was joined together in previous scripts)
> # Population data (for per-capi .... [TRUNCATED] 

> library(arrow)

> library(zoo)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(here::here("logs/10.txt"), split=TRUE)

> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m1.85GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m4.16GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m44.04MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.84GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m51.57GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m16.17GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [========================-----] [32m74.75GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [============================] [32m926.26MB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m57.08GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m64.30GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m21.48GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m25.58GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m18.12GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [=============================] [32m9.32GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m21.64GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m31.92GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> wind <- wind |>
+   group_by(regionid) |>
+   mutate(wind_km_per_ .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> df <- df |>
+   left_join(midday_emissions, by=c("regionid", "date_local")) |>
+   mutate(
+     energy_wh_per_capita_vs_midday=(energy_kwh_per_capi .... [TRUNCATED] 

> # tidy up -----------------------------------------------------------------
> 
> 
> # our weather data, AEMO data etc
> # has slightly different end .... [TRUNCATED] 

> # check data has no unexpected holes
> # we know rooftop solar data is missing from 2016 onwards
> missing <- colMeans(is.na(df))

> missing <- missing[(missing > 0) & !grepl("rooftop", names(df))]

> # Save output -------------------------------------------------------------
> # CSV for stata
> # parquet for the next R script
> 
> 
> write_csv(df .... [TRUNCATED] 
[1mwrote[0m [32m5.83MB[0m in [36m 0s[0m, [32m344.16GB/s[0m[1mwrote[0m [32m75.64MB[0m in [36m 0s[0m, [32m377.77MB/s[0m[1mwrote[0m [32m81.47MB[0m in [36m 0s[0m, [32m380.65MB/s[0m[1mwrote[0m [32m87.34MB[0m in [36m 0s[0m, [32m373.32MB/s[0m[1mwrote[0m [32m93.18MB[0m in [36m 0s[0m, [32m378.25MB/s[0m[1mwrote[0m [32m99.00MB[0m in [36m 0s[0m, [32m378.02MB/s[0m[1mwrote[0m [32m104.81MB[0m in [36m 0s[0m, [32m375.26MB/s[0m[1mwrote[0m [32m110.62MB[0m in [36m 0s[0m, [32m366.49MB/s[0m[1mwrote[0m [32m116.39MB[0m in [36m 0s[0m, [32m366.50MB/s[0m[1mwrote[0m [32m122.22MB[0m in [36m 0s[0m, [32m365.37MB/s[0m[1mwrote[0m [32m128.08MB[0m in [36m 0s[0m, [32m363.31MB/s[0m[1mwrote[0m [32m133.93MB[0m in [36m 0s[0m, [32m364.70MB/s[0m[1mwrote[0m [32m139.75MB[0m in [36m 0s[0m, [32m353.86MB/s[0m[1mwrote[0m [32m145.56MB[0m in [36m 0s[0m, [32m358.13MB/s[0m[1mwrote[0m [32m151.37MB[0m in [36m 0s[0m, [32m355.74MB/s[0m[1mwrote[0m [32m157.17MB[0m in [36m 0s[0m, [32m356.36MB/s[0m[1mwrote[0m [32m162.99MB[0m in [36m 0s[0m, [32m356.49MB/s[0m[1mwrote[0m [32m168.73MB[0m in [36m 0s[0m, [32m354.73MB/s[0m[1mwrote[0m [32m174.43MB[0m in [36m 0s[0m, [32m353.95MB/s[0m[1mwrote[0m [32m180.15MB[0m in [36m 1s[0m, [32m358.34MB/s[0m[1mwrote[0m [32m185.96MB[0m in [36m 1s[0m, [32m352.83MB/s[0m[1mwrote[0m [32m191.77MB[0m in [36m 1s[0m, [32m357.11MB/s[0m[1mwrote[0m [32m197.59MB[0m in [36m 1s[0m, [32m349.98MB/s[0m[1mwrote[0m [32m203.36MB[0m in [36m 1s[0m, [32m353.93MB/s[0m[1mwrote[0m [32m209.14MB[0m in [36m 1s[0m, [32m349.17MB/s[0m[1mwrote[0m [32m214.85MB[0m in [36m 1s[0m, [32m352.80MB/s[0m[1mwrote[0m [32m220.57MB[0m in [36m 1s[0m, [32m347.99MB/s[0m[1mwrote[0m [32m226.36MB[0m in [36m 1s[0m, [32m351.49MB/s[0m[1mwrote[0m [32m232.05MB[0m in [36m 1s[0m, [32m347.10MB/s[0m[1mwrote[0m [32m237.77MB[0m in [36m 1s[0m, [32m350.36MB/s[0m[1mwrote[0m [32m243.51MB[0m in [36m 1s[0m, [32m345.77MB/s[0m[1mwrote[0m [32m249.31MB[0m in [36m 1s[0m, [32m348.96MB/s[0m[1mwrote[0m [32m255.03MB[0m in [36m 1s[0m, [32m343.11MB/s[0m[1mwrote[0m [32m260.75MB[0m in [36m 1s[0m, [32m341.90MB/s[0m[1mwrote[0m [32m266.54MB[0m in [36m 1s[0m, [32m341.61MB/s[0m[1mwrote[0m [32m272.34MB[0m in [36m 1s[0m, [32m343.55MB/s[0m[1mwrote[0m [32m278.15MB[0m in [36m 1s[0m, [32m336.89MB/s[0m[1mwrote[0m [32m283.89MB[0m in [36m 1s[0m, [32m338.29MB/s[0m[1mwrote[0m [32m289.88MB[0m in [36m 1s[0m, [32m336.38MB/s[0m[1mwrote[0m [32m295.93MB[0m in [36m 1s[0m, [32m335.46MB/s[0m[1mwrote[0m [32m301.95MB[0m in [36m 1s[0m, [32m336.57MB/s[0m[1mwrote[0m [32m308.05MB[0m in [36m 1s[0m, [32m336.77MB/s[0m[1mwrote[0m [32m314.15MB[0m in [36m 1s[0m, [32m333.46MB/s[0m[1mwrote[0m [32m320.26MB[0m in [36m 2s[0m, [32m183.56MB/s[0m[1mwrote[0m [32m326.29MB[0m in [36m 2s[0m, [32m184.02MB/s[0m[1mwrote[0m [32m332.36MB[0m in [36m 2s[0m, [32m186.12MB/s[0m[1mwrote[0m [32m338.43MB[0m in [36m 2s[0m, [32m187.98MB/s[0m[1mwrote[0m [32m344.55MB[0m in [36m 2s[0m, [32m188.46MB/s[0m[1mwrote[0m [32m350.66MB[0m in [36m 2s[0m, [32m190.70MB/s[0m[1mwrote[0m [32m356.71MB[0m in [36m 2s[0m, [32m191.06MB/s[0m[1mwrote[0m [32m362.71MB[0m in [36m 2s[0m, [32m192.71MB/s[0m[1mwrote[0m [32m368.71MB[0m in [36m 2s[0m, [32m194.58MB/s[0m[1mwrote[0m [32m374.76MB[0m in [36m 2s[0m, [32m195.28MB/s[0m[1mwrote[0m [32m380.83MB[0m in [36m 2s[0m, [32m197.37MB/s[0m[1mwrote[0m [32m386.89MB[0m in [36m 2s[0m, [32m197.81MB/s[0m[1mwrote[0m [32m392.98MB[0m in [36m 2s[0m, [32m199.85MB/s[0m[1mwrote[0m [32m399.05MB[0m in [36m 2s[0m, [32m200.26MB/s[0m[1mwrote[0m [32m405.13MB[0m in [36m 2s[0m, [32m202.24MB/s[0m[1mwrote[0m [32m411.21MB[0m in [36m 2s[0m, [32m202.58MB/s[0m[1mwrote[0m [32m417.27MB[0m in [36m 2s[0m, [32m204.51MB/s[0m[1mwrote[0m [32m423.34MB[0m in [36m 2s[0m, [32m204.62MB/s[0m[1mwrote[0m [32m429.39MB[0m in [36m 2s[0m, [32m206.40MB/s[0m[1mwrote[0m [32m435.40MB[0m in [36m 2s[0m, [32m206.48MB/s[0m[1mwrote[0m [32m441.40MB[0m in [36m 2s[0m, [32m208.06MB/s[0m[1mwrote[0m [32m447.52MB[0m in [36m 2s[0m, [32m208.50MB/s[0m[1mwrote[0m [32m453.65MB[0m in [36m 2s[0m, [32m210.31MB/s[0m[1mwrote[0m [32m459.73MB[0m in [36m 2s[0m, [32m210.42MB/s[0m[1mwrote[0m [32m465.81MB[0m in [36m 2s[0m, [32m212.18MB/s[0m[1mwrote[0m [32m471.91MB[0m in [36m 2s[0m, [32m212.57MB/s[0m[1mwrote[0m [32m477.91MB[0m in [36m 2s[0m, [32m213.68MB/s[0m[1mwrote[0m [32m483.91MB[0m in [36m 2s[0m, [32m215.12MB/s[0m[1mwrote[0m [32m489.90MB[0m in [36m 2s[0m, [32m215.94MB/s[0m[1mwrote[0m [32m495.98MB[0m in [36m 2s[0m, [32m216.20MB/s[0m[1mwrote[0m [32m502.14MB[0m in [36m 2s[0m, [32m217.86MB/s[0m[1mwrote[0m [32m508.28MB[0m in [36m 2s[0m, [32m217.80MB/s[0m[1mwrote[0m [32m514.43MB[0m in [36m 2s[0m, [32m219.44MB/s[0m[1mwrote[0m [32m520.53MB[0m in [36m 2s[0m, [32m220.07MB/s[0m[1mwrote[0m [32m526.61MB[0m in [36m 2s[0m, [32m221.36MB/s[0m[1mwrote[0m [32m532.67MB[0m in [36m 2s[0m, [32m221.70MB/s[0m[1mwrote[0m [32m538.69MB[0m in [36m 2s[0m, [32m222.39MB/s[0m[1mwrote[0m [32m544.75MB[0m in [36m 2s[0m, [32m223.32MB/s[0m[1mwrote[0m [32m550.88MB[0m in [36m 2s[0m, [32m224.19MB/s[0m[1mwrote[0m [32m556.92MB[0m in [36m 2s[0m, [32m224.57MB/s[0m[1mwrote[0m [32m562.93MB[0m in [36m 2s[0m, [32m226.09MB/s[0m[1mwrote[0m [32m568.96MB[0m in [36m 3s[0m, [32m226.56MB/s[0m[1mwrote[0m [32m574.97MB[0m in [36m 3s[0m, [32m227.70MB/s[0m[1mwrote[0m [32m581.00MB[0m in [36m 3s[0m, [32m228.20MB/s[0m[1mwrote[0m [32m587.09MB[0m in [36m 3s[0m, [32m229.12MB/s[0m[1mwrote[0m [32m593.17MB[0m in [36m 3s[0m, [32m230.27MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 3s[0m, [32m388.13GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
wind_km_per_h 
  0.002696049 

> # This file merges all our datasets together
> # AEMO data (all AEMO data was joined together in previous scripts)
> # Population data (for per-capi .... [TRUNCATED] 

> library(arrow)

> library(zoo)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(NULL) # unset from previous runs

> sink(here::here("logs/10.txt"), split=TRUE)

> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m1.94GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m3.16GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m37.88MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.36GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m44.84GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m18.84GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [======================-------] [32m49.93GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [============================] [32m535.50MB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m42.43GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m63.49GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m22.56GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m15.39GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m19.70GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [=============================] [32m9.41GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m24.24GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m36.70GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> wind <- wind |>
+   complete(regionid, date_local = seq(start_dat .... [TRUNCATED] 
   [1] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [11] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [21] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [31] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [41] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [51] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [61] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [71] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [81] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
  [91] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [101] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [111] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [121] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [131] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [141] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [151] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [161] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [171] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [181] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [191] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [201] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [211] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [221] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [231] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [241] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [251] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [261] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [271] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [281] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [291] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [301] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [311] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [321] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [331] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [341] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [351] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [361] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [371] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [381] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [391] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [401] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [411] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [421] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [431] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [441] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [451] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [461] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [471] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [481] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [491] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [501] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [511] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [521] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [531] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [541] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [551] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [561] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [571] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [581] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [591] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [601] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [611] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [621] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [631] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [641] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [651] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [661] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [671] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [681] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [691] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [701] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [711] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [721] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [731] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [741] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [751] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [761] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [771] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [781] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [791] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [801] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [811] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [821] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [831] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [841] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [851] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [861] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [871] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [881] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [891] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [901] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [911] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [921] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [931] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [941] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [951] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [961] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [971] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [981] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [991] "NSW1" "QLD1" "SA1"  "TAS1" "VIC1" "NSW1" "QLD1" "SA1"  "TAS1" "VIC1"
 [ reached getOption("max.print") -- omitted 26319 entries ]
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m38.64GB/s[0m, eta: [36m 0s[0m                                                                                                                  # A tibble: 27,390 × 3
# Groups:   regionid [5]
   regionid date_local wind_km_per_h
   <chr>    <date>             <dbl>
 1 NSW1     2009-07-01          17.3
 2 NSW1     2009-07-02          10.8
 3 NSW1     2009-07-03          25.2
 4 NSW1     2009-07-04          15.5
 5 NSW1     2009-07-05          12.6
 6 NSW1     2009-07-06          10.4
 7 NSW1     2009-07-07          19.8
 8 NSW1     2009-07-08          19.1
 9 NSW1     2009-07-09          18  
10 NSW1     2009-07-10          18.4
# ℹ 27,380 more rows
# ℹ Use `print(n = ...)` to see more rows
# A tibble: 27,390 × 3
   regionid date_local wind_km_per_h
   <chr>    <date>             <dbl>
 1 NSW1     2009-07-01          17.3
 2 NSW1     2009-07-02          10.8
 3 NSW1     2009-07-03          25.2
 4 NSW1     2009-07-04          15.5
 5 NSW1     2009-07-05          12.6
 6 NSW1     2009-07-06          10.4
 7 NSW1     2009-07-07          19.8
 8 NSW1     2009-07-08          19.1
 9 NSW1     2009-07-09          18  
10 NSW1     2009-07-10          18.4
# ℹ 27,380 more rows
# ℹ Use `print(n = ...)` to see more rows

> # This file merges all our datasets together
> # AEMO data (all AEMO data was joined together in previous scripts)
> # Population data (for per-capi .... [TRUNCATED] 

> library(arrow)

> library(zoo)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(NULL) # unset from previous runs

> sink(here::here("logs/10.txt"), split=TRUE)

> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m1.62GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m3.13GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   replace_na(list(public_holiday=FALSE .... [TRUNCATED] 

> # This file merges all our datasets together
> # AEMO data (all AEMO data was joined together in previous scripts)
> # Population data (for per-capi .... [TRUNCATED] 

> library(arrow)

> library(zoo)

> library(here)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(NULL) # unset from previous runs

> sink(here::here("logs/10.txt"), split=TRUE)

> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> temperature_dir <- file.path(data_dir, 'raw/weather')

> sunshine_dir <- file.path(data_dir, 'raw/sunshine')

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(file.path(data_dir, "01-F-aemo-joined ..." ... [TRUNCATED] 

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [=================] [32m2.07GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays_2 <- read_csv(file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")) |>
+   rename(State=Jurisdiction) |>
+ .... [TRUNCATED] 
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [] [32m3.81GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [===============================] [32m46.36MB/s[0m, eta: [36m 0s[0m                                                                                                                  
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(file.path(data_dir, "raw/p ..." ... [TRUNCATED] 
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [====================] [32m2.20GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [===========================] [32m46.88GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [===========================] [32m17.71GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [===========================--] [32m87.08GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [==============================] [32m2.04GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [==========================] [32m45.66GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [=============================] [32m52.37GB/s[0m, eta: [36m 0s[0m                                                                                                                  Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [===========================] [32m14.32GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [========================] [32m25.58GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=============================] [32m22.65GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [=============================] [32m8.95GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [==========================] [32m25.61GB/s[0m, eta: [36m 0s[0m                                                                                                                  [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [====================================] [32m27.96GB/s[0m, eta: [36m 0s[0m                                                                                                                  
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> wind <- wind |>
+   group_by(regionid) |>
+   complete(date_local .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> df <- df |>
+   left_join(midday_emissions, by=c("regionid", "date_local")) |>
+   mutate(
+     energy_wh_per_capita_vs_midday=(energy_kwh_per_capi .... [TRUNCATED] 

> # tidy up -----------------------------------------------------------------
> 
> 
> # our weather data, AEMO data etc
> # has slightly different end .... [TRUNCATED] 

> # missing data final check ------------------------------------------------
> 
> # check data has no unexpected holes
> # we know rooftop solar data .... [TRUNCATED] 

> missing <- missing[(missing > 0) & !grepl("rooftop", names(df))]

> stopifnot(length(missing) == 0)

> # Save output -------------------------------------------------------------
> # CSV for stata
> # parquet for the next R script
> 
> 
> write_csv(df .... [TRUNCATED] 
[1mwrote[0m [32m5.83MB[0m in [36m 0s[0m, [32m277.68GB/s[0m[1mwrote[0m [32m81.47MB[0m in [36m 0s[0m, [32m373.85MB/s[0m[1mwrote[0m [32m87.34MB[0m in [36m 0s[0m, [32m382.53MB/s[0m[1mwrote[0m [32m93.18MB[0m in [36m 0s[0m, [32m368.07MB/s[0m[1mwrote[0m [32m99.00MB[0m in [36m 0s[0m, [32m376.01MB/s[0m[1mwrote[0m [32m104.81MB[0m in [36m 0s[0m, [32m374.25MB/s[0m[1mwrote[0m [32m110.62MB[0m in [36m 0s[0m, [32m363.46MB/s[0m[1mwrote[0m [32m116.39MB[0m in [36m 0s[0m, [32m370.94MB/s[0m[1mwrote[0m [32m122.22MB[0m in [36m 0s[0m, [32m360.28MB/s[0m[1mwrote[0m [32m128.09MB[0m in [36m 0s[0m, [32m366.67MB/s[0m[1mwrote[0m [32m133.93MB[0m in [36m 0s[0m, [32m363.09MB/s[0m[1mwrote[0m [32m139.75MB[0m in [36m 0s[0m, [32m365.40MB/s[0m[1mwrote[0m [32m145.56MB[0m in [36m 0s[0m, [32m361.84MB/s[0m[1mwrote[0m [32m151.37MB[0m in [36m 0s[0m, [32m363.41MB/s[0m[1mwrote[0m [32m157.17MB[0m in [36m 0s[0m, [32m360.99MB/s[0m[1mwrote[0m [32m162.99MB[0m in [36m 0s[0m, [32m366.09MB/s[0m[1mwrote[0m [32m168.73MB[0m in [36m 0s[0m, [32m360.33MB/s[0m[1mwrote[0m [32m174.43MB[0m in [36m 0s[0m, [32m362.92MB/s[0m[1mwrote[0m [32m180.15MB[0m in [36m 0s[0m, [32m363.39MB/s[0m[1mwrote[0m [32m185.96MB[0m in [36m 1s[0m, [32m360.71MB/s[0m[1mwrote[0m [32m191.77MB[0m in [36m 1s[0m, [32m365.33MB/s[0m[1mwrote[0m [32m197.59MB[0m in [36m 1s[0m, [32m359.85MB/s[0m[1mwrote[0m [32m203.36MB[0m in [36m 1s[0m, [32m361.94MB/s[0m[1mwrote[0m [32m209.14MB[0m in [36m 1s[0m, [32m358.75MB/s[0m[1mwrote[0m [32m214.86MB[0m in [36m 1s[0m, [32m356.22MB/s[0m[1mwrote[0m [32m220.58MB[0m in [36m 1s[0m, [32m352.76MB/s[0m[1mwrote[0m [32m226.37MB[0m in [36m 1s[0m, [32m350.90MB/s[0m[1mwrote[0m [32m232.06MB[0m in [36m 1s[0m, [32m352.29MB/s[0m[1mwrote[0m [32m237.78MB[0m in [36m 1s[0m, [32m349.32MB/s[0m[1mwrote[0m [32m243.52MB[0m in [36m 1s[0m, [32m351.67MB/s[0m[1mwrote[0m [32m249.32MB[0m in [36m 1s[0m, [32m346.36MB/s[0m[1mwrote[0m [32m255.04MB[0m in [36m 1s[0m, [32m349.60MB/s[0m[1mwrote[0m [32m260.76MB[0m in [36m 1s[0m, [32m344.09MB/s[0m[1mwrote[0m [32m266.55MB[0m in [36m 1s[0m, [32m343.46MB/s[0m[1mwrote[0m [32m272.35MB[0m in [36m 1s[0m, [32m345.45MB/s[0m[1mwrote[0m [32m278.16MB[0m in [36m 1s[0m, [32m341.66MB/s[0m[1mwrote[0m [32m283.90MB[0m in [36m 1s[0m, [32m342.88MB/s[0m[1mwrote[0m [32m289.89MB[0m in [36m 1s[0m, [32m338.11MB/s[0m[1mwrote[0m [32m295.94MB[0m in [36m 1s[0m, [32m340.91MB/s[0m[1mwrote[0m [32m301.96MB[0m in [36m 1s[0m, [32m335.48MB/s[0m[1mwrote[0m [32m308.06MB[0m in [36m 1s[0m, [32m336.71MB/s[0m[1mwrote[0m [32m314.16MB[0m in [36m 1s[0m, [32m336.01MB/s[0m[1mwrote[0m [32m320.27MB[0m in [36m 1s[0m, [32m333.66MB/s[0m[1mwrote[0m [32m326.30MB[0m in [36m 1s[0m, [32m333.72MB/s[0m[1mwrote[0m [32m332.38MB[0m in [36m 1s[0m, [32m335.72MB/s[0m[1mwrote[0m [32m338.45MB[0m in [36m 1s[0m, [32m332.17MB/s[0m[1mwrote[0m [32m344.57MB[0m in [36m 1s[0m, [32m332.78MB/s[0m[1mwrote[0m [32m350.68MB[0m in [36m 1s[0m, [32m331.09MB/s[0m[1mwrote[0m [32m356.73MB[0m in [36m 2s[0m, [32m203.26MB/s[0m[1mwrote[0m [32m362.73MB[0m in [36m 2s[0m, [32m202.42MB/s[0m[1mwrote[0m [32m368.73MB[0m in [36m 2s[0m, [32m204.61MB/s[0m[1mwrote[0m [32m374.77MB[0m in [36m 2s[0m, [32m204.15MB/s[0m[1mwrote[0m [32m380.84MB[0m in [36m 2s[0m, [32m206.10MB/s[0m[1mwrote[0m [32m386.92MB[0m in [36m 2s[0m, [32m206.67MB/s[0m[1mwrote[0m [32m393.01MB[0m in [36m 2s[0m, [32m208.13MB/s[0m[1mwrote[0m [32m399.08MB[0m in [36m 2s[0m, [32m208.94MB/s[0m[1mwrote[0m [32m405.15MB[0m in [36m 2s[0m, [32m211.01MB/s[0m[1mwrote[0m [32m411.24MB[0m in [36m 2s[0m, [32m211.31MB/s[0m[1mwrote[0m [32m417.30MB[0m in [36m 2s[0m, [32m213.09MB/s[0m[1mwrote[0m [32m423.37MB[0m in [36m 2s[0m, [32m213.98MB/s[0m[1mwrote[0m [32m429.42MB[0m in [36m 2s[0m, [32m215.43MB/s[0m[1mwrote[0m [32m435.43MB[0m in [36m 2s[0m, [32m216.23MB/s[0m[1mwrote[0m [32m441.43MB[0m in [36m 2s[0m, [32m217.89MB/s[0m[1mwrote[0m [32m447.55MB[0m in [36m 2s[0m, [32m218.39MB/s[0m[1mwrote[0m [32m453.68MB[0m in [36m 2s[0m, [32m219.95MB/s[0m[1mwrote[0m [32m459.76MB[0m in [36m 2s[0m, [32m220.27MB/s[0m[1mwrote[0m [32m465.84MB[0m in [36m 2s[0m, [32m221.79MB/s[0m[1mwrote[0m [32m471.94MB[0m in [36m 2s[0m, [32m222.24MB/s[0m[1mwrote[0m [32m477.94MB[0m in [36m 2s[0m, [32m224.01MB/s[0m[1mwrote[0m [32m483.94MB[0m in [36m 2s[0m, [32m223.74MB/s[0m[1mwrote[0m [32m489.93MB[0m in [36m 2s[0m, [32m225.49MB/s[0m[1mwrote[0m [32m496.01MB[0m in [36m 2s[0m, [32m225.52MB/s[0m[1mwrote[0m [32m502.18MB[0m in [36m 2s[0m, [32m226.79MB/s[0m[1mwrote[0m [32m508.31MB[0m in [36m 2s[0m, [32m227.60MB/s[0m[1mwrote[0m [32m514.46MB[0m in [36m 2s[0m, [32m228.45MB/s[0m[1mwrote[0m [32m520.57MB[0m in [36m 2s[0m, [32m229.51MB/s[0m[1mwrote[0m [32m526.64MB[0m in [36m 2s[0m, [32m230.95MB/s[0m[1mwrote[0m [32m532.71MB[0m in [36m 2s[0m, [32m231.09MB/s[0m[1mwrote[0m [32m538.72MB[0m in [36m 2s[0m, [32m232.40MB/s[0m[1mwrote[0m [32m544.79MB[0m in [36m 2s[0m, [32m232.16MB/s[0m[1mwrote[0m [32m550.91MB[0m in [36m 2s[0m, [32m233.69MB/s[0m[1mwrote[0m [32m556.95MB[0m in [36m 2s[0m, [32m233.79MB/s[0m[1mwrote[0m [32m562.97MB[0m in [36m 2s[0m, [32m234.88MB/s[0m[1mwrote[0m [32m569.00MB[0m in [36m 2s[0m, [32m235.91MB/s[0m[1mwrote[0m [32m575.00MB[0m in [36m 2s[0m, [32m235.95MB/s[0m[1mwrote[0m [32m581.04MB[0m in [36m 2s[0m, [32m237.31MB/s[0m[1mwrote[0m [32m587.13MB[0m in [36m 2s[0m, [32m238.08MB/s[0m[1mwrote[0m [32m593.21MB[0m in [36m 2s[0m, [32m239.15MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 2s[0m, [32m403.09GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
er_h, na.rm =  .... [TRUNCATED] 

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> wind <- wind |>
+   group_by(regionid) |>
+   complete(date_local .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> df <- df |>
+   left_join(midday_emissions, by=c("regionid", "date_local")) |>
+   mutate(
+     energy_wh_per_capita_vs_midday=(energy_kwh_per_capi .... [TRUNCATED] 

> # tidy up -----------------------------------------------------------------
> 
> 
> # our weather data, AEMO data etc
> # has slightly different end .... [TRUNCATED] 

> # missing data final check ------------------------------------------------
> 
> # check data has no unexpected holes
> # we know rooftop solar data .... [TRUNCATED] 

> missing <- missing[(missing > 0) & !grepl("rooftop", names(df))]

> stopifnot(length(missing) == 0)

> # Save output -------------------------------------------------------------
> # CSV for stata
> # parquet for the next R script
> 
> 
> write_csv(df .... [TRUNCATED] 
[1mwrote[0m [32m5.83MB[0m in [36m 0s[0m, [32m277.68GB/s[0m[1mwrote[0m [32m81.47MB[0m in [36m 0s[0m, [32m373.85MB/s[0m[1mwrote[0m [32m87.34MB[0m in [36m 0s[0m, [32m382.53MB/s[0m[1mwrote[0m [32m93.18MB[0m in [36m 0s[0m, [32m368.07MB/s[0m[1mwrote[0m [32m99.00MB[0m in [36m 0s[0m, [32m376.01MB/s[0m[1mwrote[0m [32m104.81MB[0m in [36m 0s[0m, [32m374.25MB/s[0m[1mwrote[0m [32m110.62MB[0m in [36m 0s[0m, [32m363.46MB/s[0m[1mwrote[0m [32m116.39MB[0m in [36m 0s[0m, [32m370.94MB/s[0m[1mwrote[0m [32m122.22MB[0m in [36m 0s[0m, [32m360.28MB/s[0m[1mwrote[0m [32m128.09MB[0m in [36m 0s[0m, [32m366.67MB/s[0m[1mwrote[0m [32m133.93MB[0m in [36m 0s[0m, [32m363.09MB/s[0m[1mwrote[0m [32m139.75MB[0m in [36m 0s[0m, [32m365.40MB/s[0m[1mwrote[0m [32m145.56MB[0m in [36m 0s[0m, [32m361.84MB/s[0m[1mwrote[0m [32m151.37MB[0m in [36m 0s[0m, [32m363.41MB/s[0m[1mwrote[0m [32m157.17MB[0m in [36m 0s[0m, [32m360.99MB/s[0m[1mwrote[0m [32m162.99MB[0m in [36m 0s[0m, [32m366.09MB/s[0m[1mwrote[0m [32m168.73MB[0m in [36m 0s[0m, [32m360.33MB/s[0m[1mwrote[0m [32m174.43MB[0m in [36m 0s[0m, [32m362.92MB/s[0m[1mwrote[0m [32m180.15MB[0m in [36m 0s[0m, [32m363.39MB/s[0m[1mwrote[0m [32m185.96MB[0m in [36m 1s[0m, [32m360.71MB/s[0m[1mwrote[0m [32m191.77MB[0m in [36m 1s[0m, [32m365.33MB/s[0m[1mwrote[0m [32m197.59MB[0m in [36m 1s[0m, [32m359.85MB/s[0m[1mwrote[0m [32m203.36MB[0m in [36m 1s[0m, [32m361.94MB/s[0m[1mwrote[0m [32m209.14MB[0m in [36m 1s[0m, [32m358.75MB/s[0m[1mwrote[0m [32m214.86MB[0m in [36m 1s[0m, [32m356.22MB/s[0m[1mwrote[0m [32m220.58MB[0m in [36m 1s[0m, [32m352.76MB/s[0m[1mwrote[0m [32m226.37MB[0m in [36m 1s[0m, [32m350.90MB/s[0m[1mwrote[0m [32m232.06MB[0m in [36m 1s[0m, [32m352.29MB/s[0m[1mwrote[0m [32m237.78MB[0m in [36m 1s[0m, [32m349.32MB/s[0m[1mwrote[0m [32m243.52MB[0m in [36m 1s[0m, [32m351.67MB/s[0m[1mwrote[0m [32m249.32MB[0m in [36m 1s[0m, [32m346.36MB/s[0m[1mwrote[0m [32m255.04MB[0m in [36m 1s[0m, [32m349.60MB/s[0m[1mwrote[0m [32m260.76MB[0m in [36m 1s[0m, [32m344.09MB/s[0m[1mwrote[0m [32m266.55MB[0m in [36m 1s[0m, [32m343.46MB/s[0m[1mwrote[0m [32m272.35MB[0m in [36m 1s[0m, [32m345.45MB/s[0m[1mwrote[0m [32m278.16MB[0m in [36m 1s[0m, [32m341.66MB/s[0m[1mwrote[0m [32m283.90MB[0m in [36m 1s[0m, [32m342.88MB/s[0m[1mwrote[0m [32m289.89MB[0m in [36m 1s[0m, [32m338.11MB/s[0m[1mwrote[0m [32m295.94MB[0m in [36m 1s[0m, [32m340.91MB/s[0m[1mwrote[0m [32m301.96MB[0m in [36m 1s[0m, [32m335.48MB/s[0m[1mwrote[0m [32m308.06MB[0m in [36m 1s[0m, [32m336.71MB/s[0m[1mwrote[0m [32m314.16MB[0m in [36m 1s[0m, [32m336.01MB/s[0m[1mwrote[0m [32m320.27MB[0m in [36m 1s[0m, [32m333.66MB/s[0m[1mwrote[0m [32m326.30MB[0m in [36m 1s[0m, [32m333.72MB/s[0m[1mwrote[0m [32m332.38MB[0m in [36m 1s[0m, [32m335.72MB/s[0m[1mwrote[0m [32m338.45MB[0m in [36m 1s[0m, [32m332.17MB/s[0m[1mwrote[0m [32m344.57MB[0m in [36m 1s[0m, [32m332.78MB/s[0m[1mwrote[0m [32m350.68MB[0m in [36m 1s[0m, [32m331.09MB/s[0m[1mwrote[0m [32m356.73MB[0m in [36m 2s[0m, [32m203.26MB/s[0m[1mwrote[0m [32m362.73MB[0m in [36m 2s[0m, [32m202.42MB/s[0m[1mwrote[0m [32m368.73MB[0m in [36m 2s[0m, [32m204.61MB/s[0m[1mwrote[0m [32m374.77MB[0m in [36m 2s[0m, [32m204.15MB/s[0m[1mwrote[0m [32m380.84MB[0m in [36m 2s[0m, [32m206.10MB/s[0m[1mwrote[0m [32m386.92MB[0m in [36m 2s[0m, [32m206.67MB/s[0m[1mwrote[0m [32m393.01MB[0m in [36m 2s[0m, [32m208.13MB/s[0m[1mwrote[0m [32m399.08MB[0m in [36m 2s[0m, [32m208.94MB/s[0m[1mwrote[0m [32m405.15MB[0m in [36m 2s[0m, [32m211.01MB/s[0m[1mwrote[0m [32m411.24MB[0m in [36m 2s[0m, [32m211.31MB/s[0m[1mwrote[0m [32m417.30MB[0m in [36m 2s[0m, [32m213.09MB/s[0m[1mwrote[0m [32m423.37MB[0m in [36m 2s[0m, [32m213.98MB/s[0m[1mwrote[0m [32m429.42MB[0m in [36m 2s[0m, [32m215.43MB/s[0m[1mwrote[0m [32m435.43MB[0m in [36m 2s[0m, [32m216.23MB/s[0m[1mwrote[0m [32m441.43MB[0m in [36m 2s[0m, [32m217.89MB/s[0m[1mwrote[0m [32m447.55MB[0m in [36m 2s[0m, [32m218.39MB/s[0m[1mwrote[0m [32m453.68MB[0m in [36m 2s[0m, [32m219.95MB/s[0m[1mwrote[0m [32m459.76MB[0m in [36m 2s[0m, [32m220.27MB/s[0m[1mwrote[0m [32m465.84MB[0m in [36m 2s[0m, [32m221.79MB/s[0m[1mwrote[0m [32m471.94MB[0m in [36m 2s[0m, [32m222.24MB/s[0m[1mwrote[0m [32m477.94MB[0m in [36m 2s[0m, [32m224.01MB/s[0m[1mwrote[0m [32m483.94MB[0m in [36m 2s[0m, [32m223.74MB/s[0m[1mwrote[0m [32m489.93MB[0m in [36m 2s[0m, [32m225.49MB/s[0m[1mwrote[0m [32m496.01MB[0m in [36m 2s[0m, [32m225.52MB/s[0m[1mwrote[0m [32m502.18MB[0m in [36m 2s[0m, [32m226.79MB/s[0m[1mwrote[0m [32m508.31MB[0m in [36m 2s[0m, [32m227.60MB/s[0m[1mwrote[0m [32m514.46MB[0m in [36m 2s[0m, [32m228.45MB/s[0m[1mwrote[0m [32m520.57MB[0m in [36m 2s[0m, [32m229.51MB/s[0m[1mwrote[0m [32m526.64MB[0m in [36m 2s[0m, [32m230.95MB/s[0m[1mwrote[0m [32m532.71MB[0m in [36m 2s[0m, [32m231.09MB/s[0m[1mwrote[0m [32m538.72MB[0m in [36m 2s[0m, [32m232.40MB/s[0m[1mwrote[0m [32m544.79MB[0m in [36m 2s[0m, [32m232.16MB/s[0m[1mwrote[0m [32m550.91MB[0m in [36m 2s[0m, [32m233.69MB/s[0m[1mwrote[0m [32m556.95MB[0m in [36m 2s[0m, [32m233.79MB/s[0m[1mwrote[0m [32m562.97MB[0m in [36m 2s[0m, [32m234.88MB/s[0m[1mwrote[0m [32m569.00MB[0m in [36m 2s[0m, [32m235.91MB/s[0m[1mwrote[0m [32m575.00MB[0m in [36m 2s[0m, [32m235.95MB/s[0m[1mwrote[0m [32m581.04MB[0m in [36m 2s[0m, [32m237.31MB/s[0m[1mwrote[0m [32m587.13MB[0m in [36m 2s[0m, [32m238.08MB/s[0m[1mwrote[0m [32m593.21MB[0m in [36m 2s[0m, [32m239.15MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 2s[0m, [32m403.09GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
2.71MB[0m in [36m 2s[0m, [32m231.09MB/s[0m[1mwrote[0m [32m538.72MB[0m in [36m 2s[0m, [32m232.40MB/s[0m[1mwrote[0m [32m544.79MB[0m in [36m 2s[0m, [32m232.16MB/s[0m[1mwrote[0m [32m550.91MB[0m in [36m 2s[0m, [32m233.69MB/s[0m[1mwrote[0m [32m556.95MB[0m in [36m 2s[0m, [32m233.79MB/s[0m[1mwrote[0m [32m562.97MB[0m in [36m 2s[0m, [32m234.88MB/s[0m[1mwrote[0m [32m569.00MB[0m in [36m 2s[0m, [32m235.91MB/s[0m[1mwrote[0m [32m575.00MB[0m in [36m 2s[0m, [32m235.95MB/s[0m[1mwrote[0m [32m581.04MB[0m in [36m 2s[0m, [32m237.31MB/s[0m[1mwrote[0m [32m587.13MB[0m in [36m 2s[0m, [32m238.08MB/s[0m[1mwrote[0m [32m593.21MB[0m in [36m 2s[0m, [32m239.15MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 2s[0m, [32m403.09GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
2.71MB[0m in [36m 2s[0m, [32m231.09MB/s[0m[1mwrote[0m [32m538.72MB[0m in [36m 2s[0m, [32m232.40MB/s[0m[1mwrote[0m [32m544.79MB[0m in [36m 2s[0m, [32m232.16MB/s[0m[1mwrote[0m [32m550.91MB[0m in [36m 2s[0m, [32m233.69MB/s[0m[1mwrote[0m [32m556.95MB[0m in [36m 2s[0m, [32m233.79MB/s[0m[1mwrote[0m [32m562.97MB[0m in [36m 2s[0m, [32m234.88MB/s[0m[1mwrote[0m [32m569.00MB[0m in [36m 2s[0m, [32m235.91MB/s[0m[1mwrote[0m [32m575.00MB[0m in [36m 2s[0m, [32m235.95MB/s[0m[1mwrote[0m [32m581.04MB[0m in [36m 2s[0m, [32m237.31MB/s[0m[1mwrote[0m [32m587.13MB[0m in [36m 2s[0m, [32m238.08MB/s[0m[1mwrote[0m [32m593.21MB[0m in [36m 2s[0m, [32m239.15MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 2s[0m, [32m403.09GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
[0m[1mwrote[0m [32m411.24MB[0m in [36m 2s[0m, [32m211.31MB/s[0m[1mwrote[0m [32m417.30MB[0m in [36m 2s[0m, [32m213.09MB/s[0m[1mwrote[0m [32m423.37MB[0m in [36m 2s[0m, [32m213.98MB/s[0m[1mwrote[0m [32m429.42MB[0m in [36m 2s[0m, [32m215.43MB/s[0m[1mwrote[0m [32m435.43MB[0m in [36m 2s[0m, [32m216.23MB/s[0m[1mwrote[0m [32m441.43MB[0m in [36m 2s[0m, [32m217.89MB/s[0m[1mwrote[0m [32m447.55MB[0m in [36m 2s[0m, [32m218.39MB/s[0m[1mwrote[0m [32m453.68MB[0m in [36m 2s[0m, [32m219.95MB/s[0m[1mwrote[0m [32m459.76MB[0m in [36m 2s[0m, [32m220.27MB/s[0m[1mwrote[0m [32m465.84MB[0m in [36m 2s[0m, [32m221.79MB/s[0m[1mwrote[0m [32m471.94MB[0m in [36m 2s[0m, [32m222.24MB/s[0m[1mwrote[0m [32m477.94MB[0m in [36m 2s[0m, [32m224.01MB/s[0m[1mwrote[0m [32m483.94MB[0m in [36m 2s[0m, [32m223.74MB/s[0m[1mwrote[0m [32m489.93MB[0m in [36m 2s[0m, [32m225.49MB/s[0m[1mwrote[0m [32m496.01MB[0m in [36m 2s[0m, [32m225.52MB/s[0m[1mwrote[0m [32m502.18MB[0m in [36m 2s[0m, [32m226.79MB/s[0m[1mwrote[0m [32m508.31MB[0m in [36m 2s[0m, [32m227.60MB/s[0m[1mwrote[0m [32m514.46MB[0m in [36m 2s[0m, [32m228.45MB/s[0m[1mwrote[0m [32m520.57MB[0m in [36m 2s[0m, [32m229.51MB/s[0m[1mwrote[0m [32m526.64MB[0m in [36m 2s[0m, [32m230.95MB/s[0m[1mwrote[0m [32m532.71MB[0m in [36m 2s[0m, [32m231.09MB/s[0m[1mwrote[0m [32m538.72MB[0m in [36m 2s[0m, [32m232.40MB/s[0m[1mwrote[0m [32m544.79MB[0m in [36m 2s[0m, [32m232.16MB/s[0m[1mwrote[0m [32m550.91MB[0m in [36m 2s[0m, [32m233.69MB/s[0m[1mwrote[0m [32m556.95MB[0m in [36m 2s[0m, [32m233.79MB/s[0m[1mwrote[0m [32m562.97MB[0m in [36m 2s[0m, [32m234.88MB/s[0m[1mwrote[0m [32m569.00MB[0m in [36m 2s[0m, [32m235.91MB/s[0m[1mwrote[0m [32m575.00MB[0m in [36m 2s[0m, [32m235.95MB/s[0m[1mwrote[0m [32m581.04MB[0m in [36m 2s[0m, [32m237.31MB/s[0m[1mwrote[0m [32m587.13MB[0m in [36m 2s[0m, [32m238.08MB/s[0m[1mwrote[0m [32m593.21MB[0m in [36m 2s[0m, [32m239.15MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 2s[0m, [32m403.09GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
m in [36m 2s[0m, [32m234.88MB/s[0m[1mwrote[0m [32m569.00MB[0m in [36m 2s[0m, [32m235.91MB/s[0m[1mwrote[0m [32m575.00MB[0m in [36m 2s[0m, [32m235.95MB/s[0m[1mwrote[0m [32m581.04MB[0m in [36m 2s[0m, [32m237.31MB/s[0m[1mwrote[0m [32m587.13MB[0m in [36m 2s[0m, [32m238.08MB/s[0m[1mwrote[0m [32m593.21MB[0m in [36m 2s[0m, [32m239.15MB/s[0m                                                                             [1mwrote[0m [32m1.00TB[0m in [36m 2s[0m, [32m403.09GB/s[0m                                                                             
> write_parquet(df, sink = file.path(data_dir, "10-half-hourly.parquet"))
