
> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> # source data
> pub_hol_path_1 <- file.path(data_dir, "raw/holidays/Aus_public_hols_2009-2022-1.csv")

> pub_hol_path_2 <- file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")

> dst_transitions_path <- file.path(data_dir, 'snapshot/02-dst-dates.csv')

> population_path <- file.path(data_dir, "raw/population/population-australia-raw.csv")

> temperature_dir <- file.path(data_dir, "raw/weather")

> sunshine_dir <- file.path(data_dir, "raw/sunshine")

> wind_path <- file.path(data_dir, "snapshot/05-wind.csv")

> aemo_pq_path <- file.path(data_dir, "snapshot/01-F-aemo-joined-all.parquet")

> output_file_path_hh_csv <- file.path(data_dir, "04-half-hourly.csv")

> output_file_path_hh_pq <- file.path(data_dir, "04-half-hourly.parquet")

> output_file_path_daily <- file.path(data_dir, "04-energy-daily.csv")

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ='UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # unit converstion constants
> # We don't want magic numbers later in the code
> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> kg_per_g <- 1/g_per_kg

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_wh <- 1/wh_per_kwh

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo'= 'VIC1',
+   'hobart' = 'TAS1' .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data --------------------------------------------------------
> 
> energy <- read_parquet(aemo_pq_path)

> # Local time, midday control and other time info --------------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to loca .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by=c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone = "A ..." ... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [==========================] [32m2.04GB/s[0m, eta: [36m 0s[0m                                                                                                                           
> holidays_2 <- read_csv(pub_hol_path_2) |>
+   rename(State=Jurisdiction) |>
+   select(Date, State) |>
+   mutate(
+     Date=ymd(Date)
+   )
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [========] [32m3.30GB/s[0m, eta: [36m 0s[0m                                                                                                                           
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> df <- holidays |>
+   rename(date_local=Date) |>
+   mutate(public_holiday=TRUE) |>
+   right_join(df, by=c("date_local", "regionid")) |> 
+   repla .... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [========================================] [32m49.62MB/s[0m, eta: [36m 0s[0m                                                                                                                           
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local=d) |>
+   right_join(df, by="date_local") |>
+   mutate(
+   .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n=n(), .by=days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> pull(n) |> abs() |> median()

> outlier_days <- samples_per_days_into_dst |> filter(abs(n) < typical_sample_count) |> pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(population_path)
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [=============================] [32m2.09GB/s[0m, eta: [36m 0s[0m                                                                                                                           
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <- c("Date", "NSW1", "VIC1", "QLD1", "SA1", "WA1", "TAS1", "NT1", "ACT1","AUS")

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> pivot_longer(cols = -Date, names_to = "regionid", values_to = "popu ..." ... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local=Date) |>
+   right_join(df, by=c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> for (file_name in list.files(temperature_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_temperature[[length(all_temperature) + 1]] <- clean .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [====================================] [32m49.11GB/s[0m, eta: [36m 0s[0m                                                                                                                           Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [====================================] [32m18.84GB/s[0m, eta: [36m 0s[0m                                                                                                                           Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [=============================---------] [32m10.75GB/s[0m, eta: [36m 0s[0m[1mindexing[0m [34mweather_hobart.csv[0m [=====================================] [32m620.60MB/s[0m, eta: [36m 0s[0m                                                                                                                           Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [===================================] [32m57.08GB/s[0m, eta: [36m 0s[0m                                                                                                                           Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [======================================] [32m48.84GB/s[0m, eta: [36m 0s[0m                                                                                                                           Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by=c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> for (file_name in list.files(sunshine_dir, pattern = "\\.csv$", full.names = TRUE)) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_comb .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [====================================] [32m16.55GB/s[0m, eta: [36m 0s[0m                                                                                                                           [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [=================================] [32m21.62GB/s[0m, eta: [36m 0s[0m                                                                                                                           [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [======================================] [32m22.65GB/s[0m, eta: [36m 0s[0m                                                                                                                           [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [======================================] [32m9.41GB/s[0m, eta: [36m 0s[0m                                                                                                                           [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [===================================] [32m23.92GB/s[0m, eta: [36m 0s[0m                                                                                                                           [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by=c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [=============================================] [32m35.38GB/s[0m, eta: [36m 0s[0m                                                                                                                           
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, na.rm =  .... [TRUNCATED] 

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> wind <- wind |>
+   group_by(regionid) |>
+   complete(date_local .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by=c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> df <- df |>
+   left_join(midday_emissions, by=c("regionid", "date_local")) |>
+   mutate(
+     energy_wh_per_capita_vs_midday=(energy_kwh_per_capi .... [TRUNCATED] 

> # tidy up -----------------------------------------------------------------
> 
> 
> # our weather data, AEMO data etc
> # has slightly different end .... [TRUNCATED] 

> # missing data final check ------------------------------------------------
> 
> # check data has no unexpected holes
> # we know rooftop solar data .... [TRUNCATED] 

> missing <- missing[(missing > 0) & !grepl("rooftop", names(df))]

> stopifnot(length(missing) == 0)
$platform
[1] "x86_64-pc-linux-gnu"

$arch
[1] "x86_64"

$os
[1] "linux-gnu"

$system
[1] "x86_64, linux-gnu"

$status
[1] "Patched"

$major
[1] "4"

$minor
[1] "2.2"

$year
[1] "2022"

$month
[1] "11"

$day
[1] "10"

$`svn rev`
[1] "83330"

$language
[1] "R"

$version.string
[1] "R version 4.2.2 Patched (2022-11-10 r83330)"

$nickname
[1] "Innocent and Trusting"


> library(tidyverse)

> library(arrow)

> library(stargazer)

> library(ggplot2)

> library(sandwich)

> library(lmtest)

> library(eventstudyr)

> library(here)

> library(broom)

> # logging -----------------------------------------------------------------
> # We were told to set up logging
> dir.create(here::here("logs"), show .... [TRUNCATED] 

> sink(NULL) # unset from previous runs
