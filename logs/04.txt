
> # Paths ---------------------------------------------
> 
> # relative to this file
> # although you can specify an absolute path if you wish.
> data .... [TRUNCATED] 

> # source data
> pub_hol_path_1 <- file.path(data_dir, "raw/holidays/Aus_public_hols_2009-2022-1.csv")

> pub_hol_path_2 <- file.path(data_dir, "raw/holidays/australian-public-holidays-combined-2021-2024.csv")

> dst_transitions_path <- file.path(data_dir, 'snapshot/02-dst-dates.csv')

> population_path <- file.path(data_dir, "raw/population/population-australia-raw.csv")

> temperature_dir <- file.path(data_dir, "raw/weather")

> sunshine_dir <- file.path(data_dir, "raw/sunshine")

> wind_path <- file.path(data_dir, "snapshot/05-wind.csv")

> aemo_pq_path <- file.path(data_dir, "snapshot/01-F-aemo-joined-all.parquet")

> output_file_path_hh_csv <- file.path(data_dir, "04-half-hourly.csv")

> output_file_path_hh_pq <- file.path(data_dir, "04-half-hourly.parquet")

> output_file_path_daily <- file.path(data_dir, "04-energy-daily.csv")

> # constants ---------------------------------------------------------------
> 
> 
> Sys.setenv(TZ = 'UTC') # see README.md

> # AEMO data is in "Market time"
> # that's this time zone
> # (No DST, just UTC+10)
> market_tz <- "Australia/Brisbane"

> # South Australia is permanently behind VIC, NSW, TAS by this much
> # (They shift forward/back on the same day by the same amount)
> SA_offset <- m .... [TRUNCATED] 

> # unit converstion constants
> # We don't want magic numbers later in the code
> # kilograms per tonne
> kg_per_t <- 1000

> # grams per kilogram
> g_per_kg <- 1000

> kg_per_g <- 1/g_per_kg

> # kil-mega-giga watt hour conversion ratios
> wh_per_kwh <- 1000

> kwh_per_wh <- 1/wh_per_kwh

> kwh_per_mwh <- 1000

> mwh_per_gwh <- 1000

> gwh_per_twh <- 1000

> mwh_per_twh <- mwh_per_gwh * gwh_per_twh

> # minutes per half hour
> min_per_hh <- 30

> # minutes per hour
> min_per_h <- 60

> # Unit conversions:
> # 1 Joule = 1 watt second
> # 1 MJ = 10^6 Ws = 10^3 kWs = 10^3 / 60^2 kWh
> # uppercase M not lowercase, to make it clear this .... [TRUNCATED] 

> # Define the city-region mapping, for weather data to AEMO regions
> # The first one is for capital cities.
> # This is for temperature, which drive .... [TRUNCATED] 

> regional_city_region_map <- c(
+   'cooberpedy' = 'SA1',
+   'richmond' = 'QLD1',
+   'dubbo' = 'NSW1',
+   'bendigo' = 'VIC1',
+   'hobart' = 'TAS1 .... [TRUNCATED] 

> # we are joining lots of different datasets
> # with many different start/end dates
> # the intersection we are aiming for is: (inclusive)
> start_d .... [TRUNCATED] 

> end_date <- make_date(2023, 12, 1)

> # load energy source data ------------------------------------------------------
> 
> energy <- read_parquet(aemo_pq_path)

> # Local time, midday control and other time info -------------------------------
> 
> # We want to convert fixed Brisbane UTC+10 time to local time
 .... [TRUNCATED] 

> df <- energy |>
+   left_join(region_tz, by = c("regionid")) |>
+   group_by(regionid) |>
+   mutate(
+     hh_end_fixed = force_tz(hh_end, tzone =  .... [TRUNCATED] 

> # Public Holidays ---------------------------------------------------------
> # we read public holiday data from two files
> # because neither one o .... [TRUNCATED] 
[1mindexing[0m [34mAus_public_hols_2009-2022-1.csv[0m [===========================] [32m4.09GB/s[0m, eta: [36m 0s[0m                                                                                                                            
> holidays_2 <- read_csv(pub_hol_path_2) |>
+   rename(State = Jurisdiction) |>
+   select(Date, State) |>
+   mutate(
+     Date = ymd(Date)
+   )
[1mindexing[0m [34maustralian-public-holidays-combined-2021-2024.csv[0m [=========] [32m5.86GB/s[0m, eta: [36m 0s[0m                                                                                                                            
> holidays <- rbind(holidays_1, holidays_2) |>
+   mutate(
+     regionid = paste0(str_to_upper(State), "1"),
+   ) |>
+   select(-State) |>
+   disti .... [TRUNCATED] 

> # Join with main dataframe
> df <- holidays |>
+   rename(date_local = Date) |>
+   mutate(public_holiday = TRUE) |>
+   right_join(df, by = c("date ..." ... [TRUNCATED] 

> # Join DST data to energy -------------------------------------------------
> 
> # dst_transitions has one row per clock change
> # we want to trans .... [TRUNCATED] 
[1mindexing[0m [34m02-dst-dates.csv[0m [=========================================] [32m92.72MB/s[0m, eta: [36m 0s[0m                                                                                                                            
> dst_transitions <- dst_transitions |>
+   rename(
+     dst_date = date,
+     dst_direction = direction) |>
+   mutate(
+     dst_direction = facto .... [TRUNCATED] 

> # create a tibble with all dates we care about
> # (plus extra)
> # and the info for the nearest DST transition
> # to make joins later
> dst_dates_ .... [TRUNCATED] 

> # now join DST info to main dataframe
> 
> df <- dst_dates_all |>
+   rename(date_local = d) |>
+   right_join(df, by = "date_local") |>
+   mutate( .... [TRUNCATED] 

> no_dst_info <- df |> filter(is.na(dst_now_here))

> stopifnot((no_dst_info |> nrow()) == 0)

> # In our time period, there's one particular day
> # that's 94 days into DST, and one that's -94
> # because the duration of DST (or not) differs sl .... [TRUNCATED] 

> samples_per_days_into_dst <- df |> summarise(n = n(), .by = days_into_dst)

> typical_sample_count <- samples_per_days_into_dst |> 
+   pull(n) |> 
+   abs() |> 
+   median()

> outlier_days <- samples_per_days_into_dst |> 
+   filter(abs(n) < typical_sample_count) |> 
+   pull(days_into_dst)

> df$days_into_dst_outlier <- df$days_into_dst %in% outlier_days

> # Add population ----------------------------------------------------------
> 
> # Load data
> population_raw <- read_csv(population_path)
[1mindexing[0m [34mpopulation-australia-raw.csv[0m [==============================] [32m2.89GB/s[0m, eta: [36m 0s[0m                                                                                                                            
> # First data cleaning
> # Doesn't work with |> instead of  %>%
> # because of (.)
> population <- population_raw %>%
+   select(1, (ncol(.) - 8):nco .... [TRUNCATED] 

> colnames(population) <-
+   c("Date",
+     "NSW1",
+     "VIC1",
+     "QLD1",
+     "SA1",
+     "WA1",
+     "TAS1",
+     "NT1",
+     "ACT1",
+ .... [TRUNCATED] 

> # Cast to numbers
> population[2:ncol(population)] <- lapply(population[2:ncol(population)], as.numeric)

> # Include Australian Capital Territory in New South Wales
> population$NSW1 <- population$NSW1 + population$ACT1

> # drop regions that aren't part of the study
> population <- population |> select(-c(ACT1, AUS, NT1, WA1))

> # Transform dates to datetime format
> population <- population |>
+   mutate(Date = parse_date(Date, "%b-%Y"))|>
+   filter(between(Date, start_dat .... [TRUNCATED] 

> # Pivot the dataframe to have one column per state
> population <- population |> 
+   pivot_longer(cols = -Date, names_to = "regionid", values_to =  .... [TRUNCATED] 

> # now linearly interpolate the 3-month data into daily
> # Note that since our main electrical dataset ends on 31st December
> # and this population .... [TRUNCATED] 

> # join to main dataframe
> df <- population |>
+   rename(date_local = Date) |>
+   right_join(df, by = c("regionid", "date_local"))

> # add temperature ---------------------------------------------------------
> 
> 
> # Define the clean and combine function for temperature data
> c .... [TRUNCATED] 

> # Create Temperature Dataframe 
> # Loop through each CSV file in the directory 
> all_temperature <- list()

> temp_files <- list.files(temperature_dir, 
+                          pattern = "\\.csv$", 
+                          full.names = TRUE)

> for (file_name in temp_files) {
+   all_temperature[[length(all_temperature) + 1]] <- clean_and_combine_temp(file_name)
+   cat(sprintf('Data cleane .... [TRUNCATED] 
[1mindexing[0m [34mweather_adelaide.csv[0m [=====================================] [32m76.40GB/s[0m, eta: [36m 0s[0m                                                                                                                            Data cleaned and added to list for SA1
[1mindexing[0m [34mweather_brisbane.csv[0m [=====================================] [32m25.66GB/s[0m, eta: [36m 0s[0m                                                                                                                            Data cleaned and added to list for QLD1
[1mindexing[0m [34mweather_hobart.csv[0m [======================================] [32m199.26GB/s[0m, eta: [36m 0s[0m                                                                                                                            Data cleaned and added to list for TAS1
[1mindexing[0m [34mweather_melbourne.csv[0m [===================================] [32m114.16GB/s[0m, eta: [36m 0s[0m                                                                                                                            Data cleaned and added to list for VIC1
[1mindexing[0m [34mweather_sydney.csv[0m [======================================] [32m110.42GB/s[0m, eta: [36m 0s[0m                                                                                                                            Data cleaned and added to list for NSW1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_temperature) > 0)

> # Merge all temperature data frames
> temperature <- bind_rows(all_temperature)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> temperature <- temperature |>
+   group_by(regionid) |>
+   mutat .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, temperature, by = c("regionid", "date_local"))

> # add sunshine ------------------------------------------------------------
> 
> # Define the clean and combine function for sunshine data
> clean_a .... [TRUNCATED] 

> #Create Sunshine Dataframe
> all_sunshine <- list()

> sunshine_files <- list.files(sunshine_dir, 
+                              pattern = "\\.csv$", 
+                              full.names = TRUE)

> for (file_name in sunshine_files) {
+   all_sunshine[[length(all_sunshine) + 1]] <- clean_and_combine_sunshine(file_name)
+   cat(sprintf('Data clea .... [TRUNCATED] 
[1mindexing[0m [34msunshine-bendigo.csv[0m [=====================================] [32m39.23GB/s[0m, eta: [36m 0s[0m                                                                                                                            [1] "Trying to find region for city bendigo , found  VIC1"
Data cleaned and added to list for VIC1
[1mindexing[0m [34msunshine-cooberpedy.csv[0m [==================================] [32m43.25GB/s[0m, eta: [36m 0s[0m                                                                                                                            [1] "Trying to find region for city cooberpedy , found  SA1"
Data cleaned and added to list for SA1
[1mindexing[0m [34msunshine-dubbo.csv[0m [=======================================] [32m36.24GB/s[0m, eta: [36m 0s[0m                                                                                                                            [1] "Trying to find region for city dubbo , found  NSW1"
Data cleaned and added to list for NSW1
[1mindexing[0m [34msunshine-hobart.csv[0m [======================================] [32m26.58GB/s[0m, eta: [36m 0s[0m                                                                                                                            [1] "Trying to find region for city hobart , found  TAS1"
Data cleaned and added to list for TAS1
[1mindexing[0m [34msunshine-richmond.csv[0m [====================================] [32m43.29GB/s[0m, eta: [36m 0s[0m                                                                                                                            [1] "Trying to find region for city richmond , found  QLD1"
Data cleaned and added to list for QLD1

> # check that we have found some data
> # (i.e. source data not silently missing)
> stopifnot(length(all_sunshine) > 0)

> # Merge all sunshine data frames
> sunshine <- bind_rows(all_sunshine)

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> sunshine <- sunshine |>
+   group_by(regionid) |>
+   mutate(sola .... [TRUNCATED] 

> # join to main dataframe
> df <- left_join(df, sunshine, by = c("regionid", "date_local"))

> # Wind data ---------------------------------------------------------------
> 
> 
> # fill in that one gap, linear interpolation
> wind <- read_csv( .... [TRUNCATED] 
[1mindexing[0m [34m05-wind.csv[0m [==============================================] [32m77.27GB/s[0m, eta: [36m 0s[0m                                                                                                                            
> # we're missing a lot of max wind speed data
> # but only one average wind speed record
> stopifnot(sum(is.na(wind$avg_wind_speed_km_per_h)) <= 1)

> wind <- wind |>
+   group_by(regionid) |>
+   arrange(date) |>
+   mutate(avg_wind_speed_km_per_h = zoo::na.approx(avg_wind_speed_km_per_h, 
+       .... [TRUNCATED] 

> # Fill in gaps which are larger than one day in a row by interpolating linearly 
> wind <- wind |>
+   group_by(regionid) |>
+   complete(date_local .... [TRUNCATED] 

> # add to main dataframe
> df <- df |>
+   left_join(wind, by = c("date_local", "regionid"))

> # Per capita calculations -------------------------------------------------
> 
> # do division to get per-capita 
> # also normalise values by chang .... [TRUNCATED] 

> # add midday float --------------------------------------------------------
> # we already have a dummy column for if this half our is a midday cont .... [TRUNCATED] 

> df <- df |>
+   left_join(midday_emissions, by = c("regionid", "date_local")) |>
+   mutate(
+     energy_wh_per_capita_vs_midday = (energy_kwh_per_ .... [TRUNCATED] 

> # tidy up -----------------------------------------------------------------
> 
> 
> # our weather data, AEMO data etc
> # has slightly different end .... [TRUNCATED] 

> # missing data final check ------------------------------------------------
> 
> # check data has no unexpected holes
> # we know rooftop solar data .... [TRUNCATED] 

> missing <- missing[(missing > 0) & !grepl("rooftop", names(df))]

> stopifnot(length(missing) == 0)

> # Save half hourly output ------------------------------------------------------
> # CSV for stata
> # parquet for the next R script
> 
> write_csv( .... [TRUNCATED] 
[1mwrote[0m [32m5.61MB[0m in [36m 0s[0m, [32m560.19GB/s[0m[1mwrote[0m [32m186.55MB[0m in [36m 0s[0m, [32m916.63MB/s[0m[1mwrote[0m [32m192.30MB[0m in [36m 0s[0m, [32m919.31MB/s[0m[1mwrote[0m [32m197.99MB[0m in [36m 0s[0m, [32m920.87MB/s[0m[1mwrote[0m [32m203.68MB[0m in [36m 0s[0m, [32m914.87MB/s[0m[1mwrote[0m [32m209.31MB[0m in [36m 0s[0m, [32m910.96MB/s[0m[1mwrote[0m [32m214.95MB[0m in [36m 0s[0m, [32m908.34MB/s[0m[1mwrote[0m [32m220.66MB[0m in [36m 0s[0m, [32m914.24MB/s[0m[1mwrote[0m [32m226.27MB[0m in [36m 0s[0m, [32m901.46MB/s[0m[1mwrote[0m [32m231.90MB[0m in [36m 0s[0m, [32m909.29MB/s[0m[1mwrote[0m [32m237.56MB[0m in [36m 0s[0m, [32m893.14MB/s[0m[1mwrote[0m [32m243.27MB[0m in [36m 0s[0m, [32m900.95MB/s[0m[1mwrote[0m [32m248.90MB[0m in [36m 0s[0m, [32m897.57MB/s[0m[1mwrote[0m [32m254.54MB[0m in [36m 0s[0m, [32m888.47MB/s[0m[1mwrote[0m [32m260.25MB[0m in [36m 0s[0m, [32m881.63MB/s[0m[1mwrote[0m [32m265.97MB[0m in [36m 0s[0m, [32m886.30MB/s[0m[1mwrote[0m [32m271.68MB[0m in [36m 0s[0m, [32m881.25MB/s[0m[1mwrote[0m [32m277.34MB[0m in [36m 0s[0m, [32m873.66MB/s[0m[1mwrote[0m [32m283.25MB[0m in [36m 0s[0m, [32m878.05MB/s[0m[1mwrote[0m [32m289.21MB[0m in [36m 0s[0m, [32m872.86MB/s[0m[1mwrote[0m [32m295.14MB[0m in [36m 0s[0m, [32m867.60MB/s[0m[1mwrote[0m [32m301.16MB[0m in [36m 0s[0m, [32m867.22MB/s[0m[1mwrote[0m [32m307.17MB[0m in [36m 0s[0m, [32m862.57MB/s[0m[1mwrote[0m [32m313.20MB[0m in [36m 0s[0m, [32m864.61MB/s[0m[1mwrote[0m [32m319.15MB[0m in [36m 0s[0m, [32m860.12MB/s[0m[1mwrote[0m [32m325.14MB[0m in [36m 0s[0m, [32m853.59MB/s[0m[1mwrote[0m [32m331.12MB[0m in [36m 0s[0m, [32m858.57MB/s[0m[1mwrote[0m [32m337.18MB[0m in [36m 0s[0m, [32m845.03MB/s[0m[1mwrote[0m [32m343.26MB[0m in [36m 0s[0m, [32m844.51MB/s[0m[1mwrote[0m [32m349.27MB[0m in [36m 0s[0m, [32m847.02MB/s[0m[1mwrote[0m [32m355.23MB[0m in [36m 0s[0m, [32m842.76MB/s[0m[1mwrote[0m [32m361.18MB[0m in [36m 0s[0m, [32m848.29MB/s[0m[1mwrote[0m [32m367.19MB[0m in [36m 0s[0m, [32m839.91MB/s[0m[1mwrote[0m [32m373.23MB[0m in [36m 0s[0m, [32m845.47MB/s[0m[1mwrote[0m [32m379.27MB[0m in [36m 0s[0m, [32m841.40MB/s[0m[1mwrote[0m [32m385.32MB[0m in [36m 0s[0m, [32m844.27MB/s[0m[1mwrote[0m [32m391.35MB[0m in [36m 0s[0m, [32m839.60MB/s[0m[1mwrote[0m [32m397.38MB[0m in [36m 0s[0m, [32m844.70MB/s[0m[1mwrote[0m [32m403.42MB[0m in [36m 0s[0m, [32m839.34MB/s[0m[1mwrote[0m [32m409.44MB[0m in [36m 0s[0m, [32m843.37MB/s[0m[1mwrote[0m [32m415.47MB[0m in [36m 0s[0m, [32m841.71MB/s[0m[1mwrote[0m [32m421.48MB[0m in [36m 1s[0m, [32m840.95MB/s[0m[1mwrote[0m [32m427.46MB[0m in [36m 1s[0m, [32m835.00MB/s[0m[1mwrote[0m [32m433.42MB[0m in [36m 1s[0m, [32m839.00MB/s[0m[1mwrote[0m [32m439.50MB[0m in [36m 1s[0m, [32m827.95MB/s[0m[1mwrote[0m [32m445.59MB[0m in [36m 1s[0m, [32m832.60MB/s[0m[1mwrote[0m [32m451.64MB[0m in [36m 1s[0m, [32m823.57MB/s[0m[1mwrote[0m [32m457.67MB[0m in [36m 1s[0m, [32m825.23MB/s[0m[1mwrote[0m [32m463.73MB[0m in [36m 1s[0m, [32m821.58MB/s[0m[1mwrote[0m [32m469.70MB[0m in [36m 1s[0m, [32m818.89MB/s[0m[1mwrote[0m [32m475.66MB[0m in [36m 1s[0m, [32m817.67MB/s[0m[1mwrote[0m [32m481.62MB[0m in [36m 1s[0m, [32m816.42MB/s[0m[1mwrote[0m [32m487.66MB[0m in [36m 1s[0m, [32m816.00MB/s[0m[1mwrote[0m [32m493.79MB[0m in [36m 1s[0m, [32m811.10MB/s[0m[1mwrote[0m [32m499.88MB[0m in [36m 1s[0m, [32m811.14MB/s[0m[1mwrote[0m [32m506.00MB[0m in [36m 1s[0m, [32m813.67MB/s[0m[1mwrote[0m [32m512.03MB[0m in [36m 1s[0m, [32m810.90MB/s[0m[1mwrote[0m [32m518.01MB[0m in [36m 1s[0m, [32m814.06MB/s[0m[1mwrote[0m [32m523.99MB[0m in [36m 1s[0m, [32m812.00MB/s[0m[1mwrote[0m [32m529.92MB[0m in [36m 1s[0m, [32m809.75MB/s[0m[1mwrote[0m [32m535.91MB[0m in [36m 1s[0m, [32m806.38MB/s[0m[1mwrote[0m [32m541.95MB[0m in [36m 1s[0m, [32m810.10MB/s[0m[1mwrote[0m [32m547.90MB[0m in [36m 1s[0m, [32m807.76MB/s[0m[1mwrote[0m [32m553.83MB[0m in [36m 1s[0m, [32m810.83MB/s[0m[1mwrote[0m [32m559.78MB[0m in [36m 1s[0m, [32m806.17MB/s[0m[1mwrote[0m [32m565.69MB[0m in [36m 1s[0m, [32m809.54MB/s[0m[1mwrote[0m [32m571.64MB[0m in [36m 1s[0m, [32m803.02MB/s[0m[1mwrote[0m [32m577.65MB[0m in [36m 1s[0m, [32m806.44MB/s[0m[1mwrote[0m [32m583.64MB[0m in [36m 1s[0m, [32m792.14MB/s[0m                                                                                       [1mwrote[0m [32m1.00TB[0m in [36m 1s[0m, [32m1.36TB/s[0m                                                                                       
> write_parquet(df, sink = output_file_path_hh_pq)

> # downsample half hourly to daily -----------------------------------------
> 
> 
> 
> # midday for daily ------------------------------------------ .... [TRUNCATED] 

> # multiply some values by day_length_scale_factor
> # to account for the fact that some days have a bit fewer/more than 48 half hours
> # i.e. it's  .... [TRUNCATED] 

> daily |> write_csv(output_file_path_daily)
[1mwrote[0m [32m80.22kB[0m in [36m 0s[0m, [32m8.85GB/s[0m[1mwrote[0m [32m1.00TB[0m in [36m 0s[0m, [32m62.38TB/s[0m                                                                                       
> # this script uses a lot of memory
> # free up some with aggressive garbage collection
> # to leave room for the next script.
> # (It's better if th .... [TRUNCATED] 
           used  (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells  1482551  79.2    4453623  237.9   5567028  297.4
Vcells 61695571 470.7  172290137 1314.5 172193314 1313.8

> # graphs ------------------------------------------------------------------
> # we generate one graph that's easier than in stata. The rest is done  .... [TRUNCATED] 

> typical_midday <- ddd_es |>
+   filter(! not_midday_control_fixed) |>
+   pull(co2) |>
+   mean()

> ddd_es |>
+   mutate(
+     co2 = co2 - typical_midday
+   ) |>
+   
+   ggplot(aes(x=hr_fixed, y=co2)) +
+   geom_line() +
+   labs(
+     title="D ..." ... [TRUNCATED] 

> ggsave(here("plots/16-DDD-event-study-average.png"), width=9, height=7)

> print('done')
[1] "done"

> sink(NULL) # close log file
