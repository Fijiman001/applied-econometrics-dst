{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc77c51-01c0-4027-9fb9-c46b76caae3a",
   "metadata": {},
   "source": [
    "# Nemweb file download, split, aggregation and converstion\n",
    "\n",
    "This script downloads data files from https://www.nemweb.com.au/REPORTS/ and https://www.nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/ .\n",
    "\n",
    "1. Crawl those pages to find a list of URLs for CSV and .zip files\n",
    "2. Filter that list. e.g. delete large files we don't need\n",
    "3. Download those files. (Suprisingly hard. The server is unreliable. Throttles us, gives corrupt files etc)\n",
    "4. Unzip each zip. (Possibly recursively. They have zips of zips of CSVs). The total dataset is about 1TB uncompressed. So we keep .csv files as .csv.gz (gzip compressed CSV)\n",
    "5. For each CSV file, split it up into several CSV files. This is because AEMO's CSV files contain data for different tables (e.g. pricing and billing stuff in the same file.)\n",
    "6. After splitting into groups of files, figure out which 'table name' to map each file to a table [here](https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report.htm)\n",
    "\n",
    "The code may seem more complex than you expected. That's mostly because I want to do multiprocessing to speed it up. But then this complicates things because:\n",
    "\n",
    "* our downloads get throttled, so we need to sleep and retry\n",
    "* Sometimes you can download/unzip/process 1000 files, and then file 1001 fails. I want to keep processing the remainder, gather up all the success/failures, and then compare them. So there's some try/catch stuff to handle that.\n",
    "\n",
    "## Remaining to do (as of 30/11/2023)\n",
    "\n",
    "* debug why mwe.csv can't be processed with pyarrow. Continue shrinking down to mwe.\n",
    "* try doing the parquet converstion with R\n",
    "* try re-running the cell at the bottom to test which rows to skip\n",
    "* If I can't solve it, for the parquet conversion, only choose the tables we care about. (Because some fail with an error, and I don't know why.)\n",
    "* Re-run the download for all files.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1722f72e-e08d-401b-b5aa-f3fdae3e4aa5",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "If you don't have these libraries installed, run `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52747299-874d-4489-8817-6be9929b5521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a050fd1-0264-4b95-a491-64c4368d70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "import urllib3\n",
    "import shutil\n",
    "from urllib.parse import  urljoin, urlparse\n",
    "from zipfile import ZipFile\n",
    "from time import sleep, time\n",
    "import re\n",
    "import gzip\n",
    "from random import randrange, shuffle\n",
    "from uuid import uuid4\n",
    "from typing import Set, Tuple # type annotations, for doc clarity only. Ignore them if you're not used to them\n",
    "from itertools import zip_longest\n",
    "import csv\n",
    "from copy import deepcopy as copy\n",
    "import pickle\n",
    "import datetime as dt\n",
    "from typing import Dict\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "#from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # webscraping\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyarrow.csv\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46927f8e-2e66-4f42-bd02-dbc157da6846",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51f76b-9013-4c91-a6b6-7a1b29a05b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = [\n",
    "    'https://www.nemweb.com.au/REPORTS/CURRENT/', # last year\n",
    "    #'https://www.nemweb.com.au/REPORTS/ARCHIVE/', # last year\n",
    "    'https://www.nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/2023/MMSDM_2023_10/', \n",
    "    #'https://nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/' # 1-10 years old, in a different folder and zip structure\n",
    "]\n",
    "file_suffixes = ['.zip', '.csv'] # will be made case insensitive later\n",
    "expected_domains = ('www.nemweb.com.au', 'nemweb.com.au')\n",
    "\n",
    "# ignore any URL containing one of these\n",
    "url_substrings_to_skip = [\n",
    "    '/NEMDE/', # don't bother looking in these folders\n",
    "    '/Gas_Supply_Guarantee/',\n",
    "    '/VicGas/',\n",
    "    '/DWGM/',\n",
    "    'Predispatch', # predictions (very large)\n",
    "    'FCAS', # unrelated to our purposes, and quite large\n",
    "    'BIDPEROFFER', # large, unrelated\n",
    "    'BIDDAYOFFER', # large, unrelated\n",
    "    'BIDOFFER',\n",
    "    'DAYOFFER',\n",
    "    'MTPASA_OFFER',\n",
    "    'MTPASA',\n",
    "    'PDPASA',\n",
    "    'PRUDENTIAL',\n",
    "    'MMSDM_CLI_', # not data\n",
    "    'MMSDM_GUI_', # not data\n",
    "]\n",
    "\n",
    "max_files_per_page = 2\n",
    "\n",
    "base_data_dir = 'data'\n",
    "urls_file_path = os.path.join(base_data_dir, 'urls.txt')\n",
    "raw_files_path = os.path.join(base_data_dir, '01-A-raw/')\n",
    "unzipping_temp_path = os.path.join(base_data_dir, '01-B-unzipping/')\n",
    "unzipped_files_path = os.path.join(base_data_dir, '01-C-unzipped/')\n",
    "split_unmapped_files_path = os.path.join(base_data_dir, '01-D-split/')\n",
    "split_mapped_files_path = os.path.join(base_data_dir, '01-E-split-mapped/')\n",
    "one_parquet_per_table_path = os.path.join(base_data_dir, '01-F-one-parquet-per-table/')\n",
    "debug_path = os.path.join(base_data_dir, 'debug/')\n",
    "\n",
    "\n",
    "compresslevel = 2 # how hardcore the gzip algorithm is for .csv.gz files. Low to prioritise speed over size\n",
    "\n",
    "# process files with multiprocessing, to be faster.\n",
    "# If set to False, will use a for loop, which gives clearer traceback error messages.\n",
    "use_multiprocessing = True\n",
    "\n",
    "# set to True to leave one unused CPU when multiprocessing\n",
    "# so that you can still do other stuff on your laptop without your internet browser or whatever being laggy\n",
    "leave_unused_cpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af169103-6df6-4512-a53e-8cb535a23214",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = os.path.join(base_data_dir, 'MMS_Data_Model_Table_to_File_to_Report_Relationships_51.xlsx')\n",
    "sheet = 'MMSDM_v5-1'\n",
    "report_subtype_column = 'PDR_REPORT_SUB_TYPE'\n",
    "report_name_column = 'PDR_REPORT_NAME'\n",
    "table_name_column = 'MMSDM_TABLE_NAME'\n",
    "\n",
    "REPORT_NAME_NULL_PLACEHOLDER = 'NULL'\n",
    "REPORT_SUBTYPE_NULL_PLACEHOLDER = 'NULL'\n",
    "\n",
    "# AEMO data is always in UTC+10\n",
    "TIMEZONE_OFFSET=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123a9d0-fac9-49f3-b720-90cae21fdd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Linux and Mac this makes this python process lower priority\n",
    "# so when it's running and using up all your CPU, your interface won't lag.\n",
    "# So you can keep browsing the web, typing documents etc\n",
    "try:\n",
    "    os.nice(20)\n",
    "except OSError: # ignore error, this is probably on Windows\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0552589-8e53-4389-96f9-f39edfd96d40",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "Each step takes a long time (many files). I want to know that it's doing something. But if I just `print()`, there will be too much text. So I write to a file, and you can look at that file to watch the logs. (On Linux or Mac use `tail -f log.txt` in a terminal. Windows users probably have to buy some app from the Windows app store. I dunno.)\n",
    "\n",
    "As a lazy but concurrency-safe approach, we just open and close the file for each message. Meh, good enough for now. (The `logging` library is not multiprocessing safe. You can [make it work with logs of effort](https://docs.python.org/3/howto/logging-cookbook.html#logging-to-a-single-file-from-multiple-processes))\n",
    "\n",
    "For this approach, log with:\n",
    "\n",
    "* `logger.info(\"Something\")`\n",
    "* `logger.error(\"Something\")`\n",
    "\n",
    "And to wipe the file (e.g. when retrying a cell) call `logger.reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c083668-1c31-4dbf-9f87-0ef4fc52104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, path=os.path.join(base_data_dir, 'log.txt')):\n",
    "        self.path = path\n",
    "        self.reset()\n",
    "        \n",
    "    def write(self, msg):\n",
    "        with open(self.path, 'a') as f:\n",
    "            f.write(msg.rstrip() + '\\n')\n",
    "    def info(self, msg):\n",
    "        self.write(f\"INFO: {msg}\")\n",
    "    def warn(self, msg):\n",
    "        self.write(f\"WARNING: {msg}\")\n",
    "    def warning(self, msg):\n",
    "        self.warn(msg)\n",
    "    def error(self, msg):\n",
    "        self.write(f\"ERROR: {msg}\")\n",
    "\n",
    "    # erase the file\n",
    "    # but leave a blank file in place\n",
    "    # to do that, open in `w` mode, and write nothing.\n",
    "    def reset(self):\n",
    "        with open(self.path, 'w'):\n",
    "            pass\n",
    "\n",
    "logger = Logger()\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68597489-e528-4079-8047-69417287cae7",
   "metadata": {},
   "source": [
    "## Discover which files to download\n",
    "\n",
    "We don't actually download the .zip or .csv files yet.\n",
    "We just get a list of the URLs of the .zip and .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba34003-7899-4447-a367-7a1644f2aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a session object to re-use between requests\n",
    "# to hopefully speed up downloads by not re-doing the TLS handshake for each HTTP request\n",
    "# (unsure if this actually speeds things up)\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512b2e5-76cd-4569-91eb-947cd7d15d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "progress_bar = tqdm(leave=False)\n",
    "\n",
    "\n",
    "def force_https(url):\n",
    "    p = urlparse(url)\n",
    "    if p.scheme != 'https':\n",
    "        assert url.count('http://') == 1, f\"Strange URL: {url}\" # abort, for security reasons\n",
    "        url = url.replace('http://', 'https://')\n",
    "    return url\n",
    "\n",
    "def is_child_of(parent, child):\n",
    "    p = urlparse(parent)\n",
    "    c = urlparse(child)\n",
    "    return (p.hostname in expected_domains) and (c.hostname in expected_domains) \\\n",
    "        and c.path.rstrip('/').startswith(p.path.rstrip('/')) \\\n",
    "        and c.path.rstrip('/') != p.path.rstrip('/')\n",
    "\n",
    "# unit tests\n",
    "assert is_child_of('https://www.nemweb.com.au/REPORTS/', 'https://www.nemweb.com.au/REPORTS/CURRENT/')\n",
    "assert not is_child_of('https://www.nemweb.com.au/REPORTS/CURRENT', 'https://www.nemweb.com.au/REPORTS/')\n",
    "assert not is_child_of('https://www.nemweb.com.au/REPORTS/CURRENT/', 'https://www.nemweb.com.au/REPORTS/CURRENT/')\n",
    "assert not is_child_of('https://www.nemweb.com.au/REPORTS/CURRENT/', 'https://www.nemweb.com.au/REPORTS/ARCHIVE/')\n",
    "\n",
    "def extract_links(url):\n",
    "    logger.info(f\"Checking for links in {url}\")\n",
    "    if any(ss in url for ss in url_substrings_to_skip):\n",
    "        # skip this. Don't bother looking inside it.\n",
    "        logger.info(f\"Skipping {url} because of substrings\")\n",
    "        return []\n",
    "    # this is a web page listing other files or pages of other files\n",
    "    r = session.get(url)\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "\n",
    "    # it's called \"soup\" because the python webscraping library is called \"beautiful soup\"\n",
    "    soup = BeautifulSoup(html)\n",
    "    links = [a['href'] for a in soup.find_all('a')]\n",
    "    \n",
    "    # convert potentially relative URLs to absolute\n",
    "    links = [urljoin(start_url, u) for u in links]\n",
    "\n",
    "    # remove any links that aren't files on this page or subpages\n",
    "    # watch out, the domain can sometimes change (www. removed or added)\n",
    "    links = [u for u in links if is_child_of(parent=url, child=u)]\n",
    "\n",
    "    # recursively check child pages\n",
    "    links_to_children = [u for u in links if u.endswith('/')]\n",
    "    links_of_children = [extract_links(u) for u in links_to_children]\n",
    "    # flatten list of list to just a list\n",
    "    links_of_children = [u for links_of_child in links_of_children for u in links_of_child]\n",
    "\n",
    "    # filter to only ones matching our file suffixes\n",
    "    links = [u for u in links if any(u.lower().endswith(ext.lower()) for ext in file_suffixes)]\n",
    "\n",
    "    # delete any links which contain the blacklisted substrings\n",
    "    links = [u for u in links if not any(ss in url for ss in url_substrings_to_skip)]\n",
    "    \n",
    "    # only a few files per page\n",
    "    # except MMSDM pages, which have one file of each type\n",
    "    if max_files_per_page and (max_files_per_page > 0) and ('/MMSDM/' not in url):\n",
    "        links = links[:max_files_per_page]\n",
    "    links.extend(links_of_children)\n",
    "\n",
    "    progress_bar.update()\n",
    "    logger.info(f\"Found {len(links)} links at/under {url}\")\n",
    "    return links\n",
    "\n",
    "urls = []\n",
    "for start_url in start_urls:\n",
    "    urls.extend(extract_links(start_url))\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Discovered {len(urls)} URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d1943-9db3-4910-b8ec-d6a3972c8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Writing {len(urls)} urls to {urls_file_path}\")\n",
    "with open(urls_file_path, 'w') as f:\n",
    "    for u in urls:\n",
    "        f.write(u + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff78907-9261-4e97-a262-8b8ec7472991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to skip re-indexing, run the playbook from here\n",
    "with open(urls_file_path, 'r') as f:\n",
    "    urls = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c4cda-8178-42b3-a1f3-899bbe81331b",
   "metadata": {},
   "source": [
    "Now we filter the list of URLs, to discard the ones we don't care about. (Large ones, gas ones etc)\n",
    "\n",
    "Just delete URLs with string from `url_substrings_to_skip` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a34aa5-78d3-439d-a245-69d45f162e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [u for u in urls if not any(s.lower() in u.lower() for s in url_substrings_to_skip)]\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770f9ae-e6b0-423c-8f71-9bfeaa5d3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the older files, each year is available as one big file, or lots of little files\n",
    "# e.g. https://nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/2017/\n",
    "# We don't want to download both\n",
    "# drop the few large files, keep the little ones\n",
    "urls = [u for u in  urls if not re.search(r\"MMSDM_\\d{4}_\\d{2}.zip\", u)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4189d3-318a-4675-8bb4-21b2d8876532",
   "metadata": {},
   "source": [
    "## Download the files\n",
    "\n",
    "\n",
    "Since we're downloading a lot of files, from the other side of the world, over slow wifi, we can get an error.\n",
    "We don't want to redownload all the files when one has an error. So let's just carry on, then retry the whole lot.\n",
    "When retrying, check if the file already exists on disk. If not, dowload it.  If yes, it might be an incomplete download. To check, we ask the server for the size of the file with a HEAD request (which is almost instant, quicker than downloading it.) and compare that to the size of the file on disk. If it's the same, do not re-download the file.\n",
    "\n",
    "We get throttled a lot, which appears as 403 errors. Sleeping and retrying fixes that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07df976-e521-4714-9021-3dde56d24720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw folder if it doesn't exist\n",
    "if not os.path.exists(raw_files_path):\n",
    "    os.makedirs(raw_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bacd0-4d98-49f1-9955-e3328e49c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there are no duplicate files in different locations on the website\n",
    "\n",
    "# deduplicate based on everything after the last slash\n",
    "urls = list({u.split('/')[-1]:u for u in urls}.values())\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be123fa4-41e7-411e-b640-c9b1c63be16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shuffle, so that large files aren't clumped together.\n",
    "# this makes the progress bar estimate more accurate (compared to downloading all the big files last/first)\n",
    "# and if we don't download all the small files consecutively, we're less likely to be throttled\n",
    "shuffle(urls) # mutates list in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc8905-b8b1-4291-8baa-85d6e433eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# using urllib3 instead of requests\n",
    "# because it's better for streaming large files to disk\n",
    "# https://stackoverflow.com/a/62075390/5443120\n",
    "http = urllib3.PoolManager(retries=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b80bd4-5322-42a7-a111-8ac44d4f1d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_remote_size(url, retries=6):\n",
    "    r = http.request('HEAD', url)\n",
    "    if r.status >= 300:\n",
    "        if retries > 0:\n",
    "            logger.warning(f\"Retrying after bad status ({r.status}) for HEAD {url}\")\n",
    "            sleep(randrange(5))\n",
    "            return get_remote_size(url, retries=retries-1)\n",
    "        else:\n",
    "            logger.error(f\"Not retrying after bad status ({r.status}) for HEAD {url}\")\n",
    "            raise ValueError(f\"bad status ({r.status}) for HEAD {url}\")\n",
    "    return int(r.headers['Content-Length'])\n",
    "\n",
    "def already_downloaded(url, local):\n",
    "    if not os.path.exists(local):\n",
    "        return False\n",
    "    else:\n",
    "        return get_remote_size(url) == os.path.getsize(local)\n",
    "\n",
    "def download(url, retries=6):\n",
    "    fname = url.split('/')[-1]\n",
    "    local_path = os.path.join(raw_files_path, fname)\n",
    "    try:\n",
    "        if already_downloaded(url, local_path):\n",
    "            logger.info(f\"Skipping already downloaded {url}\")\n",
    "            return 'skipped'\n",
    "        else:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                r = http.request('GET', url, preload_content=False)\n",
    "                if r.status >= 300:\n",
    "                    logger.warning(f\"Retrying after bad status ({r.status}) for GET {url}\")\n",
    "                    sleep(randrange(5))\n",
    "                    return download(url, retries=retries-1)\n",
    "                logger.info(f\"Downloading {url}\")\n",
    "                shutil.copyfileobj(r, f)\n",
    "                logger.info(f\"Downloaded {url}\\n\")\n",
    "    except (urllib3.exceptions.HTTPError, ValueError) as ex:\n",
    "        # tidy up partial download\n",
    "        try:\n",
    "            os.remove(local_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        if retries <= 0:\n",
    "            logger.error(f\"Not retrying after error with {url}: {ex}\")\n",
    "            return ex\n",
    "        else:\n",
    "            logger.warning(f\"Retrying after error with {url}: {ex}\")\n",
    "            sleep(randrange(3))\n",
    "            return download(url, retries=retries-1)\n",
    "    return 'ok'\n",
    "                \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # use multiprocessing to process the files concurrently\n",
    "    # tqdm for a progress bar\n",
    "    # list(tqdm(imap())) thing explained here: https://stackoverflow.com/a/41921948/5443120\n",
    "    if use_multiprocessing:\n",
    "        with Pool(2*os.cpu_count()) as p:\n",
    "            statuses = list(tqdm(p.imap(download, urls), total=len(urls)))\n",
    "    else:\n",
    "        statuses = [download(url) for url in tqdm(urls)]\n",
    "\n",
    "    # if these are only HEAD 404s for a few files, that's ok. They've probably moved from /CURRENT to /ARCHIVE (under a different file name)\n",
    "    # re-download later and you'll get them\n",
    "    assert all(s in ['ok', 'skipped'] for s in statuses), \"some files failed to be downloaded.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de404e-b000-45f7-a5eb-5f7feab257db",
   "metadata": {},
   "source": [
    "## Unzip\n",
    "\n",
    "We list all files in the directory, instead of getting the list of files from earlier. This is because we may want to re-run the playbook from this point, without re-downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1baac1-890a-4b5e-99e6-aa4d0be12612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw folder if it doesn't exist\n",
    "for d in [unzipped_files_path, unzipping_temp_path]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c981f33-0a63-46c8-8775-595ab9fb4970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.reset()\n",
    "\n",
    "# takes a CSV\n",
    "# fname is the name of the CSV\n",
    "# with f being an optional virtual file-like object inside an unzipped zip\n",
    "# we write to a random filename, then move to the destination\n",
    "# so that if two processes are unzipping different files to the same destination concurrently, they won't corrupt the file.\n",
    "def process_csv(fname, f=None):\n",
    "    assert fname.lower().endswith('.csv')\n",
    "    temp_path = os.path.join(unzipping_temp_path, str(uuid4())) # random file within that folder\n",
    "    dest = os.path.join(unzipped_files_path, fname.split('/')[-1])\n",
    "    if not dest.lower().endswith('.gz'):\n",
    "        dest = dest + '.gz'\n",
    "        \n",
    "    if f is None:\n",
    "        with open(fname, 'rb') as f:\n",
    "            process_csv(fname, f)\n",
    "    else:\n",
    "        with gzip.open(temp_path, 'wb', compresslevel=compresslevel) as out:\n",
    "            shutil.copyfileobj(f, out)\n",
    "        os.rename(temp_path, dest)\n",
    "        \n",
    "def process_zip(fname, f=None):\n",
    "    if f is None:\n",
    "        with open(fname, 'rb') as f:\n",
    "            process_zip(fname, f)\n",
    "    else:\n",
    "        #print(f\"Processing {fname}\")\n",
    "        with ZipFile(f) as z:\n",
    "            for m in z.namelist():\n",
    "                if m.lower().endswith('.zip'):\n",
    "                    with z.open(m, mode='r') as mf:\n",
    "                        process_zip(m, mf)\n",
    "                elif m.lower().endswith('.csv'):\n",
    "                    with z.open(m, mode='r') as mf:\n",
    "                        process_csv(m, mf)\n",
    "                else:\n",
    "                    logger.warning(f\"Found unknown file {m} in zip {fname}\")\n",
    "\n",
    "\n",
    "def process(path):\n",
    "    if path.lower().endswith('.csv'):\n",
    "        process_csv(path)\n",
    "    else:\n",
    "        process_zip(path)\n",
    "\n",
    "# call process()\n",
    "# catch (almost) all errors, and return them\n",
    "# then after we process all files, check these errors\n",
    "def _process(path):\n",
    "    try:\n",
    "        process(path)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as ex: # catch anything except keyboard interrupt\n",
    "        logger.error(f\"Failed to unzip {path}: {ex}\")\n",
    "        print(f\"Failed to unzip {path}: {ex}\")\n",
    "        return ex # not rethrown\n",
    "        \n",
    "# tqdm is used to show a progress bar\n",
    "# If you want to add multiprocessing here, watch out for file clobbering\n",
    "# multiple source zip files may contain duplicates of the same destination .csv\n",
    "# if you try to write to the dest file twice concurrently, you'll get corrupted files, but may not notice\n",
    "# For a given .csv filename, the content is always the same if you find it in different source zips\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    files = [os.path.join(raw_files_path, p) for p in os.listdir(raw_files_path)]\n",
    "\n",
    "    # shuffle file order\n",
    "    # so that we don't do all the small files first\n",
    "    # then all the large files last \n",
    "    # (or the other way around)\n",
    "    # this makes the progress bar more accurate\n",
    "    shuffle(files) # mutates list in place\n",
    "    \n",
    "    if use_multiprocessing:\n",
    "        if leave_unused_cpu:\n",
    "            num_processes = os.cpu_count() - 2 # *2 because we assume hyperthreading\n",
    "        else:\n",
    "            num_processes = os.cpu_count()\n",
    "        with Pool(num_processes) as p:\n",
    "            statuses = list(tqdm(p.imap(_process, files), total=len(files)))\n",
    "    else:\n",
    "        statuses = [_process(file) for file in tqdm(files)]\n",
    "    assert all(s is None for s in statuses), \"some files failed to be unzipped\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acedd5-f145-4ee3-aa86-e116fabc800b",
   "metadata": {},
   "source": [
    "## Convert Files\n",
    "\n",
    "After unzipping files, we're left with what look like a bunch of CSVs. (Or `.csv.gz`, which I'm hoping excel can open.) But if you open them up you'll see they're not a normal CSV. They're a concatenation of many CSV files into one. (e.g. note how the number of columns changes as you scroll down.) So we want to unconcatenate them, or \"split\" them. So each \".CSV\" file becomes many \".CSV\" files.\n",
    "\n",
    "To understand this format, read `Guide to CSV Data Format Standard.pdf`, which is saved adjacent to this file. Available [here](https://web.archive.org/web/20230414160249/https://aemo.com.au/-/media/files/market-it-systems/guide-to-csv-data-format-standard.pdf?la=en).\n",
    "\n",
    "To read these files, look at the first column, which is C, D, or I.\n",
    "\n",
    "* `C` means this is metadata. Typically the first and last row of the file. The first row tells you things like when the file was generated. We can probably ignore this. (Other timestamps are inside the data itself.)\n",
    "* `I` means this is the header row for a new table\n",
    "* `D` means this is a data row for the same table as the previous row\n",
    "\n",
    "In the `D` and `I` rows, the second and third column contain info about which table this data belongs to (`REPORT_NAME` and `REPORT_SUBTYPE`). This is described more below. For now we put output files under a 2-layer folder structure based on this. \n",
    "\n",
    "The fourth column is an integer related to some kind of schema versioning. It's really nuanced. I don't think this will matter for our purposes. \n",
    "\n",
    "Note that pandas cannot read these files (because the datatype changes across rows within the same file, and the number of rows changes.) So we just read line-by-line as text. Once it's split, at a later stage we'll read with Pandas (or R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8521192-30b9-4fad-973c-e06226d4f65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've found some particular files have funny encodings\n",
    "# just ignore them. We don't need them for our analysis.\n",
    "csvgz_files_to_ignore = [\n",
    "    # regex patterns\n",
    "    f\"7_days_High_Impact_Outages_\\d+.csv.gz\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6308301-be2e-4162-8910-bf0b68f313e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.reset()\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "if not os.path.exists(split_unmapped_files_path):\n",
    "    os.makedirs(split_unmapped_files_path)\n",
    "\n",
    "# call split\n",
    "# catch (almost) all errors and return them\n",
    "# so that if one file fails, we still process the others\n",
    "# and then check errors all at the end.\n",
    "def _split(fname):\n",
    "    try:\n",
    "        split(fname)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as ex:\n",
    "        if any(re.search(ptn, fname) for ptn in csvgz_files_to_ignore):\n",
    "            logger.warning(f\"Failed to split {fname}: {ex}, but ignoring this particular file\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to split {fname}: {ex}\")\n",
    "            return ex\n",
    "\n",
    "def split(fname):\n",
    "    source_path = os.path.join(unzipped_files_path, fname)\n",
    "\n",
    "    if fname.lower().endswith('.gz'):\n",
    "        with gzip.open(source_path, 'rt', newline='') as f:\n",
    "            split_f(f, fname)\n",
    "    else:\n",
    "        assert fname.lower().endswith('.csv'), f\"Unsure how to open {fname}\"\n",
    "        with open(source_path, 'r', newline='') as f:\n",
    "            split_f(f, fname)\n",
    "        \n",
    "\n",
    "def split_f(src, fname):\n",
    "    csv_r = csv.reader(src)\n",
    "    first_row = next(csv_r)\n",
    "    expected_start = 'C,SETP.WORLD'\n",
    "    if first_row[0] != 'C':\n",
    "        # some other files end up in the dataset\n",
    "        # e.g. int668_v1_schedule_log_rpt_1~20231124133149.csv.gz\n",
    "        # just ignore files like that\n",
    "        logger.warning(f\"First line does not start with C, {first_row[:10]=} in {fname}. Ignoring\")\n",
    "        return\n",
    "\n",
    "    row = next(csv_r)\n",
    "\n",
    "    # these will be set later\n",
    "    dst = None\n",
    "    chars_to_skip = None\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if row[0] == 'C':\n",
    "                # 2nd C line, which should be the last line of the file\n",
    "                if row[1] == 'END OF REPORT':\n",
    "                    checksum = row[-1]\n",
    "                    assert int(checksum) == csv_r.line_num, f\"Checksum on last line doesn't match in {fname}\"\n",
    "        \n",
    "                    # close off the last output file\n",
    "                    assert dst is not None\n",
    "                    dst = None \n",
    "                    fout.close()\n",
    "                    \n",
    "                    assert src.read().strip() == ''\n",
    "\n",
    "                    if num_d_rows == 0:\n",
    "                        logger.warning(f\"No data rows, only header row written for {fname}, so deleting output file\")\n",
    "                        os.remove(dest_path)\n",
    "                    \n",
    "                    break\n",
    "                elif row[1] == 'SETTLEMENTS RESIDUE CONTRACT REPORT':\n",
    "                    logger.info(f\"Ignoring C line {csv_r.line_num} in {fname}: {row[1]}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected C line {csv_r.line_num} in {fname}: {row}\")\n",
    "            elif row[0] == 'I':\n",
    "                # start of new file\n",
    "                row_type, report_name, report_subtype, version, *remainder = row\n",
    "                cols_to_skip = 4\n",
    "    \n",
    "                if report_name in ['', None]:\n",
    "                    report_name = REPORT_NAME_NULL_PLACEHOLDER\n",
    "                if report_subtype in ['', None]:\n",
    "                    report_subtype = REPORT_SUBTYPE_NULL_PLACEHOLDER\n",
    "                \n",
    "                # create destination folder if it doesn't exist\n",
    "                dest_folder = os.path.join(split_unmapped_files_path, report_name, report_subtype)\n",
    "                if not os.path.exists(dest_folder):\n",
    "                    try:\n",
    "                        os.makedirs(dest_folder)\n",
    "                    except FileExistsError:\n",
    "                        # race condition\n",
    "                        pass\n",
    "                \n",
    "                dest_path = os.path.join(dest_folder, fname)\n",
    "                fout = gzip.open(dest_path, 'wt', compresslevel=compresslevel)\n",
    "                dst = csv.writer(fout)\n",
    "                dst.writerow(remainder)\n",
    "                num_d_rows = 0\n",
    "    \n",
    "            else:\n",
    "                assert row[0] == 'D'\n",
    "                dst.writerow(row[cols_to_skip:])\n",
    "                num_d_rows += 1\n",
    "    \n",
    "            row = next(csv_r)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {fname}: {e}\")\n",
    "        # tidy up partial write\n",
    "        try:\n",
    "            fout.close()\n",
    "        except (NameError, OSError):\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(dest_path)\n",
    "        except (NameError, OSError):\n",
    "            pass\n",
    "        raise # rethrow original error\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    files = os.listdir(unzipped_files_path)\n",
    "\n",
    "    # shuffle file order\n",
    "    # so that we don't do all the small files first\n",
    "    # then all the large files last \n",
    "    # (or the other way around)\n",
    "    # this makes the progress bar more accurate\n",
    "    # it also means that we process the first file of each type sooner\n",
    "    # so that if there's issues, we can spot them in the logs sooner (e.g. 20 minutes in, instead of 1h in.)\n",
    "    shuffle(files) # mutates list in place\n",
    "    \n",
    "    if use_multiprocessing:\n",
    "        if leave_unused_cpu:\n",
    "            num_processes = os.cpu_count() - 2 # *2 because we assume hyperthreading\n",
    "        else:\n",
    "            num_processes = os.cpu_count()\n",
    "        with Pool(num_processes) as p:\n",
    "            statuses = list(tqdm(p.imap(_split, files), total=len(files)))\n",
    "    else:\n",
    "        statuses = [_split(file) for file in tqdm(files)]\n",
    "    \n",
    "    [(s,f) for (s,f) in zip(statuses, files) if s is not None][:10]\n",
    "    assert all(s is None for s in statuses), \"some files failed to be split\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5c757-ae69-4584-95e3-da8af967e2f4",
   "metadata": {},
   "source": [
    "## Map each file to a table\n",
    "\n",
    "We now look at the REPORT_TYPE and REPORT_SUBTYPE to figure out which table each one belongs to.\n",
    "\n",
    "Sometimes it's obvious. E.g. `DISPATCH` and `PRICE` belongs to the `DISPATCHPRICE` table. But sometimes it's not obvious. (And sometimes there are blanks.) The best way I've found is to look up those two values in a particular file. That file is no longer on the internet. But we can find an older version [here](https://web.archive.org/web/20220812083311/https://visualisations.aemo.com.au/aemo/di-help/Content/Data_Model/MMS_Data_Model.htm). (That's for schema version 5.1. We're up to 5.2 now. But the differences are small enough that it shouldn't be an issue. We care about so few tables that we could probably hard-code this mapping.) \n",
    "\n",
    "Here we just move the files on disk from one folder to another. We don't copy and re-write them.\n",
    "\n",
    "We could have done this step when we originally wrote the files. But I deliberately split it out to make it clearer what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead197c-ee81-416d-91a4-b9255c8d50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_excel(metadata_path, sheet_name=sheet)\n",
    "metadata_df[report_name_column].fillna(REPORT_NAME_NULL_PLACEHOLDER, inplace=True)\n",
    "metadata_df[report_subtype_column].fillna(REPORT_SUBTYPE_NULL_PLACEHOLDER, inplace=True)\n",
    "\n",
    "# there are some packages I do not know how to map to tables\n",
    "\n",
    "packages_to_ignore = [\n",
    "    # maybe there's a table for these,\n",
    "    # or they're legacy tables\n",
    "    # these are ones we don't care about\n",
    "    ('DINT', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('TINT', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('DCONS', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('DREGION', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('SPDCPC', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('SRAFINANCIALS', 'RECONCILIATION_SUMMARY'),\n",
    "    ('DAILY', 'MLF'), # electrical transmission loss factors.\n",
    "    ('BILLING_CONFIG', 'BILLSMELTERRATE'), # alumninium smelter info. Possibly relevant? (Smeltering makes up 30% of NSW load)\n",
    "    ('BILLING', 'CSP_SUPPORTDATA_SRA'),\n",
    "    ('BILLING', 'ASPAYMENT_SUMMARY'), # probably belongs to BILLINGASPAYMENTS table, but this is not relevant to us so I haven't bothered checking\n",
    "    ('BILLING', 'DIRECTION_CRA'),\n",
    "    ('TRADING', 'CUMULATIVE_PRICE'),\n",
    "    ('TREGION', 'NULL'),\n",
    "    ('DAILY', 'WDR_NO_SCADA'),\n",
    "    ('METER_DATA', 'GEN_DUID'),\n",
    "    ('SEVENDAYOUTLOOK', 'PEAK'),\n",
    "    ('TUNIT', 'NULL'),\n",
    "    ('GPG', 'MARKET_SUMMARY'),\n",
    "    ('GPG', 'CASESOLUTION'),\n",
    "    ('GPG', 'CONSTRAINTSOLUTION'),\n",
    "    ('GPG', 'PRICESOLUTION'),\n",
    "    ('GPG', 'INTERCONNECTORSOLUTION'),\n",
    "    ('CAUSER_PAYS_SCADA', 'NETWORK'),\n",
    "    ('PDR_REPORT', 'COLUMN'),\n",
    "    ('DUNIT', 'NULL'),\n",
    "    ('YESTBID', 'BIDDAYOFFER'),\n",
    "    ('YESTBID', 'BIDPEROFFER'),\n",
    "    ('RESIDUE_PRICE_OFFER', 'NULL'),\n",
    "    ('RESIDUE_PRICE_BID', 'NULL'), # columns don't match the RESIDUE_PRICE_BID table, almost match RESIDUE_PRICE_FUNDS_BID\n",
    "    ('IBEI', 'PUBLISHING'),\n",
    "    ('EMSLIMITS', 'LIM_ALTLIM'),\n",
    "    ('DEMAND', 'HISTORIC'),\n",
    "\n",
    "    \n",
    "    ('PDR_REPORT', 'TABLE'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'FILE'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'MAPPING'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'COLUMN'),    # manifest metadata\n",
    "\n",
    "    # Gas data\n",
    "    ('GSH', 'PARTICIPANT_OPSTATE'),\n",
    "    ('GSH', 'PARTICIPANTS'),\n",
    "    ('GSH', 'AUCTION_CURTAILMENT_NOTICE'),\n",
    "    ('GSH', 'AUCTION_PRICE_VOLUME'),\n",
    "    ('GSH', 'BENCHMARK_PRICE'),\n",
    "    ('GSH', 'PARK_SERVICES'),\n",
    "    ('GSH', 'AUCTION_QUANTITIES'),\n",
    "    ('GSH', 'FACILITIES'),\n",
    "    ('GSH', 'TRANSACTION_SUMMARY'),\n",
    "    ('GSH', 'HISTORICAL_SUMMARY'),\n",
    "    ('GSH', 'NOTIONAL_POINTS'),\n",
    "    ('GSH', 'REVISED_AUCTION_QUANTITIES'),\n",
    "    ('GSH', 'PIPELINE_SEGMENTS'),\n",
    "    ('GSH', 'CAPACITY_TRANSACTION'),\n",
    "    ('GSH', 'ZONES'),\n",
    "    ('GSH', 'SERVICE_POINTS'),\n",
    "\n",
    "    # we should maybe double check\n",
    "    ('FORCE_MAJEURE', 'MARKETSUSREGION'),\n",
    "    ('FORCE_MAJEURE', 'MARKETSUSPENSION')\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# there are some packages which aren't in the spreadsheet, but we can guess them\n",
    "# in particular there's a CO2 one I can't \n",
    "package_exceptions = {\n",
    "    ('DISPATCH', 'CASESOLUTION'): 'DISPATCHCASESOLUTION',\n",
    "    ('GENCONSETTRK', REPORT_SUBTYPE_NULL_PLACEHOLDER): 'GENCONSETTRK',\n",
    "    ('DISPATCH', 'REGIONFCASREQUIREMENT'): 'DISPATCH_FCAS_REQ',\n",
    "\n",
    "    # this one is possibly useful\n",
    "    # example file is CO2EII_AVAILABLE_GENERATORS.CSV.gz\n",
    "    # It looks like it might go into BILLING_CO2E_PUBLICATION, but the columns are different\n",
    "    # Example:\n",
    "    #STATIONNAME,DUID,GENSETID,REGIONID,CO2E_EMISSIONS_FACTOR,CO2E_ENERGY_SOURCE,CO2E_DATA_SOURCE\n",
    "    #\"Appin Power Plant\",APPIN,APPIN,NSW1,0.56318004,\"Coal seam methane\",NGA2022\n",
    "    # so define a new table for it\n",
    "    ('CO2EII', 'PUBLISHING'): 'CO2EII_AVAILABLE_GENERATORS',\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "def map_table(report_name, report_subtype):\n",
    "    candidates = metadata_df.loc[(metadata_df[report_name_column] == report_name) & (metadata_df[report_subtype_column] == report_subtype), table_name_column]\n",
    "    # sometimes there's duplicates\n",
    "    # but assert they're all the same answer\n",
    "    tables = set(candidates)\n",
    "    if len(tables) == 0 and (report_name, report_subtype) in package_exceptions:\n",
    "        return package_exceptions[(report_name, report_subtype)]\n",
    "        \n",
    "    assert len(tables) > 0, f\"Unable to map {report_name=} {report_subtype=} to a table name. If you don't need this table, add ('{report_name}', '{report_subtype}') to packages_to_ignore\"\n",
    "    assert len(tables) == 1, f\"Multiple target tables found for  {report_name=} {report_subtype=}: {tables}\"\n",
    "    table = tables.pop() # choose the only one\n",
    "    return table\n",
    "\n",
    "# unit testing\n",
    "assert map_table('DISPATCH', 'PRICE') == 'DISPATCHPRICE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9dea70-c807-40bd-bab2-04b11e532e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't bother with multiprocessing for this\n",
    "# moving a folder of N files is O(1) not O(N)\n",
    "unmappable = []\n",
    "for report_name in tqdm(os.listdir(split_unmapped_files_path)):\n",
    "    subdir = os.path.join(split_unmapped_files_path, report_name)\n",
    "    for report_subtype in os.listdir(subdir):\n",
    "        subsubdir = os.path.join(subdir, report_subtype)\n",
    "        \n",
    "        if (report_name, report_subtype) in packages_to_ignore:\n",
    "            # leave these files where they are\n",
    "            continue\n",
    "        if len(os.listdir(subsubdir)) == 0:\n",
    "            # empty folder, don't bother mapping\n",
    "            continue\n",
    "        try:\n",
    "            table = map_table(report_name, report_subtype)\n",
    "        except AssertionError:\n",
    "            unmappable.append((report_name, report_subtype))\n",
    "        else:\n",
    "            # note that multiple source directories may map to the same destination directory\n",
    "            # so we move files one at a time, instead of moving the whole directory and erasing what's there\n",
    "            for filename in os.listdir(subsubdir):\n",
    "                source_path = os.path.join(subsubdir, filename)\n",
    "                dest_dir = os.path.join(split_mapped_files_path, table)\n",
    "                if not os.path.exists(dest_dir):\n",
    "                    os.makedirs(dest_dir)\n",
    "                dest_path = os.path.join(dest_dir, filename)\n",
    "                shutil.move(source_path, dest_path)\n",
    "assert len(unmappable) == 0, f\"Unable to map some files to tables. Details in unmappable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc938ca-d188-4468-9ec9-cb086d71fa3f",
   "metadata": {},
   "source": [
    "## Convert many CSV to Parquet\n",
    "\n",
    "We have many files per folder. We want to combine into just one parquet file. Some are too large to fit in memory.\n",
    "So we stream the files when converting, using Arrow. (We don't use pandas because AEMO data has missing values in columns which are integers, datetimes etc. Pandas doesn't like that.) \n",
    "\n",
    "But Arrow throws an error when combining one file with an integer column, and another file with double/float/decimal in the same column. It's too dumb to merge those automatically. So we need to tell it explicitly the data type of decimal columns.\n",
    "\n",
    "Note that AEMO's schemas change over time. New columns get added over the years. So a particular column might be missing in some files. And the column ordering is not consistent.\n",
    "\n",
    "Note also that some files have no value in a certain column for the first million rows, then a value appears. Pyarrow and pandas both check only the first few rows to detect datatypes in CSV files. \n",
    "\n",
    "Note that some cells can be like `-7E-05`. So there's a letter character, but it's a float.\n",
    "\n",
    "Note that there's too many datasets to explicitly hard-code each column type. (And AEMO does not offer a machine-readable version. Unless you use their proprietary software that's complex and not available to researchers.) So we want an automatic way of detecting data types.\n",
    "\n",
    "So our solution here is:\n",
    "\n",
    "1. We go through each file in a folder, find the union of all the column names.\n",
    "2. We read through each file, checking each column, each row. We want to keep track of columns which are truly integers everywhere, vs ones which have at least one float (number with a decimal point). We also keep track of what's a string, bool etc. This is using out custom `SchemaType` type. For this step we tell pandas that all columns are string type, and we try to y\u0000p\u0000e\u0000t to int/float etc, and see if there's an error. We do this with pandas, batching large files.\n",
    "3. For each folder (set of files that belong in the same final file) we call pyarrow, point it at the folder of input CSVs, and tell it to write a single output CSV. We tell pyarrow explicitly what datatypes of each column are. For timestamps, pyarrow can't read timestamps from CSV. So we tell pyarrow to read them as a string, and then we convert to a datetime in a step before writing. This is a bit fiddle because we're chunking the file into batches. Note that handling the timezone is a bit tricky.\n",
    "4. We confirm that the timezone was handled correctly by unit-testing a specific value in a final parquet file.\n",
    "\n",
    "We could also group by some columns. e.g. to write one parquet file per region. This might improve query performance later. But we haven't done this yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507bab4d-ad13-4dd3-b930-3f5340c3a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(one_csv_per_table_path):\n",
    "    os.makedirs(one_csv_per_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba962bda-13f2-4216-b52c-a8118b3b21b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next cell will take a long time to run\n",
    "# and uses lots of CPU\n",
    "# If you want to free up CPU to use your laptop for something else,\n",
    "# without losing progress,\n",
    "# create this file next to this ipynb file\n",
    "pause_path = 'pause.txt'\n",
    "def check_for_pause():\n",
    "    if os.path.exists(pause_path):\n",
    "        logger.info(\"paused\")\n",
    "        while os.path.exists(pause_path):\n",
    "            sleep(5)\n",
    "        logger.info(\"Resumed\")\n",
    "\n",
    "check_for_pause()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e9c35-c1fa-417d-9366-e6e417478ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we iterate through the data row by row\n",
    "# this object keeps track of the datatypes we've seen for a particular column\n",
    "class SchemaType:\n",
    "    # start with all attributes True\n",
    "    # i.e. the corresponding column could be any datatype\n",
    "    # When we see a value not compatible with any of these datatypes\n",
    "    # set it to False\n",
    "    empty = True # are all values in the column empty?\n",
    "    bool = True # a.k.a. binary\n",
    "    int = True\n",
    "    decimal = True # a.k.a. float/double\n",
    "    datetime = True\n",
    "    time = True # time without date\n",
    "    string = True\n",
    "\n",
    "    # if it's an int, what's the min and max values seen?\n",
    "    # to figure out int32 vs int64 vs uint32\n",
    "    max_int = None\n",
    "    min_int = None\n",
    "\n",
    "    # take in a numpy array (column of a pandas dataframe)\n",
    "    # which is a string (that may have null values)\n",
    "    # and check whether it's incomptible with each datatype\n",
    "    # this will be called many times per column\n",
    "    def update(self, vals: np.array):\n",
    "        # ignore empty values\n",
    "        vals = vals.dropna()\n",
    "        if len(vals) == 0:\n",
    "            return\n",
    "        else:\n",
    "            self.empty = False\n",
    "\n",
    "        # For booleans, only 1 and 0 is in AEMO data. Not T, true, Y etc\n",
    "        # but watch out. Some data is \"01\". We don't want to count that as boolean.\n",
    "        # (Arrow won't convert that later.)\n",
    "        # note that vals is an array of strings (of int or whatever)\n",
    "        if self.bool:\n",
    "            self.bool &= vals.isin(['0', '1']).all()\n",
    "        \n",
    "        if self.int:\n",
    "            try:\n",
    "                i = vals.astype(int)\n",
    "            except:\n",
    "                self.int = False\n",
    "            else:\n",
    "                # it is an int. Update min/max values for this column\n",
    "                if self.max_int is None:\n",
    "                    self.max_int = i.max()\n",
    "                    self.min_int = i.min()\n",
    "                else:\n",
    "                    self.max_int = max(i.max(), self.max_int)\n",
    "                    self.min_int = min(i.min(), self.min_int)\n",
    "\n",
    "\n",
    "        # not elif, since we want to go here if we've just set self.int to False after `if self.int`\n",
    "        if self.decimal and (not self.int):\n",
    "            # remember that floats can be like '1e2', so not just digits and a dot\n",
    "            try:\n",
    "                vals.astype(float)\n",
    "            except ValueError:\n",
    "                self.decimal = False\n",
    "\n",
    "        if self.datetime:\n",
    "            # e.g. 2023/10/01 00:00:00\n",
    "            # not using strptime yet. We just want to check that it is a datetime.\n",
    "            # regex match is faster than strptime\n",
    "            matches = vals.str.match(r\"\\d{4}/\\d{1,2}/\\d{1,2} \\d{1,2}:\\d{1,2}:\\d{1,2}\")\n",
    "            if not matches.all():\n",
    "                self.datetime = False\n",
    "                \n",
    "        if self.time:\n",
    "            # e.g. 12:34:56\n",
    "            matches = vals.str.match(r\"\\d{1,2}:\\d{1,2}:\\d{1,2}\")\n",
    "            if not matches.all():\n",
    "                self.time = False\n",
    "\n",
    "    # return a datatype, as a pyarrow datatype\n",
    "    # which is not the same as a normal python datatype\n",
    "    # e.g. pyarrow.bool_() instead of just bool\n",
    "    def get_type(self) -> pyarrow.DataType:\n",
    "        if self.empty:\n",
    "            # all rows have an empty value\n",
    "            # choose anything\n",
    "            logger.info(f\"Col is empty\")\n",
    "            return pyarrow.string()\n",
    "        elif self.bool:\n",
    "            return pyarrow.bool_()\n",
    "        elif self.int:\n",
    "            # it's an integer, not decimal\n",
    "            # but is it signed/unsigned, and how many bits?\n",
    "            assert self.min_int is not None\n",
    "            assert self.max_int is not None\n",
    "            if self.min_int >= 0:\n",
    "                # unsigned\n",
    "                _type = pyarrow.int64()\n",
    "                for bits in [32, 16, 8]:\n",
    "                    if self.max_int <= 2**bits - 1:\n",
    "                        _type = getattr(pyarrow, f\"uint{bits}\")()\n",
    "            else:\n",
    "                # signed\n",
    "                _type = pyarrow.int64()\n",
    "                for bits in [32, 16, 8]:\n",
    "                    if (self.max_int <= 2**(bits-1) - 1) and (self.min_int >= -2**(bits-1) ):\n",
    "                        _type = getattr(pyarrow, f\"int{bits}\")()\n",
    "            return _type\n",
    "        elif self.decimal:\n",
    "            # we could differentiate between 32 and 64. I can't be bothered.\n",
    "            # I think R treats everything as 64 bits anyway\n",
    "            return pyarrow.float64()\n",
    "        else:\n",
    "            # includes arbitrary strings, as well as datetime\n",
    "            # datetime will be parsed from CSV as string\n",
    "            # and then converted to datetime/time as a second step afterwards\n",
    "            return pyarrow.string()\n",
    "\n",
    "# takes in the name of a table (one folder of CSVs)\n",
    "# returns a dictionary of column name to SchemaType\n",
    "def detect_schema(table) -> Dict[str, SchemaType]:\n",
    "    source_dir = os.path.join(split_mapped_files_path, table)\n",
    "\n",
    "    # first loop through each file, read just the first row to get the header row\n",
    "    logger.info(f\"Checking headers in {table}\")\n",
    "    schema = {}\n",
    "    \n",
    "    for file in os.listdir(source_dir):\n",
    "        check_for_pause()\n",
    "        path = os.path.join(source_dir, file)\n",
    "        assert path.lower().endswith('.csv.gz')\n",
    "\n",
    "        for df in pd.read_csv(path, chunksize=100000, dtype=str):\n",
    "            t = pyarrow.Table.from_pandas(df)\n",
    "            for col in df:\n",
    "                if col not in schema:\n",
    "                    schema[col] = SchemaType()\n",
    "                schema[col].update(df[col])\n",
    "    \n",
    "    return schema\n",
    "\n",
    "# catch errors for multiprocessing\n",
    "# so if one fails, the others keep going\n",
    "def _detect_schema(table):\n",
    "    try:\n",
    "        schema = detect_schema(table)\n",
    "    except Exception as ex:\n",
    "        logger.exception(f\"Failed to process {table}\")\n",
    "        logger.error(ex)\n",
    "        return [None, ex]\n",
    "    else:\n",
    "        return [schema, None]\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    folders = os.listdir(split_mapped_files_path)\n",
    "    \n",
    "    if use_multiprocessing:\n",
    "        if leave_unused_cpu:\n",
    "            num_processes = os.cpu_count() - 2 # *2 because we assume hyperthreading\n",
    "        else:\n",
    "            num_processes = os.cpu_count()\n",
    "        with Pool(num_processes) as p:\n",
    "            result = list(tqdm(p.imap(detect_schema, folders), total=len(folders)))\n",
    "    else:\n",
    "        result = [detect_schema(folder) for folder in tqdm(folders)]\n",
    "\n",
    "\n",
    "    schemas = dict(zip(folders, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429d297-81f5-42fa-be5d-d2caaf7b7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a file, for debugging purposes\n",
    "schema_pickle_path = os.path.join(base_data_dir, 'schemas.pickle')\n",
    "with open(schema_pickle_path, 'wb') as f:\n",
    "    pickle.dump({'test': schemas}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43934cb-abff-4005-8db7-a2581a4334c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(one_parquet_per_table_path):\n",
    "    os.makedirs(one_parquet_per_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fb731-08e2-42f2-a936-8610e91bb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in the name of a folder of CSVs\n",
    "# converts them all to a single parquet file\n",
    "def convert_csv_parquet(table):\n",
    "    csv_folder = os.path.join(split_mapped_files_path, table)\n",
    "    parquet_file = os.path.join(one_parquet_per_table_path, table + '.parquet')\n",
    "\n",
    "    input_schema = {}\n",
    "    output_schema = {}\n",
    "    datetime_columns = []\n",
    "    column_names = []\n",
    "    for (c,s) in schemas[table].items():\n",
    "        input_schema[c] = s.get_type()\n",
    "        if s.datetime:\n",
    "            datetime_columns.append(c)\n",
    "            output_schema[c] = pyarrow.timestamp('s', tz=f'+{TIMEZONE_OFFSET:02d}:00')\n",
    "        else:\n",
    "            output_schema[c] = s.get_type()\n",
    "        column_names.append(c)\n",
    "    input_schema = pa.schema(input_schema)\n",
    "    output_schema = pa.schema(output_schema)\n",
    "            \n",
    "    \n",
    "    with pyarrow.parquet.ParquetWriter(parquet_file, output_schema) as writer:\n",
    "        for csv_file in os.listdir(csv_folder):\n",
    "            try:\n",
    "                csv_path = os.path.join(csv_folder, csv_file)\n",
    "                csv_reader = pyarrow.csv.open_csv(csv_path, \n",
    "                                                  convert_options=pyarrow.csv.ConvertOptions(\n",
    "                                                      column_types=input_schema, \n",
    "                                                      include_missing_columns=True, \n",
    "                                                      include_columns=column_names))\n",
    "    \n",
    "                for batch in csv_reader:\n",
    "                    check_for_pause()\n",
    "                    columns = []\n",
    "                    if datetime_columns == []:\n",
    "                        # table has no datetime columns\n",
    "                        # just write straight into the parquet file\n",
    "                        writer.write(batch)\n",
    "                    else:\n",
    "                        # parse some columns from string to datetime\n",
    "                        \n",
    "                        for (i, column_name) in enumerate(column_names):\n",
    "                            assert isinstance(column_name, str), f\"column_name is type {type(column_name)}\"\n",
    "                            column_data = batch[column_name]\n",
    "                            if column_name in datetime_columns:\n",
    "                                # pyarrow assumes the timesstamp is UTC by default\n",
    "                                # the schema specifies the timezone as UTC+10 (which we want)\n",
    "                                # but it adds 10h when doing the conversion\n",
    "                                # so let's subtract 10h\n",
    "                                # This is checked with a unit test later\n",
    "                                column_data = pc.strptime(column_data, format=\"%Y/%m/%d %H:%M:%S\", unit=\"s\", error_is_null=True)\n",
    "                                column_data = pc.add(column_data, -dt.timedelta(hours=TIMEZONE_OFFSET))\n",
    "                            columns.append(column_data)\n",
    "                        \n",
    "                        updated_table = pyarrow.Table.from_arrays(\n",
    "                            columns, \n",
    "                            schema=output_schema\n",
    "                        )\n",
    "                        writer.write(updated_table)\n",
    "            except Exception:\n",
    "                # additional logging\n",
    "                logger.error(f\"Error with file {csv_path}\")\n",
    "                raise\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folders = os.listdir(split_mapped_files_path)\n",
    "    \n",
    "    if use_multiprocessing:\n",
    "        if leave_unused_cpu:\n",
    "            num_processes = os.cpu_count() - 2 # *2 because we assume hyperthreading\n",
    "        else:\n",
    "            num_processes = os.cpu_count()\n",
    "        with Pool(num_processes) as p:\n",
    "            list(tqdm(p.imap(convert_csv_parquet, folders), total=len(folders)))\n",
    "    else:\n",
    "        for folder in tqdm(folders):\n",
    "            convert_csv_parquet(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12f8be-3c66-408f-8c9f-8127d3093c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test a specific value\n",
    "table = 'DISPATCHPRICE'\n",
    "path = parquet_file = os.path.join(one_parquet_per_table_path, table + '.parquet')\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "# check the first row of PUBLIC_DISPATCHIS_201708091630_0000000286225673.CSV\n",
    "utc_offset = dt.timedelta(hours=TIMEZONE_OFFSET)\n",
    "tz = dt.timezone(utc_offset)\n",
    "t = dt.datetime(year=2017, month=8, day=9, hour=16, minute=30, tzinfo=tz)\n",
    "\n",
    "# I manually looked in PUBLIC_DISPATCHIS_201708091630_0000000286225673.CSV\n",
    "# at the first row.\n",
    "expected_rrp = 78.47404\n",
    "vals = df.loc[(df['SETTLEMENTDATE'] == t) & (df['REGIONID'] == 'SA1'), 'RRP']\n",
    "assert len(vals) == 1\n",
    "assert (vals == expected_rrp).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
