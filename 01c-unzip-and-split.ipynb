{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d6fa6-f3ce-4d03-9566-74789fa7470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from multiprocessing import Pool, current_process\n",
    "import shutil\n",
    "from typing import List, Dict, Tuple, Optional, TextIO, BinaryIO\n",
    "import csv\n",
    "import traceback\n",
    "import json\n",
    "from random import shuffle\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import io\n",
    "import gzip\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0446d7-a46d-4628-834b-04dd20c53e0f",
   "metadata": {},
   "source": [
    "## Path constants\n",
    "\n",
    "You may need to change these, if you're trying to rerun the code on your own machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0d85b-4fb3-4db9-b060-bac1b9c7e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_one = '/media/matthew/Tux/AppliedEconometrics/data'\n",
    "disk_two = '/media/matthew/nemweb/AppliedEconometrics/data'\n",
    "repo_data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data/'\n",
    "\n",
    "source_dir = os.path.join(disk_one, '01-A-raw')\n",
    "archive_dir = os.path.join(disk_one, '01-A-raw-done')\n",
    "#source_dir = os.path.join(repo_data_dir, 'debug', 'test-in')\n",
    "temp_dir = os.path.join(repo_data_dir, '01-B-split-mapped-csv-temp/')\n",
    "dest_dir = os.path.join(repo_data_dir, '01-E-split-mapped-csv/')\n",
    "\n",
    "debug_path = os.path.join(repo_data_dir, 'debug/')\n",
    "bad_zip_dir = os.path.join(disk_two, 'error/bad_zip/')\n",
    "bad_csv_dir = os.path.join(disk_two, 'error/bad_csv/')\n",
    "log_file = os.path.join(repo_data_dir, 'logs.txt')\n",
    "\n",
    "metadata_path = os.path.join(repo_data_dir, 'MMS_Data_Model_Table_to_File_to_Report_Relationships_51.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a031477-1489-48ab-bdea-43998f7a757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when finished processing a zip\n",
    "# move it to archive_dir\n",
    "# so if we stop and restart the playbook, we don't redo that zip\n",
    "# set to True when you think the code is good.\n",
    "# set to False when still writing the code and testing,\n",
    "# or if you've changed any of the code.\n",
    "move_when_done = True\n",
    "\n",
    "\n",
    "# process files with multiprocessing, to be faster.\n",
    "# If set to False, will use a for loop, which gives clearer traceback error messages.\n",
    "use_multiprocessing = True\n",
    "\n",
    "# set to True to leave one unused CPU when multiprocessing\n",
    "# so that you can still do other stuff on your laptop without your internet browser or whatever being laggy\n",
    "leave_unused_cpu = True\n",
    "\n",
    "if leave_unused_cpu:\n",
    "    num_processes = os.cpu_count() - 2 # *2 because we assume hyperthreading\n",
    "else:\n",
    "    num_processes = os.cpu_count()\n",
    "\n",
    "# for Matt, this ended up being optimal\n",
    "# higher means more CPUs\n",
    "# but too high and the disk is the bottleneck. Don't do too many concurrent writes.\n",
    "#num_processes = 4\n",
    "\n",
    "# how hardcore the gzip algorithm is for .csv.gz files. Low to prioritise speed over size\n",
    "# but if you have an external hard drive, too low is probably slow and large\n",
    "# (it's about disk write speed vs CPU compression speed) \n",
    "compresslevel = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094b868-570c-41f0-a912-7dffa5b0e1bc",
   "metadata": {},
   "source": [
    "## Other constants\n",
    "\n",
    "You'll rarely need to change these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e985a2-a4ee-41a6-9ae3-862e2f9f7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are for what's inside metadata_path\n",
    "sheet = 'MMSDM_v5-1'\n",
    "report_subtype_column = 'PDR_REPORT_SUB_TYPE'\n",
    "report_name_column = 'PDR_REPORT_NAME'\n",
    "table_name_column = 'MMSDM_TABLE_NAME'\n",
    "\n",
    "REPORT_NAME_NULL_PLACEHOLDER = 'NULL'\n",
    "REPORT_SUBTYPE_NULL_PLACEHOLDER = 'NULL'\n",
    "\n",
    "# AEMO data is always in UTC+10\n",
    "TIMEZONE_OFFSET=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4cd050-9651-4dd9-8c72-84a0d2bd8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've found some particular files have funny encodings\n",
    "# just ignore them. We don't need them for our analysis.\n",
    "csvgz_files_to_ignore = [\n",
    "    # regex patterns\n",
    "    r\"7_days_High_Impact_Outages_\\d+.csv.gz\",\n",
    "    r\"High_Impact_Outages_\\d+\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f12965-423f-4fbb-a3e2-fbe84790b55b",
   "metadata": {},
   "source": [
    "# Stuff that should go in a shared library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3e76f-7c45-4c39-bdd7-c9314b0d76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on Linux and Mac this makes this python process lower priority\n",
    "# so when it's running and using up all your CPU, your interface won't lag.\n",
    "# So you can keep browsing the web, typing documents etc\n",
    "try:\n",
    "    os.nice(20)\n",
    "except OSError: # ignore error, this is probably on Windows\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a0488-5c21-48c2-80fc-425789ecdbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, path=log_file):\n",
    "        self.path = path\n",
    "        self.reset()\n",
    "        self.f = open(self.path, 'a')\n",
    "        \n",
    "    def write(self, msg, flush=None):\n",
    "        self.f.write(msg.rstrip() + '\\n')\n",
    "        #if (current_process().name != 'MainProcess') or flush:\n",
    "        #    # we are in a child process, doing multiprocessing\n",
    "        #    # so flush the log, to be psuedo-concurrency safe\n",
    "        self.f.flush()\n",
    "    def debug(self, msg):\n",
    "        self.write(f\"DEBUG: {msg}\")\n",
    "    def info(self, msg):\n",
    "        self.write(f\"INFO: {msg}\")\n",
    "    def warn(self, msg, flush=None):\n",
    "        self.write(f\"WARNING: {msg}\")\n",
    "    def warning(self, msg):\n",
    "        self.warn(msg)\n",
    "    def error(self, msg):\n",
    "        self.write(f\"ERROR: {msg}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.f.close()\n",
    "\n",
    "    def flush(self):\n",
    "        self.f.flush()\n",
    "        \n",
    "    # erase the file\n",
    "    # but leave a blank file in place\n",
    "    # to do that, open in `w` mode, and write nothing.\n",
    "    def reset(self):\n",
    "        with open(self.path, 'w'):\n",
    "            pass\n",
    "\n",
    "logger = Logger()\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6420d9-0d7d-4cf1-aefc-3f0e430743c2",
   "metadata": {},
   "source": [
    "# Zip and split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86b485-5602-45f6-b084-d668ea382d0e",
   "metadata": {},
   "source": [
    "## Unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc9ef9-1324-45de-9bc0-3f769d1069cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw folder if it doesn't exist\n",
    "for d in [temp_dir, dest_dir, archive_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f33c3f-05dd-47b4-8030-f56b8370a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call process()\n",
    "# catch (almost) all errors, and return them\n",
    "# then after we process all files, check these errors\n",
    "def _process_raw(path) -> Tuple[Optional[int], Optional[Exception]]:\n",
    "    try:\n",
    "        bytes_written = process_raw(path)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as ex: # catch anything except keyboard interrupt\n",
    "        logger.error(f\"Failed to unzip {path}: {ex}\")\n",
    "        return ex\n",
    "    else:\n",
    "        if move_when_done:\n",
    "            # move the file to another folder\n",
    "            # so if we stop and restart, we will not \n",
    "            archive_path = os.path.join(archive_dir, os.path.basename(path))\n",
    "            os.rename(path, archive_path)\n",
    "        return None\n",
    "\n",
    "# takes a string of a path to a file on disk\n",
    "# either .csv or .zip\n",
    "def process_raw(path: str):\n",
    "    if path.lower().endswith('.csv'):\n",
    "        with open(path, 'r', newline='') as f:\n",
    "            process_csv([path], f)\n",
    "    elif path.lower().endswith('.zip'):\n",
    "        with open(path, 'rb') as f:\n",
    "            process_zip([path], f)\n",
    "    else:\n",
    "        logger.warning(f\"Ignoring file {path}\")\n",
    "    #logger.debug(f\"Finished processing {path}\")\n",
    "\n",
    "# takes in a list of filenames\n",
    "# e.g. if there's a/myzip.zip on disk, which contains otherzip.zip\n",
    "# for the inner zip paths is ['a/myzip.zip', 'otherzip.zip']\n",
    "# These names are just for debugging and logging \n",
    "# the file-like object f is used for reading content\n",
    "def process_zip(fnames: List[str], f: BinaryIO):\n",
    "    if isinstance(fnames, str):\n",
    "        fnames = [fnames]\n",
    "        \n",
    "    try:\n",
    "        with zipfile.ZipFile(f) as z:\n",
    "            exceptions = []\n",
    "            for m in z.namelist():\n",
    "                try:\n",
    "                    if m.lower().endswith('.zip'):\n",
    "                        with z.open(m, mode='r') as mf:\n",
    "                            process_zip(fnames + [m], mf)\n",
    "                    elif m.lower().endswith('.csv'):\n",
    "                            with z.open(m, mode='r') as mfb:\n",
    "                                with io.TextIOWrapper(mfb, newline='') as mft:\n",
    "                                    process_csv(fnames + [m], mft)\n",
    "                    else:\n",
    "                        paths_debug = ' -> '.join(fnames)\n",
    "                        logger.warning(f\"Found unknown file {m} in zip {paths_debug}\")\n",
    "                \n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except Exception as ex:\n",
    "                    # ignore this one for now,\n",
    "                    # continue processing the other files in this zip\n",
    "                    # then rethrow this later\n",
    "                    exceptions.append(ex)\n",
    "    except zipfile.BadZipFile as e:\n",
    "        f.seek(0) # go back to the start of the file\n",
    "        debug_path = os.path.join(bad_zip_dir, os.path.basename(fnames[-1]))\n",
    "        if not os.path.exists(os.path.dirname(debug_path)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(debug_path))\n",
    "            except FileExistsError:\n",
    "                # race condition when multiprocessing\n",
    "                pass\n",
    "        with open(debug_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f, f_out)\n",
    "            \n",
    "        # save the error to a file\n",
    "        with open(debug_path + '.error.txt', 'w') as f_err:\n",
    "            f_err.write(f\"Issue is in file heirachy: {fnames}\\n\")\n",
    "            f_err.write(str(e) + \"\\n\")\n",
    "            traceback.print_exc(file=f_err)\n",
    "        raise\n",
    "    if exceptions:\n",
    "        # rethrow an exception from processing one file in this zip\n",
    "        raise exceptions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71ed39-bdac-4271-b932-f95bdc9c1e76",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ee917-4623-4e58-b803-1568907de797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicFile: #(io.TextIOBase):\n",
    "    # newline='' is for csv.writer\n",
    "    def __init__(self, temp_path, final_path, newline=''):\n",
    "        self.temp_path = temp_path\n",
    "        self.final_path = final_path\n",
    "        \n",
    "        # create destination folder if it doesn't exist\n",
    "        if not os.path.exists(os.path.dirname(temp_path)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(temp_path))\n",
    "            except FileExistsError:\n",
    "                # race condition when multiprocessing\n",
    "                pass\n",
    "        #logger.info(f\"Openning {temp_path} for writing\")\n",
    "        if temp_path.lower().endswith('.gz'):\n",
    "            self.temp_f = gzip.open(temp_path, mode='wt', compresslevel=compresslevel, newline=newline)\n",
    "        else:\n",
    "            self.temp_f = open(temp_path, mode='w', newline=newline)\n",
    "        self.closed = False\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        assert not self.closed\n",
    "        return self.temp_f.read(size)\n",
    "\n",
    "    def write(self, s):\n",
    "        assert not self.closed\n",
    "        return self.temp_f.write(s)\n",
    "\n",
    "    def close(self):\n",
    "        \n",
    "        if not self.closed:\n",
    "            #logger.info(f\"Closing {self.temp_path} after writing\")\n",
    "            self.temp_f.close()\n",
    "            self.closed = True\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(self.final_path)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(self.final_path))\n",
    "                except FileExistsError:\n",
    "                    # race condition when multiprocessing\n",
    "                    pass\n",
    "            \n",
    "            os.rename(self.temp_path, self.final_path)\n",
    "            #logger.info(f\"Moving {self.temp_path} to {self.final_path}\")\n",
    "            self.delete_temp_dir()\n",
    "\n",
    "    def delete_temp_file(self):\n",
    "        try:\n",
    "            os.remove(self.temp_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def delete_final_file(self):\n",
    "        try:\n",
    "            os.remove(self.final_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        \n",
    "    def delete_temp_dir(self):\n",
    "            # in our case, the parent directory is likely now empty\n",
    "            # delete it, if there are no other files in it\n",
    "            try:\n",
    "                os.rmdir(os.path.dirname(self.temp_path))\n",
    "            except OSError:\n",
    "                # another file is in the same directory\n",
    "                # (unlikely, given how we partition the directory)\n",
    "                pass   \n",
    "                \n",
    "    # like close, but delete everything\n",
    "    def abort(self):\n",
    "        \n",
    "        \n",
    "        if not self.closed:\n",
    "            self.temp_f.close()\n",
    "            self.closed = True\n",
    "            self.delete_temp_file()\n",
    "            self.delete_temp_dir()\n",
    "\n",
    "    def tell(self):\n",
    "        return self.temp_f.tell()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283d59c-dfcf-4c6f-9136-30086babb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# takes a CSV\n",
    "# fnames is a list of files\n",
    "# first is the raw file on disk\n",
    "# last is the CSV itself\n",
    "# e.g. if this is a.txt inside b.zip inside c.zip\n",
    "# fnames is ['c.zip', 'b.zip', 'a.csv']\n",
    "# \n",
    "# we write to a random filename, then move to the destination\n",
    "# so that if two processes are unzipping different files to the same destination concurrently, they won't corrupt the file.\n",
    "#\n",
    "# returns number of bytes written (uncompressed)\n",
    "def process_csv(fnames: List[str], f_in: TextIO) -> int:\n",
    "    if isinstance(fnames, str):\n",
    "        fnames = [fnames]\n",
    "    csv_name = os.path.basename(fnames[-1])\n",
    "\n",
    "    try:\n",
    "        csv_r = csv.reader(f_in)\n",
    "        first_row = next(csv_r)\n",
    "        assert isinstance(first_row[0], str), f\"First row is wrong type, probably bytes not string. Is {type(first_row[0])}\"\n",
    "        # this keeps track of the 'row'\n",
    "        # but note that some data values are text including a newline character\n",
    "        # this is the number of records in the spreadsheet, not lines of text\n",
    "        record_num = 1\n",
    "        expected_start = 'C,SETP.WORLD'\n",
    "        if first_row[0] != 'C':\n",
    "            # some other files end up in the dataset\n",
    "            # e.g. int668_v1_schedule_log_rpt_1~20231124133149.csv.gz\n",
    "            # just ignore files like that\n",
    "            logger.warning(f\"First line does not start with C, {first_row[:10]=} in {fnames}. Ignoring\")\n",
    "            return\n",
    "\n",
    "        # the first row contains a timestamp for when the file was created\n",
    "        # e.g. 2020/06/04,00:03:11 (as two CSV cells)\n",
    "        # change that to 2020_06_04_00_03_11\n",
    "        # (only characters that can go in a folder name)\n",
    "        t1 = first_row[5]\n",
    "        if len(first_row) >= 6+1:\n",
    "            t2 = first_row[6]\n",
    "            top_timestamp_s = t1 + '_' + t2\n",
    "        else:\n",
    "            top_timestamp_s = t1\n",
    "        top_timestamp_s = top_timestamp_s.replace('/', '_').replace(' ', '_').replace(':', '_').replace('\\\\', '_')\n",
    "        assert '/' not in top_timestamp_s, f\"Bad top timestabl: {first_row=}\"\n",
    "        \n",
    "        row = next(csv_r)\n",
    "        record_num = record_num + 1\n",
    "    \n",
    "        # these will be set later\n",
    "        csv_w = None\n",
    "        chars_to_skip = None\n",
    "        f_out = None\n",
    "\n",
    "        while True:\n",
    "            if row[0] in ['C', 'I']:\n",
    "                # close off the last output file\n",
    "                csv_w = None \n",
    "                if f_out is not None:\n",
    "                    f_out.close()\n",
    "\n",
    "                    if num_d_rows == 0:\n",
    "                        logger.warning(f\"No data rows, only header row written for {fnames}, so deleting output file\")\n",
    "                        f_out.delete_final_file()\n",
    "                    \n",
    "                \n",
    "            if row[0] == 'C':\n",
    "\n",
    "                # 2nd C line, which should be the last line of the file\n",
    "                if row[1] == 'END OF REPORT':\n",
    "                                                    \n",
    "                    assert f_in.read().strip() == '', \"Remainder after end of file\"\n",
    "\n",
    "                    checksum = int(row[-1])\n",
    "                    if csv_name != 'PUBLIC_PDR_CONFIG_non_mms_data_model.CSV':\n",
    "                        # sometimes AEMO do the checksum wrong, and there's a -1 there\n",
    "                        assert checksum in (record_num, record_num - 1), f\"Checksum on last line doesn't match in {fnames}, {checksum=} {record_num=}\"\n",
    "        \n",
    "                    break\n",
    "                elif row[1] == 'SETTLEMENTS RESIDUE CONTRACT REPORT':\n",
    "                    logger.info(f\"Ignoring C line {csv_r.line_num} in {fnames}: {row[1]}\")\n",
    "                else:\n",
    "                    # additional C lines can be used for arbitrary comments\n",
    "                    logger.warning(f\"Unexpected C line {csv_r.line_num} in {fnames}: {row}\")\n",
    "            elif row[0] == 'I':\n",
    "                # start of new file\n",
    "                # The 'version' value is an int even on I rows\n",
    "                # let's write a column header instead for this I row\n",
    "                # then include the version value in subsequent D rows\n",
    "                # choosing a column name that won't clash with others\n",
    "                version_col_name = 'SCHEMA_VERSION'\n",
    "                top_timestamp_col_name = 'TOP_TIMESTAMP'\n",
    "                row_type, report_name, report_subtype, version, *remainder = row\n",
    "                cols_to_skip = 4\n",
    "    \n",
    "                if report_name in ['', None]:\n",
    "                    report_name = REPORT_NAME_NULL_PLACEHOLDER\n",
    "                if report_subtype in ['', None]:\n",
    "                    report_subtype = REPORT_SUBTYPE_NULL_PLACEHOLDER\n",
    "\n",
    "                # which SQL 'table' does this map to?\n",
    "                # e.g. 'DISPATCH', 'PRICE' maps to 'DISPATCHPRICE'\n",
    "                table = map_table(report_name, report_subtype)\n",
    "\n",
    "                if (table is None) or (table not in tables_to_skip):\n",
    "                    if (table is None) and ((report_name, report_subtype) not in packages_to_ignore):\n",
    "                        logger.warning(f\"Skipping/ignoring {report_name}, {report_subtype} -> {table} in {fnames} with columns {remainder}\")\n",
    "                    dest_folder = None\n",
    "                    final_path = None\n",
    "                    dst = None\n",
    "                    skip = True\n",
    "                    assert (f_out is None) or f_out.closed\n",
    "                    f_out = None\n",
    "                else:\n",
    "\n",
    "    \n",
    "                    # for the file, include two columns in the folder, not the file itself\n",
    "                    # i.e. hive-style partitioning\n",
    "                    # (this saves space compared to repeating it on each line.)\n",
    "                    # we use these values for deduplicating rows across files.\n",
    "                    subdir = os.path.join(report_name, report_subtype, f\"{version_col_name}={version}\", f\"{top_timestamp_col_name}={top_timestamp_s}\")\n",
    "\n",
    "                    final_path = os.path.join(dest_dir, subdir, csv_name + '.gz')\n",
    "                    temp_path = os.path.join(temp_dir, subdir, csv_name + '.' + str(uuid4()) + '.tmp.gz')\n",
    "                    \n",
    "                    f_out = AtomicFile(temp_path, final_path)\n",
    "                    #f_out = open(temp_path, 'w')\n",
    "                    csv_w = csv.writer(f_out)\n",
    "                    csv_w.writerow(remainder)\n",
    "                    skip = False\n",
    "                \n",
    "                num_d_rows = 0\n",
    "    \n",
    "            else:\n",
    "                assert row[0] == 'D', f\"Unexpected non-D row: {row[:100]}\" \n",
    "                if not skip:\n",
    "                    csv_w.writerow(row[cols_to_skip:])\n",
    "                num_d_rows += 1\n",
    "    \n",
    "            row = next(csv_r)\n",
    "            record_num = record_num + 1\n",
    "        return bytes_written\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e: \n",
    "        # close the output file if open\n",
    "        try:\n",
    "            if f_out is not None:\n",
    "                f_out.abort()\n",
    "        except NameError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(temp_path)\n",
    "        except (FileNotFoundError, NameError, OSError):\n",
    "            pass\n",
    "        if any(re.search(ptn, csv_name) for ptn in csvgz_files_to_ignore):\n",
    "            logger.info(f\"Ignoring error splitting {csv_name}\")\n",
    "            return bytes_written\n",
    "        else:\n",
    "            # save the file itself (since it's inside a zip, it's annoying to find for debugging)\n",
    "            f_in.seek(0) # go back to the start of the file\n",
    "            debug_path = os.path.join(bad_csv_dir, csv_name) + '.gz'\n",
    "            if not os.path.exists(os.path.dirname(debug_path)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(debug_path))\n",
    "                except FileExistsError:\n",
    "                    # race condition when multiprocessing\n",
    "                    pass\n",
    "            with gzip.open(debug_path, 'wt') as f_out_copy:\n",
    "                s = f_in.read(1)\n",
    "                assert isinstance(s, str), f\"s is {type(s)}\"\n",
    "                f_in.seek(0)\n",
    "                shutil.copyfileobj(f_in, f_out_copy)\n",
    "                \n",
    "            with open(debug_path + '.error.txt', 'w') as f_err:\n",
    "                f_err.write(f\"Issue is in file heirachy: {fnames}\\n\")\n",
    "                try:\n",
    "                    f_err.write(f\"On text line {csv_r.line_num}\")\n",
    "                except NameError:\n",
    "                    # csv_r is not defined\n",
    "                    pass\n",
    "                try:\n",
    "                    f_err.write(f\"CSV record {record_num}\")\n",
    "                except NameError:\n",
    "                    pass\n",
    "                f_err.write(str(e) + '\\n')\n",
    "                traceback.print_exc(file=f_err)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de698f23-9397-4b64-81d8-7455763516df",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8140b70-a92b-4bea-beb5-85638a05cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_excel(metadata_path, sheet_name=sheet)\n",
    "metadata_df[report_name_column].fillna(REPORT_NAME_NULL_PLACEHOLDER, inplace=True)\n",
    "metadata_df[report_subtype_column].fillna(REPORT_SUBTYPE_NULL_PLACEHOLDER, inplace=True)\n",
    "\n",
    "# there are some packages I do not know how to map to tables\n",
    "packages_to_ignore = [\n",
    "    # maybe there's a table for these,\n",
    "    # or they're legacy tables\n",
    "    # these are ones we don't care about\n",
    "    ('DINT', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('TINT', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('DCONS', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('DREGION', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('SPDCPC', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('SRAFINANCIALS', 'RECONCILIATION_SUMMARY'),\n",
    "    ('DAILY', 'MLF'), # electrical transmission loss factors.\n",
    "    ('BILLING_CONFIG', 'BILLSMELTERRATE'), # alumninium smelter info. Possibly relevant? (Smeltering makes up 30% of NSW load)\n",
    "    ('BILLING', 'CSP_SUPPORTDATA_SRA'),\n",
    "    ('BILLING', 'ASPAYMENT_SUMMARY'), # probably belongs to BILLINGASPAYMENTS table, but this is not relevant to us so I haven't bothered checking\n",
    "    ('BILLING', 'DIRECTION_CRA'),\n",
    "    ('TRADING', 'CUMULATIVE_PRICE'),\n",
    "    ('TREGION', 'NULL'),\n",
    "    ('DAILY', 'WDR_NO_SCADA'),\n",
    "    ('METER_DATA', 'GEN_DUID'),\n",
    "    ('SEVENDAYOUTLOOK', 'PEAK'),\n",
    "    ('TUNIT', 'NULL'),\n",
    "    ('GPG', 'MARKET_SUMMARY'),\n",
    "    ('GPG', 'CASESOLUTION'),\n",
    "    ('GPG', 'CONSTRAINTSOLUTION'),\n",
    "    ('GPG', 'PRICESOLUTION'),\n",
    "    ('GPG', 'INTERCONNECTORSOLUTION'),\n",
    "    ('CAUSER_PAYS_SCADA', 'NETWORK'),\n",
    "    ('PDR_REPORT', 'COLUMN'),\n",
    "    ('DUNIT', 'NULL'),\n",
    "    ('YESTBID', 'BIDDAYOFFER'),\n",
    "    ('YESTBID', 'BIDPEROFFER'),\n",
    "    ('RESIDUE_PRICE_OFFER', 'NULL'),\n",
    "    ('RESIDUE_PRICE_BID', 'NULL'), # columns don't match the RESIDUE_PRICE_BID table, almost match RESIDUE_PRICE_FUNDS_BID\n",
    "    ('IBEI', 'PUBLISHING'),\n",
    "    ('EMSLIMITS', 'LIM_ALTLIM'),\n",
    "    ('DEMAND', 'HISTORIC'),\n",
    "\n",
    "    \n",
    "    ('PDR_REPORT', 'TABLE'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'FILE'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'MAPPING'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'COLUMN'),    # manifest metadata\n",
    "\n",
    "    # Gas data\n",
    "    ('GSH', 'PARTICIPANT_OPSTATE'),\n",
    "    ('GSH', 'PARTICIPANTS'),\n",
    "    ('GSH', 'AUCTION_CURTAILMENT_NOTICE'),\n",
    "    ('GSH', 'AUCTION_PRICE_VOLUME'),\n",
    "    ('GSH', 'BENCHMARK_PRICE'),\n",
    "    ('GSH', 'PARK_SERVICES'),\n",
    "    ('GSH', 'AUCTION_QUANTITIES'),\n",
    "    ('GSH', 'FACILITIES'),\n",
    "    ('GSH', 'TRANSACTION_SUMMARY'),\n",
    "    ('GSH', 'HISTORICAL_SUMMARY'),\n",
    "    ('GSH', 'NOTIONAL_POINTS'),\n",
    "    ('GSH', 'REVISED_AUCTION_QUANTITIES'),\n",
    "    ('GSH', 'PIPELINE_SEGMENTS'),\n",
    "    ('GSH', 'CAPACITY_TRANSACTION'),\n",
    "    ('GSH', 'ZONES'),\n",
    "    ('GSH', 'SERVICE_POINTS'),\n",
    "\n",
    "    # we should maybe double check\n",
    "    ('FORCE_MAJEURE', 'MARKETSUSREGION'),\n",
    "    ('FORCE_MAJEURE', 'MARKETSUSPENSION')\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# there are some packages which aren't in the spreadsheet, but we can guess them\n",
    "# in particular there's a CO2 one I can't \n",
    "package_exceptions = {\n",
    "    ('DISPATCH', 'CASESOLUTION'): 'DISPATCHCASESOLUTION',\n",
    "    ('GENCONSETTRK', REPORT_SUBTYPE_NULL_PLACEHOLDER): 'GENCONSETTRK',\n",
    "    ('DISPATCH', 'REGIONFCASREQUIREMENT'): 'DISPATCH_FCAS_REQ',\n",
    "\n",
    "    # this one is possibly useful\n",
    "    # example file is CO2EII_AVAILABLE_GENERATORS.CSV.gz\n",
    "    # It looks like it might go into BILLING_CO2E_PUBLICATION, but the columns are different\n",
    "    # Example:\n",
    "    #STATIONNAME,DUID,GENSETID,REGIONID,CO2E_EMISSIONS_FACTOR,CO2E_ENERGY_SOURCE,CO2E_DATA_SOURCE\n",
    "    #\"Appin Power Plant\",APPIN,APPIN,NSW1,0.56318004,\"Coal seam methane\",NGA2022\n",
    "    # so define a new table for it\n",
    "    ('CO2EII', 'PUBLISHING'): 'CO2EII_AVAILABLE_GENERATORS',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c07ba7-2aaf-485d-8af7-3423748b8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some tables we skip just because they are huge\n",
    "# most of these are each larger than the 200 smallest tables\n",
    "tables_to_skip = [\n",
    "    'P5MIN_UNITSOLUTION', \n",
    "    'P5MIN_CASESOLUTION',\n",
    "    'P5MIN_CONSTRAINTSOLUTION', \n",
    "    'P5MIN_LOCALPRICE', # not what people get paid. Related to electrical constraints\n",
    "    'DISPATCH_LOCALPRICE', # not what people get paid. Related to electrical constraints\n",
    "    'NETWORK_OUTAGEDETAIL',\n",
    "    'MCC_CASESOLUTION',\n",
    "    'MCC_CONSTRAINTSOLUTION',\n",
    "    'DISPATCHOFFERTRK', \n",
    "    'DISPATCHCASESOLUTION',\n",
    "    'P5MIN_REGIONSOLUTION', \n",
    "    'DISPATCHCONSTRAINT',\n",
    "    'SETFCASREGIONRECOVERY',\n",
    "    'TRADINGPRICE', # surprisingly big\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a0264-ba0d-4140-8bf5-3a0923022b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# returns None if we can't find a table\n",
    "def map_table(report_name, report_subtype) -> Optional[str]:\n",
    "    candidates = metadata_df.loc[(metadata_df[report_name_column] == report_name) & (metadata_df[report_subtype_column] == report_subtype), table_name_column]\n",
    "    # sometimes there's duplicates\n",
    "    # but assert they're all the same answer\n",
    "    tables = set(candidates)\n",
    "    if len(tables) == 0 and (report_name, report_subtype) in package_exceptions:\n",
    "        return package_exceptions[(report_name, report_subtype)]\n",
    "        \n",
    "    if len(tables) != 1:\n",
    "        return None\n",
    "    table = tables.pop() # choose the only one\n",
    "    return table\n",
    "\n",
    "# unit testing\n",
    "assert map_table('DISPATCH', 'PRICE') == 'DISPATCHPRICE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae1c26-4285-4051-93c8-d512f3194ba2",
   "metadata": {},
   "source": [
    "## Run the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a683e-6c47-4e1a-80a7-4e5ed1cce0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# __name__ thing to fix a multiprocessing issue\n",
    "if __name__ == '__main__':\n",
    "    logger.reset()\n",
    "    \n",
    "    files = [os.path.join(source_dir, p) for p in os.listdir(source_dir)]\n",
    "\n",
    "    # shuffle file order\n",
    "    # so that we don't do all the small files first\n",
    "    # then all the large files last \n",
    "    # (or the other way around)\n",
    "    # this makes the progress bar more accurate\n",
    "    shuffle(files) # mutates list in place\n",
    "\n",
    "    print(\"Starting\")\n",
    "    \n",
    "    if use_multiprocessing and False:\n",
    "        with Pool(num_processes) as p:\n",
    "            statuses = list(tqdm(p.imap(_process_raw, files), total=len(files)))\n",
    "    else:\n",
    "        #ret = [process_raw(file) for file in tqdm(files)]\n",
    "        statuses = [_process_raw(file) for file in tqdm(files)]\n",
    "    assert all(s is None for s in statuses), \"some files failed to be processed, to debug, run the next cell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ade12-85e2-49c4-b1db-1a0501b76272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the files which didn't download properly\n",
    "# e.g. do they matter for our case?\n",
    "# can you find it elsewhere?\n",
    "# e.g. PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20231226053000_0000000406806404.zip was corrupted\n",
    "# but can be found inside PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20231221.zip\n",
    "[(str(s)[:100], f) for (s,f) in zip(statuses, files) if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4e30c-cbed-4877-baf4-240ced38ed4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
