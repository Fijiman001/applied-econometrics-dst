{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2621476e-93e0-4e60-bdfa-d93765298e5f",
   "metadata": {},
   "source": [
    "# Unzip and Split\n",
    "\n",
    "## Unzip\n",
    "\n",
    "In a previous script we downloaded a lot of `.zip` files (and some `.csv`). Now we want to unzip them.\n",
    "Note that some files are zips of zips of CSVs. Some zips have hundreds of files inside them (which I call \"children\" or \"members\").\n",
    "\n",
    "The unzipped files are too large to fit onto my hard drive. (approx 2.4TB). So we compress each file with gzip (`.csv.gz`). Note that most tools (e.g. R, pandas, Excel) can decompress that on the fly.\n",
    "It may seem pointless to decompress and immediately recompress. The aim is to split up the zips which contain many .csv files, and un-nest the recursive zipping. We also discard a lot of data (described more below.) and split each CSV file.\n",
    "\n",
    "## Convert (Split) Files\n",
    "\n",
    "After unzipping files, we're left with what look like a bunch of CSVs. But if you open them up you'll see they're not a normal CSV. They're a concatenation of many CSV files into one. (e.g. note how the number of columns changes as you scroll down.) So we want to unconcatenate them, or \"split\" them. So each \".CSV\" file becomes many \".CSV\" files.\n",
    "\n",
    "To understand this format, read `Guide to CSV Data Format Standard.pdf`, which is saved in `./documentation`. Available [here](https://web.archive.org/web/20230414160249/https://aemo.com.au/-/media/files/market-it-systems/guide-to-csv-data-format-standard.pdf?la=en).\n",
    "\n",
    "To read these files, look at the first column, which is C, D, or I.\n",
    "\n",
    "* `C` means this is metadata. Typically the first and last row of the file. The first row tells you things like when the file was generated. We can probably ignore this. (Other timestamps are inside the data itself.)\n",
    "* `I` means this is the header row for a new table\n",
    "* `D` means this is a data row for the same table as the previous row\n",
    "\n",
    "In the `D` and `I` rows, the second and third column contain info about which table this data belongs to (`REPORT_NAME` and `REPORT_SUBTYPE`). This is described more below. For now we put output files under a 2-layer folder structure based on this. \n",
    "\n",
    "The fourth column is an integer related to some kind of schema versioning. We'll keep that, so that later if we find duplicate/clashing rows, choose the higher version number when duplicating.\n",
    "\n",
    "As an example, consider this fictitious file:\n",
    "\n",
    "```\n",
    "C,SETP.WORLD,TER_DAILY,AEMO,PUBLIC,2024/01/07,08:00:01,0000000407795252,,0000000407795252\n",
    "I,A,B,123,\"col1\",\"col2\"\n",
    "D,A,B,123,a1,b1\n",
    "D,A,B,123,a2,b2\n",
    "I,C,E,321,col_alpha,col_beta,col_sigma\n",
    "D,C,E,321,x1,x1,x1\n",
    "D,C,E,321,y2,y2,y2\n",
    "D,C,E,321,z3,z3,z3\n",
    "C,\"END OF REPORT\",9\n",
    "```\n",
    "\n",
    "Which contains two dataframes:\n",
    "\n",
    "```\n",
    "\"col1\",\"col2\"\n",
    "a1,b1\n",
    "a2,b2\n",
    "```\n",
    "```\n",
    "col_alpha,col_beta,col_sigma\n",
    "x1,x1,x1\n",
    "y2,y2,y2\n",
    "z3,z3,z3\n",
    "```\n",
    "\n",
    "So for each input file, we want multiple output files. I call this process 'splitting'.\n",
    "\n",
    "Note that pandas cannot read these original files, because the datatype changes across rows within the same file, and the number of rows changes. But the `csv` library can handle this.\n",
    "\n",
    "## Map each file chunk to a table\n",
    "\n",
    "In the sample file above, we split one file into two. We discarded the first few columns, with values `C,E` and `A,B`. These columns tell us what dataset each chunk belongs to. I call them \"tables\" because AEMO expects this data to end up in a SQL database.\n",
    "\n",
    "These columns are called `REPORT_TYPE` and `REPORT_SUBTYPE`. We need to map these to a table name.\n",
    "Sometimes it's obvious. E.g. `DISPATCH` and `PRICE` belongs to the `DISPATCHPRICE` table. But sometimes it's not obvious. (And sometimes there are blanks.) For each table, there are multiple pairs of `REPORT_TYPE` and `REPORT_SUBTYPE` for that table. This is not properly documented. The best way I've found is to look up those two values in a particular file. That file is no longer on the internet. But we can find an older version [here](https://web.archive.org/web/20220812083311/https://visualisations.aemo.com.au/aemo/di-help/Content/Data_Model/MMS_Data_Model.htm). (That's for schema version 5.1. We're up to 5.2 now. But the differences are small enough that it shouldn't be an issue. We care about so few tables that we could probably hard-code this mapping.)\n",
    "\n",
    "There are a lot of exceptions which I have hard coded.\n",
    "\n",
    "Note that for each output file, we save it with hive style partitioning.\n",
    "In the example file above, see the values `123` and `321`. Those are versions of the schema for each table. They almost never matter. But for one table (I forget which) they do. So we want to keep that, and use it when deduplicating later. (For clashing rows, choose the more recent schema version.)\n",
    "There's also a timestamp in the first row, which I call the \"top timestamp\". Again we want to keep this to help us decide which row to keep when deduplicating later.\n",
    "Since both of these values are the same for every row in the output file, it would be wasteful to write them into the file itself. So we put them in the folder name. This is a standard practice called \"Hive partitioning\". e.g. if the second chunk above, with values `C,E` maps to table `sometable`, and the original file was `somefile.csv`, then the output file will be `something/sometable/SCHEMA_VERSION=321/TOP_TIMESTAMP=2024_01_07_08_00_01/somefile.csv.gz`. Many tools (e.g. `read_dataset` in R) will read these values automatically when you point them at folder `something/sometable/`.\n",
    "\n",
    "## Combining together\n",
    "\n",
    "For performance reasons, we do all 3 steps at once, including recompressing the final output to `.csv.gz`.\n",
    "This makes it a bit confusing to debug and reason with. But it speeds up processing by not writing out the intermediate `.csv` files to disk. (And saves hundreds of GB of disk space.)\n",
    "\n",
    "To help debug this, files we can't process (e.g. corrupt zips, or CSVs that don't match the expected format) are copied to an `error` folder. (Because the file might not exist directly in the source directory, if it's unzipped from an unzipped zip.)\n",
    "\n",
    "When handling errors, we try not to let an error in file 20000 stop us from attempting to process files 20001, 20002 etc. So we wrap each attempt in a `try`/`except` block. We also delete any half-written files if there's an error. \n",
    "\n",
    "Watch out, there are two particular zip files which are corrupt. (They weren't downloaded wrong, they were just published as corrupt files.) I've checked those two and they're not relevant for us. So when you run this script, you will get an error that some files haven't been processed. If it's only a handful, check each on a case-by-case basis. (The errors are saved in text files next to the troublesome files, in the error directory.) And then decide to ignore them (or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306d6fa6-f3ce-4d03-9566-74789fa7470a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/matthew/Documents/TSE/AppliedEconometrics/repo/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "from typing import List, Dict, Tuple, Optional, TextIO, BinaryIO\n",
    "import csv\n",
    "import traceback\n",
    "import json\n",
    "from random import shuffle\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import io\n",
    "import gzip\n",
    "import importlib\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0446d7-a46d-4628-834b-04dd20c53e0f",
   "metadata": {},
   "source": [
    "## Path constants\n",
    "\n",
    "You may need to change these, if you're trying to rerun the code on your own machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f0d85b-4fb3-4db9-b060-bac1b9c7e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the large files will go here\n",
    "# you need a few hundred GB of space\n",
    "# this should contain the results of the previous scripts\n",
    "base_data_dir = '/home/matthew/data/'\n",
    "tux_dir = '/media/matthew/Tux/AppliedEconometrics/data'\n",
    "\n",
    "# ./data relative to this playbook\n",
    "# (But for some reason ./data didn't work as expected, so I hard coded it for my laptop.)\n",
    "repo_data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data/'\n",
    "\n",
    "# the results of the unzipping script were saved here\n",
    "source_dir = os.path.join(tux_dir, '01-A-raw')\n",
    "\n",
    "# For each file in source_dir, after processing it will be moved here\n",
    "# (so if we stop and restart processing, we don't reprocess files)\n",
    "# only if move_when_done=True\n",
    "archive_dir = os.path.join(tux_dir, '01-C-raw-done')\n",
    "temp_dir = os.path.join(base_data_dir, '01-C-split-mapped-csv-temp/')\n",
    "dest_dir = os.path.join(base_data_dir, '01-C-split-mapped-csv/')\n",
    "\n",
    "debug_path = os.path.join(repo_data_dir, 'debug/')\n",
    "bad_zip_dir = os.path.join(base_data_dir, 'error/bad_zip/')\n",
    "bad_csv_dir = os.path.join(base_data_dir, 'error/bad_csv/')\n",
    "log_file = os.path.join(repo_data_dir, 'logs.txt')\n",
    "\n",
    "metadata_path = os.path.join(repo_data_dir, 'MMS_Data_Model_Table_to_File_to_Report_Relationships_51.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ffd60b-45e5-4a48-9ec1-1002fd6362f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I somehow lost the result for this one\n",
    "# re-run only for this\n",
    "# if you want to process all tables (other than ones excluded below) leave this empty\n",
    "only_tables = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094b868-570c-41f0-a912-7dffa5b0e1bc",
   "metadata": {},
   "source": [
    "## Other constants\n",
    "\n",
    "You'll rarely need to change these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a031477-1489-48ab-bdea-43998f7a757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when finished processing a zip\n",
    "# move it to archive_dir\n",
    "# so if we stop and restart the playbook, we don't redo that zip\n",
    "# set to True when you think the code is good.\n",
    "# set to False when still writing the code and testing,\n",
    "# or if you've changed any of the code.\n",
    "move_when_done = True\n",
    "\n",
    "\n",
    "# process files with multiprocessing, to be faster.\n",
    "# If set to False, will use a for loop, which gives clearer traceback error messages.\n",
    "use_multiprocessing = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7311af-8936-4ef6-921c-bd278518dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how hardcore the gzip algorithm is for .csv.gz files. Low to prioritise speed over size\n",
    "# but if you have an external hard drive, too low is probably slow and large\n",
    "# (it's about disk write speed vs CPU compression speed) \n",
    "compresslevel = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6460463-169f-465f-9b90-b4a6b448fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_col_name = 'SCHEMA_VERSION'\n",
    "top_timestamp_col_name = 'TOP_TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e985a2-a4ee-41a6-9ae3-862e2f9f7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are for what's inside metadata_path\n",
    "sheet = 'MMSDM_v5-1'\n",
    "report_subtype_column = 'PDR_REPORT_SUB_TYPE'\n",
    "report_name_column = 'PDR_REPORT_NAME'\n",
    "table_name_column = 'MMSDM_TABLE_NAME'\n",
    "\n",
    "REPORT_NAME_NULL_PLACEHOLDER = 'NULL'\n",
    "REPORT_SUBTYPE_NULL_PLACEHOLDER = 'NULL'\n",
    "\n",
    "# AEMO data is always in UTC+10\n",
    "TIMEZONE_OFFSET=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4cd050-9651-4dd9-8c72-84a0d2bd8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've found some particular files have funny encodings\n",
    "# just ignore them. We don't need them for our analysis.\n",
    "csvgz_files_to_ignore = [\n",
    "    # regex patterns\n",
    "    r\"7_days_High_Impact_Outages_\\d+.csv.gz\",\n",
    "    r\"High_Impact_Outages_\\d+\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f12965-423f-4fbb-a3e2-fbe84790b55b",
   "metadata": {},
   "source": [
    "# Stuff that should go in a shared library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6d3e76f-7c45-4c39-bdd7-c9314b0d76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease the priority of this script given by the OS\n",
    "# so you can keep using your laptop for other stuff\n",
    "utils.renice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb5a0488-5c21-48c2-80fc-425789ecdbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(log_file)\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86b485-5602-45f6-b084-d668ea382d0e",
   "metadata": {},
   "source": [
    "## Unzip\n",
    "\n",
    "For now we just define functions, but don't actually execute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eafc9ef9-1324-45de-9bc0-3f769d1069cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [temp_dir, dest_dir, archive_dir]:\n",
    "    utils.create_dir(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f33c3f-05dd-47b4-8030-f56b8370a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call process()\n",
    "# catch (almost) all errors, and return them\n",
    "# then after we process all files, check these errors\n",
    "def _process_raw(path) -> Tuple[Optional[int], Optional[Exception]]:\n",
    "    try:\n",
    "        process_raw(path)\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as ex: # catch anything except keyboard interrupt\n",
    "        logger.error(f\"Failed to unzip {path}: {ex}\")\n",
    "        return ex\n",
    "    else:\n",
    "        if move_when_done:\n",
    "            # move the file to another folder\n",
    "            # so if we stop and restart, we will not \n",
    "            archive_path = os.path.join(archive_dir, os.path.basename(path))\n",
    "            os.rename(path, archive_path)\n",
    "        return None\n",
    "\n",
    "# takes a string of a path to a file on disk\n",
    "# either .csv or .zip\n",
    "def process_raw(path: str):\n",
    "    if path.lower().endswith('.csv'):\n",
    "        with open(path, 'r', newline='') as f:\n",
    "            process_csv([path], f)\n",
    "    elif path.lower().endswith('.zip'):\n",
    "        with open(path, 'rb') as f:\n",
    "            process_zip([path], f)\n",
    "    else:\n",
    "        logger.warning(f\"Ignoring file {path}\")\n",
    "    #logger.debug(f\"Finished processing {path}\")\n",
    "\n",
    "# takes in a list of filenames\n",
    "# e.g. if there's a/myzip.zip on disk, which contains otherzip.zip\n",
    "# for the inner zip paths is ['a/myzip.zip', 'otherzip.zip']\n",
    "# These names are just for debugging and logging \n",
    "# the file-like object f is used for reading content\n",
    "def process_zip(fnames: List[str], f: BinaryIO):\n",
    "    if isinstance(fnames, str):\n",
    "        fnames = [fnames]\n",
    "        \n",
    "    try:\n",
    "        with zipfile.ZipFile(f) as z:\n",
    "            exceptions = []\n",
    "            for m in z.namelist():\n",
    "                try:\n",
    "                    if m.lower().endswith('.zip'):\n",
    "                        with z.open(m, mode='r') as mf:\n",
    "                            process_zip(fnames + [m], mf)\n",
    "                    elif m.lower().endswith('.csv'):\n",
    "                            with z.open(m, mode='r') as mfb:\n",
    "                                with io.TextIOWrapper(mfb, newline='') as mft:\n",
    "                                    process_csv(fnames + [m], mft)\n",
    "                    else:\n",
    "                        paths_debug = ' -> '.join(fnames)\n",
    "                        logger.warning(f\"Found unknown file {m} in zip {paths_debug}\")\n",
    "                \n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except Exception as ex:\n",
    "                    # ignore this one for now,\n",
    "                    # continue processing the other files in this zip\n",
    "                    # then rethrow this later\n",
    "                    exceptions.append(ex)\n",
    "    except zipfile.BadZipFile as e:\n",
    "        f.seek(0) # go back to the start of the file\n",
    "        debug_path = os.path.join(bad_zip_dir, os.path.basename(fnames[-1]))\n",
    "        if not os.path.exists(os.path.dirname(debug_path)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(debug_path))\n",
    "            except FileExistsError:\n",
    "                # race condition when multiprocessing\n",
    "                pass\n",
    "        with open(debug_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f, f_out)\n",
    "            \n",
    "        # save the error to a file\n",
    "        with open(debug_path + '.error.txt', 'w') as f_err:\n",
    "            f_err.write(f\"Issue is in file heirachy: {fnames}\\n\")\n",
    "            f_err.write(str(e) + \"\\n\")\n",
    "            traceback.print_exc(file=f_err)\n",
    "        raise\n",
    "    if exceptions:\n",
    "        # rethrow an exception from processing one file in this zip\n",
    "        raise exceptions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71ed39-bdac-4271-b932-f95bdc9c1e76",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "### Atomic writes\n",
    "\n",
    "It's possible to have the same final file be present in multiple source files. Since we're using multiprocessing, this could lead to race conditions where two processes write to the same file at once. That will jumple stuff up and corrupt the file. So we want some kind of atomicity.\n",
    "\n",
    "To achieve this, we write to a random file path on the same disk. Then once it's done, we move that file across. We assume that moving a file to another folder on the same physical disk is atomic. \n",
    "\n",
    "I wanted to subclass `ioTextIOBase`, but I can't figure out how. Some attributes can't be overwritten. It's quite confusing. So I just wrote a vanilla class with the standard methods.\n",
    "\n",
    "This class can handle compressed `.csv.gz` or uncompressed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3ee917-4623-4e58-b803-1568907de797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicFile: #(io.TextIOBase):\n",
    "    # newline='' is for csv.writer\n",
    "    def __init__(self, temp_path, final_path, newline=''):\n",
    "        self.temp_path = temp_path\n",
    "        self.final_path = final_path\n",
    "        \n",
    "        # create destination folder if it doesn't exist\n",
    "        if not os.path.exists(os.path.dirname(temp_path)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(temp_path))\n",
    "            except FileExistsError:\n",
    "                # race condition when multiprocessing\n",
    "                pass\n",
    "        #logger.info(f\"Openning {temp_path} for writing\")\n",
    "        if temp_path.lower().endswith('.gz'):\n",
    "            self.temp_f = gzip.open(temp_path, mode='wt', compresslevel=compresslevel, newline=newline)\n",
    "        else:\n",
    "            self.temp_f = open(temp_path, mode='w', newline=newline)\n",
    "        self.closed = False\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        assert not self.closed\n",
    "        return self.temp_f.read(size)\n",
    "\n",
    "    def write(self, s):\n",
    "        assert not self.closed\n",
    "        return self.temp_f.write(s)\n",
    "\n",
    "    def close(self):\n",
    "        \n",
    "        if not self.closed:\n",
    "            #logger.info(f\"Closing {self.temp_path} after writing\")\n",
    "            self.temp_f.close()\n",
    "            self.closed = True\n",
    "\n",
    "            if not os.path.exists(os.path.dirname(self.final_path)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(self.final_path))\n",
    "                except FileExistsError:\n",
    "                    # race condition when multiprocessing\n",
    "                    pass\n",
    "            \n",
    "            os.rename(self.temp_path, self.final_path)\n",
    "            #logger.info(f\"Moving {self.temp_path} to {self.final_path}\")\n",
    "            self.delete_temp_dir()\n",
    "\n",
    "    def delete_temp_file(self):\n",
    "        try:\n",
    "            os.remove(self.temp_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def delete_final_file(self):\n",
    "        try:\n",
    "            os.remove(self.final_path)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        \n",
    "    def delete_temp_dir(self):\n",
    "            # in our case, the parent directory is likely now empty\n",
    "            # delete it, if there are no other files in it\n",
    "            try:\n",
    "                os.rmdir(os.path.dirname(self.temp_path))\n",
    "            except OSError:\n",
    "                # another file is in the same directory\n",
    "                # (unlikely, given how we partition the directory)\n",
    "                pass   \n",
    "                \n",
    "    # like close, but delete everything\n",
    "    def abort(self):\n",
    "        \n",
    "        \n",
    "        if not self.closed:\n",
    "            self.temp_f.close()\n",
    "            self.closed = True\n",
    "            self.delete_temp_file()\n",
    "            self.delete_temp_dir()\n",
    "\n",
    "    def tell(self):\n",
    "        return self.temp_f.tell()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b283d59c-dfcf-4c6f-9136-30086babb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# takes a CSV\n",
    "# fnames is a list of files\n",
    "# first is the raw file on disk\n",
    "# last is the CSV itself\n",
    "# e.g. if this is a.txt inside b.zip inside c.zip\n",
    "# fnames is ['c.zip', 'b.zip', 'a.csv']\n",
    "# \n",
    "# we write to a random filename, then move to the destination\n",
    "# so that if two processes are unzipping different files to the same destination concurrently, they won't corrupt the file.\n",
    "#\n",
    "# returns number of bytes written (uncompressed)\n",
    "def process_csv(fnames: List[str], f_in: TextIO) -> int:\n",
    "    if isinstance(fnames, str):\n",
    "        fnames = [fnames]\n",
    "    csv_name = os.path.basename(fnames[-1])\n",
    "\n",
    "    try:\n",
    "        csv_r = csv.reader(f_in)\n",
    "        first_row = next(csv_r)\n",
    "        assert isinstance(first_row[0], str), f\"First row is wrong type, probably bytes not string. Is {type(first_row[0])}\"\n",
    "        # this keeps track of the 'row'\n",
    "        # but note that some data values are text including a newline character\n",
    "        # this is the number of records in the spreadsheet, not lines of text\n",
    "        record_num = 1\n",
    "        expected_start = 'C,SETP.WORLD'\n",
    "        if first_row[0] != 'C':\n",
    "            # some other files end up in the dataset\n",
    "            # e.g. int668_v1_schedule_log_rpt_1~20231124133149.csv.gz\n",
    "            # just ignore files like that\n",
    "            logger.warning(f\"First line does not start with C, {first_row[:10]=} in {fnames}. Ignoring\")\n",
    "            return\n",
    "\n",
    "        # the first row contains a timestamp for when the file was created\n",
    "        # e.g. 2020/06/04,00:03:11 (as two CSV cells)\n",
    "        # change that to 2020_06_04_00_03_11\n",
    "        # (only characters that can go in a folder name)\n",
    "        t1 = first_row[5]\n",
    "        if len(first_row) >= 6+1:\n",
    "            t2 = first_row[6]\n",
    "            top_timestamp_s = t1 + '_' + t2\n",
    "        else:\n",
    "            top_timestamp_s = t1\n",
    "        top_timestamp_s = top_timestamp_s.replace('/', '_').replace(' ', '_').replace(':', '_').replace('\\\\', '_')\n",
    "        assert '/' not in top_timestamp_s, f\"Bad top timestabl: {first_row=}\"\n",
    "        \n",
    "        row = next(csv_r)\n",
    "        record_num = record_num + 1\n",
    "    \n",
    "        # these will be set later\n",
    "        csv_w = None\n",
    "        chars_to_skip = None\n",
    "        f_out = None\n",
    "\n",
    "        while True:\n",
    "            if row[0] in ['C', 'I']:\n",
    "                # close off the last output file\n",
    "                csv_w = None \n",
    "                if f_out is not None:\n",
    "                    f_out.close()\n",
    "\n",
    "                    if num_d_rows == 0:\n",
    "                        logger.warning(f\"No data rows, only header row written for {fnames}, so deleting output file\")\n",
    "                        f_out.delete_final_file()\n",
    "                    \n",
    "                \n",
    "            if row[0] == 'C':\n",
    "\n",
    "                # 2nd C line, which should be the last line of the file\n",
    "                if row[1] == 'END OF REPORT':\n",
    "                                                    \n",
    "                    assert f_in.read().strip() == '', \"Remainder after end of file\"\n",
    "\n",
    "                    checksum = int(row[-1])\n",
    "                    if csv_name != 'PUBLIC_PDR_CONFIG_non_mms_data_model.CSV':\n",
    "                        # sometimes AEMO do the checksum wrong, and there's a -1 there\n",
    "                        assert checksum in (record_num, record_num - 1), f\"Checksum on last line doesn't match in {fnames}, {checksum=} {record_num=}\"\n",
    "        \n",
    "                    break\n",
    "                elif row[1] == 'SETTLEMENTS RESIDUE CONTRACT REPORT':\n",
    "                    logger.info(f\"Ignoring C line {csv_r.line_num} in {fnames}: {row[1]}\")\n",
    "                else:\n",
    "                    # additional C lines can be used for arbitrary comments\n",
    "                    logger.warning(f\"Unexpected C line {csv_r.line_num} in {fnames}: {row}\")\n",
    "            elif row[0] == 'I':\n",
    "                # start of new file\n",
    "                # The 'version' value is an int even on I rows\n",
    "                # let's write a column header instead for this I row\n",
    "                # then include the version value in subsequent D rows\n",
    "                # choosing a column name that won't clash with others\n",
    "                row_type, report_name, report_subtype, version, *remainder = row\n",
    "                cols_to_skip = 4\n",
    "    \n",
    "                if report_name in ['', None]:\n",
    "                    report_name = REPORT_NAME_NULL_PLACEHOLDER\n",
    "                if report_subtype in ['', None]:\n",
    "                    report_subtype = REPORT_SUBTYPE_NULL_PLACEHOLDER\n",
    "\n",
    "                # which SQL 'table' does this map to?\n",
    "                # e.g. 'DISPATCH', 'PRICE' maps to 'DISPATCHPRICE'\n",
    "                table = map_table(report_name, report_subtype)\n",
    "\n",
    "                if (table is None) or (table in tables_to_skip) or (only_tables and table not in only_tables):\n",
    "                    if (table is None) and ((report_name, report_subtype) not in packages_to_ignore):\n",
    "                        logger.warning(f\"Skipping/ignoring {report_name}, {report_subtype} -> {table} in {fnames} with columns {remainder}\")\n",
    "                    dest_folder = None\n",
    "                    final_path = None\n",
    "                    dst = None\n",
    "                    skip = True\n",
    "                    assert (f_out is None) or f_out.closed\n",
    "                    f_out = None\n",
    "                else:\n",
    "\n",
    "    \n",
    "                    # for the file, include two columns in the folder, not the file itself\n",
    "                    # i.e. hive-style partitioning\n",
    "                    # (this saves space compared to repeating it on each line.)\n",
    "                    # we use these values for deduplicating rows across files.\n",
    "                    subdir = os.path.join(table, f\"{version_col_name}={version}\", f\"{top_timestamp_col_name}={top_timestamp_s}\")\n",
    "\n",
    "                    final_path = os.path.join(dest_dir, subdir, csv_name + '.gz')\n",
    "                    temp_path = os.path.join(temp_dir, subdir, csv_name + '.' + str(uuid4()) + '.tmp.gz')\n",
    "                    \n",
    "                    f_out = AtomicFile(temp_path, final_path)\n",
    "                    #f_out = open(temp_path, 'w')\n",
    "                    csv_w = csv.writer(f_out)\n",
    "                    csv_w.writerow(remainder)\n",
    "                    skip = False\n",
    "                \n",
    "                num_d_rows = 0\n",
    "    \n",
    "            else:\n",
    "                assert row[0] == 'D', f\"Unexpected non-D row: {row[:100]}\" \n",
    "                if not skip:\n",
    "                    csv_w.writerow(row[cols_to_skip:])\n",
    "                num_d_rows += 1\n",
    "    \n",
    "            row = next(csv_r)\n",
    "            record_num = record_num + 1\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e: \n",
    "        # close the output file if open\n",
    "        try:\n",
    "            if f_out is not None:\n",
    "                f_out.abort()\n",
    "        except NameError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(temp_path)\n",
    "        except (FileNotFoundError, NameError, OSError):\n",
    "            pass\n",
    "        if any(re.search(ptn, csv_name) for ptn in csvgz_files_to_ignore):\n",
    "            logger.info(f\"Ignoring error splitting {csv_name}\")\n",
    "            return\n",
    "        else:\n",
    "            # save the file itself (since it's inside a zip, it's annoying to find for debugging)\n",
    "            f_in.seek(0) # go back to the start of the file\n",
    "            debug_path = os.path.join(bad_csv_dir, csv_name) + '.gz'\n",
    "            if not os.path.exists(os.path.dirname(debug_path)):\n",
    "                try:\n",
    "                    os.makedirs(os.path.dirname(debug_path))\n",
    "                except FileExistsError:\n",
    "                    # race condition when multiprocessing\n",
    "                    pass\n",
    "            with gzip.open(debug_path, 'wt') as f_out_copy:\n",
    "                s = f_in.read(1)\n",
    "                assert isinstance(s, str), f\"s is {type(s)}\"\n",
    "                f_in.seek(0)\n",
    "                shutil.copyfileobj(f_in, f_out_copy)\n",
    "                \n",
    "            with open(debug_path + '.error.txt', 'w') as f_err:\n",
    "                f_err.write(f\"Issue is in file heirachy: {fnames}\\n\")\n",
    "                try:\n",
    "                    f_err.write(f\"On text line {csv_r.line_num}\")\n",
    "                except NameError:\n",
    "                    # csv_r is not defined\n",
    "                    pass\n",
    "                try:\n",
    "                    f_err.write(f\"CSV record {record_num}\")\n",
    "                except NameError:\n",
    "                    pass\n",
    "                f_err.write(str(e) + '\\n')\n",
    "                traceback.print_exc(file=f_err)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de698f23-9397-4b64-81d8-7455763516df",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f10de122-4a66-4525-88f2-ffb6ee6a41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88827/2380667581.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  metadata_df[report_name_column].fillna(REPORT_NAME_NULL_PLACEHOLDER, inplace=True)\n",
      "/tmp/ipykernel_88827/2380667581.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  metadata_df[report_subtype_column].fillna(REPORT_SUBTYPE_NULL_PLACEHOLDER, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.read_excel(metadata_path, sheet_name=sheet)\n",
    "metadata_df[report_name_column].fillna(REPORT_NAME_NULL_PLACEHOLDER, inplace=True)\n",
    "metadata_df[report_subtype_column].fillna(REPORT_SUBTYPE_NULL_PLACEHOLDER, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8140b70-a92b-4bea-beb5-85638a05cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# there are some packages I do not know how to map to tables\n",
    "packages_to_ignore = [\n",
    "    # maybe there's a table for these,\n",
    "    # or they're legacy tables\n",
    "    # these are ones we don't care about\n",
    "    ('DINT', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('TINT', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('DCONS', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('DREGION', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('SPDCPC', REPORT_SUBTYPE_NULL_PLACEHOLDER),\n",
    "    ('SRAFINANCIALS', 'RECONCILIATION_SUMMARY'),\n",
    "    ('DAILY', 'MLF'), # electrical transmission loss factors.\n",
    "    ('BILLING_CONFIG', 'BILLSMELTERRATE'), # alumninium smelter info. Possibly relevant? (Smeltering makes up 30% of NSW load)\n",
    "    ('BILLING', 'CSP_SUPPORTDATA_SRA'),\n",
    "    ('BILLING', 'ASPAYMENT_SUMMARY'), # probably belongs to BILLINGASPAYMENTS table, but this is not relevant to us so I haven't bothered checking\n",
    "    ('BILLING', 'DIRECTION_CRA'),\n",
    "    ('TRADING', 'CUMULATIVE_PRICE'),\n",
    "    ('TREGION', 'NULL'),\n",
    "    ('DAILY', 'WDR_NO_SCADA'),\n",
    "    ('METER_DATA', 'GEN_DUID'),\n",
    "    ('SEVENDAYOUTLOOK', 'PEAK'),\n",
    "    ('TUNIT', 'NULL'),\n",
    "    ('GPG', 'MARKET_SUMMARY'),\n",
    "    ('GPG', 'CASESOLUTION'),\n",
    "    ('GPG', 'CONSTRAINTSOLUTION'),\n",
    "    ('GPG', 'PRICESOLUTION'),\n",
    "    ('GPG', 'INTERCONNECTORSOLUTION'),\n",
    "    ('CAUSER_PAYS_SCADA', 'NETWORK'),\n",
    "    ('PDR_REPORT', 'COLUMN'),\n",
    "    ('DUNIT', 'NULL'),\n",
    "    ('YESTBID', 'BIDDAYOFFER'),\n",
    "    ('YESTBID', 'BIDPEROFFER'),\n",
    "    ('RESIDUE_PRICE_OFFER', 'NULL'),\n",
    "    ('RESIDUE_PRICE_BID', 'NULL'), # columns don't match the RESIDUE_PRICE_BID table, almost match RESIDUE_PRICE_FUNDS_BID\n",
    "    ('IBEI', 'PUBLISHING'),\n",
    "    ('EMSLIMITS', 'LIM_ALTLIM'),\n",
    "    ('DEMAND', 'HISTORIC'),\n",
    "\n",
    "    \n",
    "    ('PDR_REPORT', 'TABLE'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'FILE'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'MAPPING'),    # manifest metadata\n",
    "    ('PDR_REPORT', 'COLUMN'),    # manifest metadata\n",
    "\n",
    "    # Gas data\n",
    "    ('GSH', 'PARTICIPANT_OPSTATE'),\n",
    "    ('GSH', 'PARTICIPANTS'),\n",
    "    ('GSH', 'AUCTION_CURTAILMENT_NOTICE'),\n",
    "    ('GSH', 'AUCTION_PRICE_VOLUME'),\n",
    "    ('GSH', 'BENCHMARK_PRICE'),\n",
    "    ('GSH', 'PARK_SERVICES'),\n",
    "    ('GSH', 'AUCTION_QUANTITIES'),\n",
    "    ('GSH', 'FACILITIES'),\n",
    "    ('GSH', 'TRANSACTION_SUMMARY'),\n",
    "    ('GSH', 'HISTORICAL_SUMMARY'),\n",
    "    ('GSH', 'NOTIONAL_POINTS'),\n",
    "    ('GSH', 'REVISED_AUCTION_QUANTITIES'),\n",
    "    ('GSH', 'PIPELINE_SEGMENTS'),\n",
    "    ('GSH', 'CAPACITY_TRANSACTION'),\n",
    "    ('GSH', 'ZONES'),\n",
    "    ('GSH', 'SERVICE_POINTS'),\n",
    "\n",
    "    # we should maybe double check\n",
    "    ('FORCE_MAJEURE', 'MARKETSUSREGION'),\n",
    "    ('FORCE_MAJEURE', 'MARKETSUSPENSION')\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# there are some packages which aren't in the spreadsheet, but we can guess them\n",
    "# in particular there's a CO2 one I can't \n",
    "package_exceptions = {\n",
    "    ('DISPATCH', 'CASESOLUTION'): 'DISPATCHCASESOLUTION',\n",
    "    ('GENCONSETTRK', REPORT_SUBTYPE_NULL_PLACEHOLDER): 'GENCONSETTRK',\n",
    "    ('DISPATCH', 'REGIONFCASREQUIREMENT'): 'DISPATCH_FCAS_REQ',\n",
    "\n",
    "    # this one is possibly useful\n",
    "    # example file is CO2EII_AVAILABLE_GENERATORS.CSV.gz\n",
    "    # It looks like it might go into BILLING_CO2E_PUBLICATION, but the columns are different\n",
    "    # Example:\n",
    "    #STATIONNAME,DUID,GENSETID,REGIONID,CO2E_EMISSIONS_FACTOR,CO2E_ENERGY_SOURCE,CO2E_DATA_SOURCE\n",
    "    #\"Appin Power Plant\",APPIN,APPIN,NSW1,0.56318004,\"Coal seam methane\",NGA2022\n",
    "    # so define a new table for it\n",
    "    ('CO2EII', 'PUBLISHING'): 'CO2EII_AVAILABLE_GENERATORS',\n",
    "}\n",
    "\n",
    "# There are some tables we skip just because they are huge\n",
    "# most of these are each larger than the 200 smallest tables\n",
    "tables_to_skip = [\n",
    "    'P5MIN_UNITSOLUTION', \n",
    "    'P5MIN_CASESOLUTION',\n",
    "    'P5MIN_CONSTRAINTSOLUTION', \n",
    "    'P5MIN_LOCALPRICE', # not what people get paid. Related to electrical constraints\n",
    "    'DISPATCH_LOCALPRICE', # not what people get paid. Related to electrical constraints\n",
    "    'NETWORK_OUTAGEDETAIL',\n",
    "    'MCC_CASESOLUTION',\n",
    "    'MCC_CONSTRAINTSOLUTION',\n",
    "    'DISPATCHOFFERTRK', \n",
    "    'DISPATCHCASESOLUTION',\n",
    "    'P5MIN_REGIONSOLUTION', \n",
    "    'DISPATCHCONSTRAINT',\n",
    "    'SETFCASREGIONRECOVERY',\n",
    "    'TRADINGPRICE', # surprisingly big\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "130a0264-ba0d-4140-8bf5-3a0923022b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns None if we can't find a table\n",
    "def map_table(report_name, report_subtype) -> Optional[str]:\n",
    "    candidates = metadata_df.loc[(metadata_df[report_name_column] == report_name) & (metadata_df[report_subtype_column] == report_subtype), table_name_column]\n",
    "    # sometimes there's duplicates\n",
    "    # but assert they're all the same answer\n",
    "    tables = set(candidates)\n",
    "    if len(tables) == 0 and (report_name, report_subtype) in package_exceptions:\n",
    "        return package_exceptions[(report_name, report_subtype)]\n",
    "        \n",
    "    if len(tables) != 1:\n",
    "        return None\n",
    "    table = tables.pop() # choose the only one\n",
    "    return table\n",
    "\n",
    "# unit testing\n",
    "assert map_table('DISPATCH', 'PRICE') == 'DISPATCHPRICE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae1c26-4285-4051-93c8-d512f3194ba2",
   "metadata": {},
   "source": [
    "## Run the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ee9f31-8103-4be9-8ac9-912f203707ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a33a683e-6c47-4e1a-80a7-4e5ed1cce0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110911/110911 [9:11:53<00:00,  3.35it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "some files failed to be processed, to debug, run the next cell",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#ret = [process_raw(file) for file in tqdm(files)]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     statuses \u001b[38;5;241m=\u001b[39m [_process_raw(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(files)]\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m statuses), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome files failed to be processed, to debug, run the next cell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: some files failed to be processed, to debug, run the next cell"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "files = [os.path.join(source_dir, p) for p in os.listdir(source_dir)]\n",
    "\n",
    "# shuffle file order\n",
    "# so that we don't do all the small files first\n",
    "# then all the large files last \n",
    "# (or the other way around)\n",
    "# this makes the progress bar more accurate\n",
    "shuffle(files) # mutates list in place\n",
    "\n",
    "if use_multiprocessing:\n",
    "    with Pool(utils.num_cpu()) as p:\n",
    "        statuses = list(tqdm(p.imap(_process_raw, files), total=len(files)))\n",
    "else:\n",
    "    #ret = [process_raw(file) for file in tqdm(files)]\n",
    "    statuses = [_process_raw(file) for file in tqdm(files)]\n",
    "assert all(s is None for s in statuses), \"some files failed to be processed, to debug, run the next cell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ade12-85e2-49c4-b1db-1a0501b76272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the files which didn't download properly\n",
    "# e.g. do they matter for our case?\n",
    "# can you find it elsewhere?\n",
    "# e.g. PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20231226053000_0000000406806404.zip was corrupted\n",
    "# but can be found inside PUBLIC_ROOFTOP_PV_ACTUAL_MEASUREMENT_20231221.zip\n",
    "# have a look in bad_zip_dir and bad_csv_dir for the troublesome files and error info written in adjacent text files.\n",
    "[(str(s)[:100], f) for (s,f) in zip(statuses, files) if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b0b70-b66c-4a4c-90cc-860bb140058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(source_dir, p) for p in os.listdir(source_dir)]\n",
    "[f for f in files if 'DUD' in f][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e1013-ec94-4772-82b1-689f90ade4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ade9c-3136-430e-ba7a-81774840eaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
