{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the clean and combine function\n",
    "def clean_and_combine(file_path):\n",
    "    #Load the data\n",
    "    sunshine_data = pd.read_csv(file_path)\n",
    "\n",
    "    #Clean Data\n",
    "    sunshine_data['Date'] = pd.to_datetime(sunshine_data[['Year', 'Month', 'Day']])\n",
    "    sunshine_data = sunshine_data.drop(columns=[\"Product code\", \n",
    "                                            \"Bureau of Meteorology station number\",\n",
    "                                            \"Year\", \"Month\", \"Day\"])\n",
    "    sunshine_data = sunshine_data.loc[sunshine_data['Date'] >= '2009-01-01']\n",
    "\n",
    "    sunshine_data = sunshine_data.rename(columns={'Daily global solar exposure (MJ/m*m)': 'solar_exposure',})\n",
    "    sunshine_data['solar_exposure'] = sunshine_data['solar_exposure']/3.6\n",
    "    \n",
    "    # Correct NaN\n",
    "    # Calculate the rolling mean with a window of 7 days (3 days before, the current day, and 3 days after)\n",
    "    sunshine_data['rolling_mean'] = sunshine_data['solar_exposure'].rolling(window=7, min_periods=1, center=True).mean()\n",
    "    sunshine_data['solar_exposure'] = sunshine_data['solar_exposure'].fillna(sunshine_data['rolling_mean'])\n",
    "    sunshine_data = sunshine_data.drop(columns=['rolling_mean'])\n",
    "\n",
    "\n",
    "    # Define the city-region mapping\n",
    "    city_region_map = {\n",
    "            'cooberpedy': 'SA1',\n",
    "            'richmond': 'QLD1',\n",
    "            'dubbo': 'NSW1',\n",
    "            'bendigo': 'VIC1',\n",
    "            'hobart': 'TAS1'\n",
    "                }  \n",
    "    \n",
    "    # Extract the city name from the file name\n",
    "    city_name = os.path.basename(file_name).split('sunshine-')[-1].split('.csv')[0]\n",
    "    region_code = city_region_map.get(city_name)\n",
    "    sunshine_data['regionid'] = region_code\n",
    "        \n",
    "    # Append cleaned data to list\n",
    "    all_data.append(sunshine_data)\n",
    "    print(f'Data cleaned and added to list for {region_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where your CSV files are stored \n",
    "data_dir = '/home/matthew/data/'\n",
    "weather_dir = f'{data_dir}/sunshine data'\n",
    "all_data = []\n",
    "# Loop through each CSV file in the directory\n",
    "for file_name in os.listdir(weather_dir):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(weather_dir, file_name)\n",
    "\n",
    "        # Call the function to clean \n",
    "        clean_and_combine(file_path)\n",
    "\n",
    "\n",
    "# Merge all data frames\n",
    "merged_data = pd.concat(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some temperature is missing\n",
    "# check it's not on consecutive days\n",
    "# (so we can just interpolate to fill in gaps.)\n",
    "merged_data.sort_values(by=['regionid', 'Date'], inplace=True)\n",
    "\n",
    "s = merged_data['solar_exposure']\n",
    "r = merged_data['regionid']\n",
    "merged_data['missing_consec'] = s.isna() & s.shift(1, fill_value=1).isna() & (r == r.shift(1))\n",
    "\n",
    "merged_data[merged_data['missing_consec'] | merged_data['missing_consec'].shift(-1) | merged_data['missing_consec'].shift(-2) | merged_data['missing_consec'].shift(2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, fill in the gaps with a straight line\n",
    "# come back later and find a better way.\n",
    "# (We could also switch to AEMO data, or the hours of sunlight data from astral. But the units would be different.)\n",
    "\n",
    "# It seems we have up to 2 days missing in a row.\n",
    "# let's just linearly interpolate the gaps\n",
    "#merged_data.groupby('regionid').apply(lambda group: group['temperature'].interpolate(method='linear'))\n",
    "merged_data = merged_data.set_index('regionid').groupby('regionid').transform(pd.DataFrame.interpolate).reset_index()\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=['missing_consec'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged data to CSV\n",
    "merged_data.to_csv(f'{data_dir}/07-a-sunshine-merged.csv', index=False)\n",
    "print('All data merged and saved to CSV')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
