{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433b56bd-68ce-4bfc-80ca-b44d171ce6d4",
   "metadata": {},
   "source": [
    "# Deduplicate\n",
    "\n",
    "AEMO has files which they publish every 5 minutes. And then after a few days they aggregate them into daily files. That's all per \"package\" (group of tables). And after a while they aggregate into monthly files, for all tables, and monthly files for each table. And some files are supposed to contain 'updated' data for previous files. So we need to deduplicate based on some 'primary key' columns. When there's a clash, we need to choose inteligently.\n",
    "\n",
    "The primary keys are from AEMO's schema documentation, which we webscraped in an earlier script.\n",
    "The sorting rule is:\n",
    "\n",
    "* choose the largest `SCHEMA_VERSION` (a metadata field we added in the splitting stage earlier. Not present in AEMO's schema.)\n",
    "* choose the largest `TOP_TIMESTAMP` (i.e. the file which AEMO generated most recently)\n",
    "* If there is a `LASTCHANGED` column (most tables have this), this is another measure of when AEMO generated the data. Choose the largest (i.e. most recent data)\n",
    "\n",
    "## Small Files\n",
    "\n",
    "For sets of files small enough to fit into memory, we'll use [`polars`](https://docs.pola.rs/), which is like pandas but faster (and can handle empty values for all datatypes.)\n",
    "\n",
    "For the small tables, they're typically reference data, like which region each generator is in. This gets republished each month, so we end up with >90% duplicated data.\n",
    "\n",
    "## Large files\n",
    "\n",
    "If the sum of the parquet files for a table is too large to fit into memory, we'll have to use some fancy techniques. (Deduplication normally involves sorting the whole list, or group by keys and then sort. Either way you need to have all rows for the key columns in memory. Which we can't do. Polars claims to be able to do it in 'streaming' mode, but that ends up using up all memory.)\n",
    "\n",
    "So we need to use a different approach. For now, I don't have time. I've checked, and I think we only have 10% duplicated data for these datasets. That's probably for the week before we downloaded the data (when daily and 5-minute files overlap) which was not near a DST transition.\n",
    "\n",
    "Suppose we want to deduplicate on primary key columns `A` and `B`, sorting by `C`, `D`, keeping data column `E`). The process I want to use is for each input file, write out to new parquet files, with hive partitioning. e.g. `A=1/file.parquet`, `A=2/file.parquet`. (We keep `B` inside the parquet file, because otherwise we'd have too many tiny files. That's not performant. But by splitting into one file per `A`, each resulting folder is small enough to fit into memory. Then We can deduplicate each folder based on only column `B` (sorting by `C` and `D`). Then we can recombine them. (Although that's kind of unecessary.)\n",
    "\n",
    "## Notes\n",
    "\n",
    "Polars uses multithreading for us. So we won't use `multiprocessing.Pool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e462eca3-01bc-4a70-8694-386e1050c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import psutil\n",
    "import importlib\n",
    "\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026391c6-21d7-4263-a71e-91b0a8b9f0e5",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479d67c-3266-4574-9975-8cbd27d9f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data/'\n",
    "laptop_data_dir = '/home/matthew/data/'\n",
    "\n",
    "# result of the previous script\n",
    "source_dir = os.path.join(laptop_data_dir, '01-D-parquet-batches')\n",
    "\n",
    "# result of this script\n",
    "dest_dir = os.path.join(laptop_data_dir, '01-E-parquet-deduplicated')\n",
    "\n",
    "schema_path = os.path.join(repo_data_dir, 'schemas.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a17d1-f585-4576-bc8b-996baf5d6fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that if a parquet file is x bytes\n",
    "# once loaded into memory it will be x * compression_factor bytes\n",
    "compression_factor = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aeef83-2e66-49e0-9563-25a8d2cae584",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def2cbed-ca33-4ef2-a5d2-8f161683fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(os.path.join(repo_data_dir, 'logs.txt'), flush=True)\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547b4f4-e810-4be5-b6e5-c36e2ad6f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.renice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e72d11-7e25-43c9-af88-d3b0bddec121",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(schema_path, 'r') as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1655ecbb-eeaf-4cca-8506-ac9cfab8acdc",
   "metadata": {},
   "source": [
    "## Check size of each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265bff7-5cd6-47bd-8370-11a041208438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have this many bytes of memory free\n",
    "avail_memory = psutil.virtual_memory().available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f8ece-1a7d-43eb-87ae-ec4567e0d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_size(dir):\n",
    "    sz = 0\n",
    "    for path in utils.walk(dir):\n",
    "        sz = sz + os.path.getsize(path)\n",
    "    return sz\n",
    "\n",
    "tables = os.listdir(source_dir)\n",
    "table_sizes = {table: get_dir_size(os.path.join(source_dir, table)) for table in tables}\n",
    "\n",
    "big_tables = [t for t in tables if (table_sizes[t] * compression_factor > avail_memory) / 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d498f92-3b34-47e7-af7f-2001e01d9c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_dir(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644a686-885c-43c1-b7e3-46398302c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tqdm(tables):\n",
    "    logger.info(f\"Starting {table}\")\n",
    "    table_source_dir = os.path.join(source_dir, table)\n",
    "    source_paths = list(utils.walk(table_source_dir))\n",
    "    dest_path = os.path.join(dest_dir, table + '.parquet')\n",
    "\n",
    "    logger.info(f\"Source files {source_paths}\")\n",
    "    ds = pl.scan_parquet(source_paths)\n",
    "    \n",
    "    primary_keys = schemas[table]['primary_keys']\n",
    "    sort_keys = ['SCHEMA_VERSION', 'TOP_TIMESTAMP']\n",
    "    if 'LASTCHANGED' in ds.columns:\n",
    "        sort_keys.append('LASTCHANGED')\n",
    "\n",
    "    assert ds.fetch().shape[0] > 0, f\"No rows in {len(source_paths)} files for {table}\"\n",
    "    \n",
    "    if table in big_tables:\n",
    "        # for now, don't deduplicate\n",
    "        # do merge into one\n",
    "        ds.sink_parquet(dest_path)\n",
    "    else:\n",
    "        (\n",
    "            ds.sort(sort_keys, descending=True)\n",
    "            .unique(primary_keys)\n",
    "            .select(pl.exclude(\"SCHEMA_VERSION\", \"TOP_TIMESTAMP\"))\n",
    "            .sink_parquet(dest_path)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
