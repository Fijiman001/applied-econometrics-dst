{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3019da4-a1bd-4279-8446-2b5e5c4fca32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/matthew/Documents/TSE/AppliedEconometrics/repo/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import importlib\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from io import TextIOWrapper\n",
    "import urllib.parse\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d772e32d-0083-4320-8dd1-de9efdc4bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data/'\n",
    "laptop_data_dir = '/home/matthew/data/'\n",
    "\n",
    "# output of the previous script\n",
    "#source_dir = os.path.join(laptop_data_dir, '01-C-split-mapped-csv')\n",
    "source_dir = os.path.join(laptop_data_dir, '01-D-split-mapped-csv-done')\n",
    "\n",
    "# save the list of tables we have already processed in here\n",
    "# so with repeated runs, we don't redo work\n",
    "partition_progress_file = os.path.join(laptop_data_dir, '01-D-consolidate-partition-progress.txt')\n",
    "dedup_progress_file = os.path.join(laptop_data_dir, '01-D-consolidate-dedup-progress.txt')\n",
    "\n",
    "# the parquet files go here\n",
    "partitioned_duped_dir = os.path.join(laptop_data_dir, '01-D-consolidate-csv-partitioned')\n",
    "partitioned_deduped_dir = os.path.join(laptop_data_dir, '01-D-consolidate-csv-partitioned-deduplicated')\n",
    "\n",
    "schema_path = os.path.join(repo_data_dir, 'schemas.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb02f4a-20a7-4ee3-ba3f-19709f42bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_multiprocessing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0333dc1d-4eb9-4ee8-8240-05bca8c612e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_col_name = 'SCHEMA_VERSION'\n",
    "top_timestamp_col_name = 'TOP_TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1060466c-70d4-448e-80af-a7ad2835636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(schema_path, 'r') as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ba3ec3-ced0-4562-a62f-c33416aacb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(os.path.join(repo_data_dir, 'logs.txt'))\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d81468-9b29-4f41-924f-729f68240005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (table, schema) in schemas.items():\n",
    "    schema['partition_key_names'] = schema['primary_keys']\n",
    "    schema['partition_key_names'] = [c for c in schema['partition_key_names'] if 'DATE' not in c.upper()]\n",
    "    schema['partition_key_names'] = [c for c in schema['partition_key_names'] if c.upper() not in ['RUNNO', 'DISPATCHINTERVAL', 'CONTRACTID']]\n",
    "    \n",
    "    if table in ['DISPATCHLOAD', 'DISPATCHREGIONSUM']:\n",
    "        schema['columns_to_drop'] = [c for c in schemas[table]['columns'] if any(ss in c for ss in ('RAISE', 'LOWER', 'VIOLATION'))]\n",
    "    else:\n",
    "        schema['columns_to_drop'] = []  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdbc12f-2e1c-4811-8bc5-10ebb67c2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEMO's schemas have Oracle SQL types\n",
    "# map those to types polars can use\n",
    "# if date_as_str, return string instead of datetime\n",
    "# (because polars can't read datetimes when parsing from CSV)\n",
    "def aemo_type_to_polars_type(t: str, tz=None, date_as_str=False):\n",
    "    t = t.upper()\n",
    "    if re.match(r\"VARCHAR(2)?\\(\\d+\\)\", t):\n",
    "        return pl.String()\n",
    "    if re.match(r\"CHAR\\((\\d+)\\)\", t):\n",
    "        # single character\n",
    "        # arrow has no dedicated type for that\n",
    "        # so use string\n",
    "        # (could use categorical?)\n",
    "        return pl.String()\n",
    "    elif t.startswith(\"NUMBER\"):\n",
    "        match = re.match(r\"NUMBER ?\\((\\d+), ?(\\d+)\\)\", t)\n",
    "        if match:\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = int(match.group(2))\n",
    "        else:\n",
    "            # e.g. NUMBER(2)\n",
    "            match = re.match(r\"NUMBER ?\\((\\d+)\", t)\n",
    "            assert match, f\"Unsure how to cast {t} to arrow type\"\n",
    "            whole_digits = int(match.group(1))\n",
    "            decimal_digits = 0\n",
    "            \n",
    "        if decimal_digits == 0:\n",
    "            # integer\n",
    "            # we assume signed (can't tell unsigned from the schema)\n",
    "            # but how many bits?\n",
    "            max_val = 10**whole_digits\n",
    "\n",
    "            if 2**(8-1) > max_val:\n",
    "                return pl.Int8()\n",
    "            elif 2**(16-1) > max_val:\n",
    "                return pl.Int16()\n",
    "            elif 2**(32-1) > max_val:\n",
    "                return pl.Int32()\n",
    "            else:\n",
    "                return pl.Int64()\n",
    "        else:\n",
    "            # we could use pa.decimal128(whole_digits, decimal_digits)\n",
    "            # but we don't need that much accuracy\n",
    "            return pl.Float64()\n",
    "    elif (t == 'DATE') or re.match(r\"TIMESTAMP\\((\\d)\\)\", t):\n",
    "        # watch out, when AEMO say \"date\" they mean \"datetime\"\n",
    "        # for both dates and datetimes they say \"date\",\n",
    "        # but both have a time component. (For actual dates, it's always midnight.)\n",
    "        # and some dates go out as far as 9999-12-31 23:59:59.999\n",
    "        # (and some dates are 9999-12-31 23:59:59.997)\n",
    "        if date_as_str:\n",
    "            return pl.String()\n",
    "        else:\n",
    "            return pl.Datetime(time_unit='ms', time_zone=tz)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsure how to convert AEMO type {t} to polars type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45467f-955e-48e1-be69-0d365a403a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████████▍                                                                                                                            | 20/142 [00:00<00:02, 44.61it/s]"
     ]
    }
   ],
   "source": [
    "def repartition(table):\n",
    "    table_source_dir = os.path.join(source_dir, table)\n",
    "    table_partitioned_duped_dir = os.path.join(partitioned_duped_dir, table)\n",
    "    schema = schemas[table]\n",
    "    in_columns = list(schema['columns'].keys())\n",
    "    out_columns = in_columns + [version_col_name, top_timestamp_col_name]\n",
    "    \n",
    "    out_columns = [c for c in out_columns if (c not in schema['columns_to_drop']) and (c not in schema['partition_key_names'])]\n",
    "    \n",
    "    shutil.rmtree(table_partitioned_duped_dir, ignore_errors=True)\n",
    "    file_handles = OrderedDict()\n",
    "    try:\n",
    "        for csv_path in utils.walk(table_source_dir):\n",
    "            match = re.search(f\"/{version_col_name}=(\\d+)/\", csv_path)\n",
    "            assert match, f\"Unable to extract schema version from {csv_path}\"\n",
    "            schema_version = int(match.group(1))\n",
    "        \n",
    "            match = re.search(f\"/{top_timestamp_col_name}=([\\d_]+)/\", csv_path)\n",
    "            assert match, f\"Unable to extract top_timestamp from {csv_path}\"\n",
    "            top_timestamp = match.group(1)\n",
    "            \n",
    "            with gzip.open(csv_path, 'rt', newline='') as f_src_str:\n",
    "                reader = csv.DictReader(f_src_str)\n",
    "                for row in reader:\n",
    "        \n",
    "                    row.update({\n",
    "                        version_col_name: schema_version,\n",
    "                        top_timestamp_col_name: top_timestamp\n",
    "                    })\n",
    "                    \n",
    "                    partition_key_values = tuple(row.get(c, None) for c in schema['partition_key_names'])\n",
    "        \n",
    "                    if partition_key_values in file_handles:\n",
    "                        (f, writer) = file_handles[partition_key_values]\n",
    "                        file_handles.move_to_end(partition_key_values)\n",
    "                        new = False\n",
    "                    else:\n",
    "                        # decide where to save it\n",
    "                        partition_subdirs = [f\"{k}={urllib.parse.quote_plus(v)}\" for (k,v) in zip(schema['partition_key_names'], partition_key_values)]\n",
    "                        dest_path = os.path.join(table_partitioned_duped_dir, *partition_subdirs, 'data.csv.gz')\n",
    "                        if os.path.exists(dest_path):\n",
    "                            # logger.info(f\"Re-using file for {partition_key_values}\")\n",
    "                            try:\n",
    "                                f = gzip.open(dest_path, 'at', compresslevel=2, newline='')\n",
    "                            except OSError as e:\n",
    "                                if e.strerror == 'Too many open files':\n",
    "                                    for _ in range(5):\n",
    "                                        (old_keys, (old_f, old_writer)) = file_handles.popitem(last=True)\n",
    "                                        old_f.close()\n",
    "                                    f = gzip.open(dest_path, 'at', compresslevel=2, newline='')\n",
    "                                else:\n",
    "                                    raise\n",
    "                            writer = csv.DictWriter(f, fieldnames=out_columns, extrasaction='ignore')\n",
    "                        else:\n",
    "                            # logger.info(f\"Creating new file at {dest_path}\")\n",
    "                            utils.create_dir(file=dest_path)\n",
    "                            try:\n",
    "                                f = gzip.open(dest_path, 'wt', compresslevel=2, newline='')\n",
    "                            except OSError as e:\n",
    "                                if e.strerror == 'Too many open files':\n",
    "                                    for _ in range(5):\n",
    "                                        (old_keys, (old_f, old_writer)) = file_handles.popitem(last=True)\n",
    "                                        old_f.close()\n",
    "                                    f = gzip.open(dest_path, 'wt', compresslevel=2, newline='')\n",
    "                                else:\n",
    "                                    raise\n",
    "                            writer = csv.DictWriter(f, fieldnames=out_columns, extrasaction='ignore')\n",
    "                            writer.writeheader()\n",
    "                        \n",
    "                        file_handles[partition_key_values] = (f, writer)\n",
    "        \n",
    "                    writer.writerow(row)\n",
    "            logger.flush()\n",
    "        logger.info(f\"Finished for {table}\")\n",
    "    finally:\n",
    "        # tidy up\n",
    "        for (f, writer) in file_handles.values():\n",
    "            f.close()\n",
    "\n",
    "    with open(partition_progress_file, 'a') as f:\n",
    "        f.write(table + '\\n')\n",
    "\n",
    "\n",
    "tables = os.listdir(source_dir)\n",
    "\n",
    "try:\n",
    "    with open(partition_progress_file, 'r') as f:\n",
    "        already_partitioned = [line.strip() for line in f if line.strip()]\n",
    "except FileNotFoundError:\n",
    "    # first run\n",
    "    already_partitioned = []\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(dedup_progress_file, 'r') as f:\n",
    "        already_deduped = [line.strip() for line in f if line.strip()]\n",
    "except FileNotFoundError:\n",
    "    # first run\n",
    "    already_deduped = []\n",
    "\n",
    "\n",
    "\n",
    "def dedup(table):\n",
    "    table_partitioned_duped_dir = os.path.join(partitioned_duped_dir , table)\n",
    "    table_partitioned_deduped_dir = os.path.join(partitioned_deduped_dir , table)\n",
    "\n",
    "\n",
    "    src_columns = [c for c in schemas[table]['columns'] if c not in schemas[table]['columns_to_drop'] and c not in schemas[table]['partition_key_names']]\n",
    "    schema = {c: aemo_type_to_polars_type(schemas[table]['columns'][c]['AEMO_type'], date_as_str=True) for c in src_columns}\n",
    "    schema.update({\n",
    "        version_col_name: pl.UInt8(),\n",
    "        top_timestamp_col_name: pl.String(),\n",
    "    })\n",
    "    sort_keys = [version_col_name, top_timestamp_col_name]\n",
    "    if 'LASTCHANGED' in schema:\n",
    "        sort_keys.append('LASTCHANGED')\n",
    "\n",
    "    primary_keys = [p for p in schemas[table]['primary_keys'] if p not in schemas[table]['partition_key_names']]\n",
    "    if not os.path.exists(table_partitioned_duped_dir):\n",
    "        raise ValueError(f\"Source folder {table_partitioned_duped_dir} does not exist\")\n",
    "    logger.info(f\"Listing files in {table_partitioned_duped_dir}, with {schema}\")\n",
    "    for src_path in utils.walk(table_partitioned_duped_dir):\n",
    "        try:\n",
    "            sub_path = os.path.relpath(path=src_path, start=table_partitioned_duped_dir)\n",
    "            dest_path = os.path.join(table_partitioned_deduped_dir, sub_path)\n",
    "    \n",
    "            # polars can't write to .csv.gz directly, only .csv\n",
    "            # but it can write to a file-like object of a gzipped file\n",
    "            utils.create_dir(file=dest_path)\n",
    "            logger.info(f\"Openning {src_path=}\")\n",
    "            logger.flush()\n",
    "            with gzip.open(dest_path, 'wt', compresslevel=4, newline='') as f_dest:\n",
    "                (\n",
    "                    pl.read_csv(src_path, dtypes=schema)\n",
    "                    .sort(sort_keys, descending=True)\n",
    "                    .unique(primary_keys or None)\n",
    "                    .select(pl.exclude(version_col_name, top_timestamp_col_name))\n",
    "                    .write_csv(f_dest)\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {table=} {src_path=}\")\n",
    "            raise\n",
    "\n",
    "    with open(dedup_progress_file, 'a') as f:\n",
    "        f.write(table + '\\n')\n",
    "\n",
    "# no multiprocessing\n",
    "# because we'll hit the max file handler count sooner\n",
    "try:\n",
    "    for table in tqdm(tables):\n",
    "        gc.collect()\n",
    "        if table not in already_partitioned:\n",
    "            repartition(table)\n",
    "        if (table not in already_partitioned) or (table not in already_deduped):\n",
    "            dedup(table)\n",
    "finally:\n",
    "    logger.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fdc92-0743-4bb7-a0f9-3a0084242cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: partition P5MIN_INTERCONNECTORSOLN by year(INTERVAL_DATETIME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e4663-d626-407f-b883-24b70dce619f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
