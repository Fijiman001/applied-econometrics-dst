{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3019da4-a1bd-4279-8446-2b5e5c4fca32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/matthew/Documents/TSE/AppliedEconometrics/repo/utils.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "import importlib\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from io import TextIOWrapper\n",
    "import urllib.parse\n",
    "from collections import OrderedDict\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d772e32d-0083-4320-8dd1-de9efdc4bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_data_dir = '/home/matthew/Documents/TSE/AppliedEconometrics/repo/data/'\n",
    "laptop_data_dir = '/home/matthew/data/'\n",
    "\n",
    "# output of the previous script\n",
    "#source_dir = os.path.join(laptop_data_dir, '01-C-split-mapped-csv')\n",
    "source_dir = os.path.join(laptop_data_dir, '01-D-split-mapped-csv-done')\n",
    "\n",
    "# save the list of tables we have already processed in here\n",
    "# so with repeated runs, we don't redo work\n",
    "progress_file = os.path.join(laptop_data_dir, '01-D-consolidate-partition-progress.txt')\n",
    "\n",
    "# the parquet files go here\n",
    "partitioned_duped_dir = os.path.join(laptop_data_dir, '01-D-consolidate-csv-partitioned')\n",
    "partitioned_deduped_dir = os.path.join(laptop_data_dir, '01-D-consolidate-csv-partitioned-deduplicated')\n",
    "\n",
    "schema_path = os.path.join(repo_data_dir, 'schemas.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bb02f4a-20a7-4ee3-ba3f-19709f42bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_multiprocessing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0333dc1d-4eb9-4ee8-8240-05bca8c612e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_col_name = 'SCHEMA_VERSION'\n",
    "top_timestamp_col_name = 'TOP_TIMESTAMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1060466c-70d4-448e-80af-a7ad2835636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(schema_path, 'r') as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34ba3ec3-ced0-4562-a62f-c33416aacb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(os.path.join(repo_data_dir, 'logs.txt'))\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3d81468-9b29-4f41-924f-729f68240005",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (table, schema) in schemas.items():\n",
    "    schema['partition_key_names'] = schema['primary_keys']\n",
    "    schema['partition_key_names'] = [c for c in schema['partition_key_names'] if 'DATE' not in c.upper()]\n",
    "    schema['partition_key_names'] = [c for c in schema['partition_key_names'] if c.upper() not in ['RUNNO', 'DISPATCHINTERVAL', 'CONTRACTID']]\n",
    "    \n",
    "    if table == 'DISPATCHLOAD':\n",
    "        schema['columns_to_drop'] = [c for c in schemas[table]['columns'] if any(ss in c for ss in ('RAISE', 'LOWER', 'VIOLATION'))]\n",
    "    else:\n",
    "        schema['columns_to_drop'] = []  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b45467f-955e-48e1-be69-0d365a403a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                 | 0/131 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tqdm(tables):\n\u001b[1;32m    109\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 35\u001b[0m, in \u001b[0;36mrepartition\u001b[0;34m(table)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(csv_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_src_str:\n\u001b[1;32m     34\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(f_src_str)\n\u001b[0;32m---> 35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion_col_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_timestamp_col_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_timestamp\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpartition_key_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/csv.py:110\u001b[0m, in \u001b[0;36mDictReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;66;03m# Used only for its side effect.\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfieldnames\u001b[49m\n\u001b[1;32m    111\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mline_num\n",
      "File \u001b[0;32m/usr/lib/python3.11/csv.py:97\u001b[0m, in \u001b[0;36mDictReader.fieldnames\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fieldnames \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fieldnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/gzip.py:314\u001b[0m, in \u001b[0;36mGzipFile.read1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     size \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread1(size)\n",
      "File \u001b[0;32m/usr/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/lib/python3.11/gzip.py:499\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;66;03m# If the _new_member flag is set, we have to\u001b[39;00m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# jump to the next member, if there is one.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_read()\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.11/gzip.py:468\u001b[0m, in \u001b[0;36m_GzipReader._read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_gzip_header\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 468\u001b[0m     last_mtime \u001b[38;5;241m=\u001b[39m \u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_mtime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/gzip.py:423\u001b[0m, in \u001b[0;36m_read_gzip_header\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_gzip_header\u001b[39m(fp):\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Read a gzip header from `fp` and progress to the end of the header.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Returns last mtime if header was present or None otherwise.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     magic \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    425\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/gzip.py:97\u001b[0m, in \u001b[0;36m_PaddedFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     94\u001b[0m read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[read:] \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m---> 97\u001b[0m        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;241m.\u001b[39mread(size\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length\u001b[38;5;241m+\u001b[39mread)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def repartition(table):\n",
    "    table_source_dir = os.path.join(source_dir, table)\n",
    "    table_partitioned_duped_dir = os.path.join(partitioned_duped_dir, table)\n",
    "    schema = schemas[table]\n",
    "    in_columns = list(schema['columns'].keys())\n",
    "    out_columns = in_columns + [version_col_name, top_timestamp_col_name]\n",
    "    \n",
    "    out_columns = [c for c in out_columns if (c not in schema['columns_to_drop']) and (c not in schema['partition_key_names'])]\n",
    "    \n",
    "    shutil.rmtree(table_partitioned_duped_dir, ignore_errors=True)\n",
    "    file_handles = OrderedDict()\n",
    "    try:\n",
    "        for csv_path in utils.walk(table_source_dir):\n",
    "            match = re.search(f\"/{version_col_name}=(\\d+)/\", csv_path)\n",
    "            assert match, f\"Unable to extract schema version from {csv_path}\"\n",
    "            schema_version = int(match.group(1))\n",
    "        \n",
    "            match = re.search(f\"/{top_timestamp_col_name}=([\\d_]+)/\", csv_path)\n",
    "            assert match, f\"Unable to extract top_timestamp from {csv_path}\"\n",
    "            top_timestamp = match.group(1)\n",
    "            \n",
    "            with gzip.open(csv_path, 'rt', newline='') as f_src_str:\n",
    "                reader = csv.DictReader(f_src_str)\n",
    "                for row in reader:\n",
    "        \n",
    "                    row.update({\n",
    "                        version_col_name: schema_version,\n",
    "                        top_timestamp_col_name: top_timestamp\n",
    "                    })\n",
    "                    \n",
    "                    partition_key_values = tuple(row.get(c, None) for c in partition_key_names)\n",
    "        \n",
    "                    if partition_key_values in file_handles:\n",
    "                        (f, writer) = file_handles[partition_key_values]\n",
    "                        file_handles.move_to_end(partition_key_values)\n",
    "                        new = False\n",
    "                    else:\n",
    "                        # decide where to save it\n",
    "                        partition_subdirs = [f\"{k}={urllib.parse.quote_plus(v)}\" for (k,v) in zip(partition_key_names, partition_key_values)]\n",
    "                        dest_path = os.path.join(table_partitioned_duped_dir, *partition_subdirs, 'data.csv.gz')\n",
    "                        if os.path.exists(dest_path):\n",
    "                            # logger.info(f\"Re-using file for {partition_key_values}\")\n",
    "                            try:\n",
    "                                f = gzip.open(dest_path, 'at', compresslevel=2, newline='')\n",
    "                            except OSError as e:\n",
    "                                if e.strerror == 'Too many open files':\n",
    "                                    for _ in range(5):\n",
    "                                        (old_keys, (old_f, old_writer)) = file_handles.popitem(last=True)\n",
    "                                        old_f.close()\n",
    "                                    f = gzip.open(dest_path, 'at', compresslevel=2, newline='')\n",
    "                                else:\n",
    "                                    raise\n",
    "                            writer = csv.DictWriter(f, fieldnames=out_columns, extrasaction='ignore')\n",
    "                        else:\n",
    "                            # logger.info(f\"Creating new file at {dest_path}\")\n",
    "                            utils.create_dir(file=dest_path)\n",
    "                            try:\n",
    "                                f = gzip.open(dest_path, 'wt', compresslevel=2, newline='')\n",
    "                            except OSError as e:\n",
    "                                if e.strerror == 'Too many open files':\n",
    "                                    for _ in range(5):\n",
    "                                        (old_keys, (old_f, old_writer)) = file_handles.popitem(last=True)\n",
    "                                        old_f.close()\n",
    "                                    f = gzip.open(dest_path, 'wt', compresslevel=2, newline='')\n",
    "                                else:\n",
    "                                    raise\n",
    "                            writer = csv.DictWriter(f, fieldnames=out_columns, extrasaction='ignore')\n",
    "                            writer.writeheader()\n",
    "                        \n",
    "                        file_handles[partition_key_values] = (f, writer)\n",
    "        \n",
    "                    writer.writerow(row)\n",
    "            logger.flush()\n",
    "        logger.info(f\"Finished for {table}\")\n",
    "    finally:\n",
    "        # tidy up\n",
    "        for (f, writer) in file_handles.values():\n",
    "            f.close()\n",
    "\n",
    "    with open(progress_file, 'a') as f:\n",
    "        f.write(table + '\\n')\n",
    "\n",
    "\n",
    "tables = os.listdir(source_dir)\n",
    "\n",
    "try:\n",
    "    with open(progress_file, 'r') as f:\n",
    "        already_processed = [line.strip() for line in f if line.strip()]\n",
    "    tables = [t for t in tables if t not in already_processed]\n",
    "except FileNotFoundError:\n",
    "    # first run\n",
    "    pass\n",
    "\n",
    "def dedup(table):\n",
    "    table_partitioned_duped_dir = os.path.join(partitioned_duped_dir , table)\n",
    "    table_partitioned_deduped_dir = os.path.join(partitioned_deduped_dir , table)\n",
    "\n",
    "    for src_file in utils.walk(table_partitioned_duped_dir):\n",
    "        sub_path = os.path.relpath(path=src_file, start=table_partitioned_duped_dir)\n",
    "        dest_file = os.path.join(table_partitioned_deduped_dir, sub_path)\n",
    "\n",
    "        src_columns = [c for c in schemas[table]['columns'] if c not in schemas[table]['columns_to_drop'] and c not in schemas[table]['partition_key_names']]\n",
    "        schema = {c: aemo_type_to_arrow_type(schemas[table]['columns'][c]) for c in src_columns}\n",
    "        schema.update({\n",
    "            version_col_name: pl.UInt8()\n",
    "            top_timestamp_col_name: pl.String(),\n",
    "        })\n",
    "\n",
    "        # do the dedup\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "\n",
    "# no multiprocessing\n",
    "# because we'll hit the max file handler count sooner\n",
    "for table in tqdm(tables):\n",
    "    gc.collect()\n",
    "    repartition(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49c51505-d3c7-4dbf-a391-54137e3fda86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "String"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.String()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e7431-f50e-40c1-889f-2e6459f625d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
