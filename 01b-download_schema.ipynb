{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1200724-6ec3-4953-83c1-483647f24956",
   "metadata": {},
   "source": [
    "# webscrape AEMO schemas\n",
    "\n",
    "This playbook downloads [these web pages](https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_504_9.htm) to detect the list of columns, column types, and index columns for each table.\n",
    "\n",
    "Each table is documented on a page between these URLs:\n",
    "\n",
    "* https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_245.htm#1\n",
    "* https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_503.htm#1\n",
    "\n",
    "So we do a for loop to grab each one.\n",
    "\n",
    "We want the list of \"Primary Key Columns\", \"Index Columns\" (there may be multiple sets of these, and \"Content\", columns \"Name\" and \"Data Type\"\n",
    "\n",
    "This file is a work in progress. I want to add it to \"01-download.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4de3d-bf67-45b3-855b-def3e2dadf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # webscraping\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c0922-dea5-4b9b-b722-3906db80ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session object to re-use between requests\n",
    "# to hopefully speed up downloads by not re-doing the TLS handshake for each HTTP request\n",
    "# (unsure if this actually speeds things up)\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b21ef8-c0b4-47e0-841c-b015faaaa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {}\n",
    "for i in tqdm(range(1, 503+1)):\n",
    "    url = f\"https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_{i}.htm#1\"\n",
    "    r = session.get(url)\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "    \n",
    "    # it's called \"soup\" because the python webscraping library is called \"beautiful soup\"\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    h2 = soup.find('h2')\n",
    "    if h2 is None:\n",
    "        # not a table schema page, this is another page\n",
    "        # ignore it\n",
    "        # e.g. https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_2.htm#1\n",
    "        continue\n",
    "    h2_text = h2.text.strip()\n",
    "    if not h2_text.startswith(\"Table: \"):\n",
    "        if not h2_text.strip().startswith('Package') and not h2_text.strip().startswith('Diagram'):\n",
    "            print(f\"Ignoring page with h2: {h2_text}: {url}\")\n",
    "    else:\n",
    "        # watch out, sometimes the documentation has an incorrect space inside the table name\n",
    "        # e.g. SET_ APC_COMPENSATION instead of SET_APC_COMPENSATION\n",
    "        table_name = re.match(r\"Table: (.*)\", h2_text).group(1).replace(' ','')\n",
    "        \n",
    "        schemas[table_name] = {\n",
    "            'indexes': [],\n",
    "            'columns': {}\n",
    "        }\n",
    "        for (i, h3) in enumerate(soup.find_all('h3')):\n",
    "            h3_name = h3.find('a').get('name').strip()\n",
    "            h3_text = h3.text.strip()\n",
    "            if i == 0:\n",
    "                assert table_name == h3.text.strip().replace(' ',''), f\"Table name from top h2 was {table_name}, from first h3 was {h3.text.strip()} in {url}\"\n",
    "            elif h3_text == \"Primary Key Columns\":\n",
    "                # the next element is a table.\n",
    "                # grab each cell of table, except the first one\n",
    "                # that's the primary key name\n",
    "                schemas[table_name]['primary_keys'] = [td.text.strip() for td in h3.find_next('table').find_all('td')[1:]]\n",
    "            elif h3_text == \"Index Columns\":\n",
    "                # same again, but for an index\n",
    "                schemas[table_name]['indexes'].append([td.text.strip() for td in h3.find_next('table').find_all('td')[1:]])\n",
    "            elif h3_text == \"Content\":\n",
    "                # the list of all column names\n",
    "                for row in h3.find_next('table').find_all('tr')[1:]:\n",
    "                    cells = row.find_all('td')\n",
    "                    column_name = cells[0].text.strip()\n",
    "                    column_type = cells[1].text.strip()\n",
    "                    schemas[table_name]['columns'][column_name] = column_type\n",
    "            elif h3_text == \"Description\":\n",
    "                # ignore\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"Unknown h3: {h3.text.strip()} on {url}\")\n",
    "        \n",
    "        # sometimes there's so many columns that there is a second page, just of the \"Content\" table\n",
    "        # example: https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_128_1.htm\n",
    "        for a in soup.find_all('a'):\n",
    "            if a.get('href') and re.match(r\"MMS_(\\d+)_(\\d+).htm\", a['href']):\n",
    "                try:\n",
    "                    int(a.text.strip())\n",
    "                except ValueError:\n",
    "                    # this is not a subsequent page, it's another link\n",
    "                    # e.g. to the list of tables\n",
    "                    # https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_358_1.htm\n",
    "                    continue\n",
    "                else:\n",
    "                    url_prefix, slash, url_end = url.rpartition('/')\n",
    "                    new_url = url_prefix + '/' + a['href']\n",
    "                    r = session.get(new_url)\n",
    "                    r.raise_for_status()\n",
    "                    html = r.text\n",
    "                    soup = BeautifulSoup(html)\n",
    "                    h3 = soup.find('h3')\n",
    "                    assert h3.text.strip() == 'Content', f\"Unexpected first h3 in second page: {new_url} after {url}\"\n",
    "                    table = h3.find_next('table')\n",
    "                    for row in table.find_all('tr')[1:]:\n",
    "                        cells = row.find_all('td')\n",
    "                        column_name = cells[0].text.strip()\n",
    "                        column_type = cells[1].text.strip()\n",
    "                        schemas[table_name]['columns'][column_name] = column_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09453f3a-e462-4f17-948c-2ee407ba20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89616c72-5ecd-4868-a31e-239265afb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually add a schema for CO2EII_AVAILABLE_GENERATORS\n",
    "# This table doesn't appear in the nemweb schema documentation\n",
    "# and we probably need it\n",
    "# Guess the types from columns in other tables with the same name\n",
    "co2eii_cols = [\n",
    "    'STATIONNAME',\n",
    "    'DUID',\n",
    "    'GENSETID',\n",
    "    'REGIONID',\n",
    "    'CO2E_EMISSIONS_FACTOR',\n",
    "    'CO2E_ENERGY_SOURCE',\n",
    "    'CO2E_DATA_SOURCE',\n",
    "]\n",
    "\n",
    "schemas['CO2EII_AVAILABLE_GENERATORS'] = {\n",
    "    'columns': {},\n",
    "    'primary_keys': ['DUID'],\n",
    "    'indexes': [] # none\n",
    "}\n",
    "for c in co2eii_cols:\n",
    "    for (other_table, other_schema) in schemas.items():\n",
    "        if c in other_schema['columns']:\n",
    "            schemas['CO2EII_AVAILABLE_GENERATORS']['columns'][c] = other_schema['columns'][c]\n",
    "            \n",
    "    if c not in schemas['CO2EII_AVAILABLE_GENERATORS']['columns']:\n",
    "        raise ValueError(f\"Missing {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0579aff-7cf7-4717-8917-3e77ec45fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/schemas.json', 'w') as f:\n",
    "    json.dump(schemas, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b588373-db9d-4f30-82f9-782c0281447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/schemas.json', 'r') as f:\n",
    "    schemas = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f6505-ac26-4d4e-b16f-e37bbe030639",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
