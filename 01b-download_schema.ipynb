{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1200724-6ec3-4953-83c1-483647f24956",
   "metadata": {},
   "source": [
    "# webscrape AEMO schemas\n",
    "\n",
    "This playbook downloads [these web pages](https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_504_9.htm) to detect the list of columns, column types, and index columns for each table in AEMO's dataset.\n",
    "\n",
    "Each table is documented on a page between these URLs:\n",
    "\n",
    "* https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_001.htm#1\n",
    "* https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_503.htm#1\n",
    "\n",
    "So we do a for loop to grab each one. (Ignoring pages which are other kinds of documentation.)\n",
    "\n",
    "We want the list of \"Primary Key Columns\", \"Index Columns\" (there may be multiple sets of these). And we want \"Content\", with columns \"Name\" and \"Data Type\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4de3d-bf67-45b3-855b-def3e2dadf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import requests # HTTP downloads\n",
    "from bs4 import BeautifulSoup # webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a05b8d-536c-414f-868e-ed5fb582d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = '/media/matthew/nemweb/AppliedEconometrics/data'\n",
    "\n",
    "output_path = os.path.join(base_data_dir, '01-aemo-schemas.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c0922-dea5-4b9b-b722-3906db80ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session object to re-use between requests\n",
    "# to hopefully speed up downloads by not re-doing the TLS handshake for each HTTP request\n",
    "# (unsure if this actually speeds things up)\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b21ef8-c0b4-47e0-841c-b015faaaa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {}\n",
    "for i in tqdm(range(1, 503+1)):\n",
    "    url = f\"https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_{i}.htm#1\"\n",
    "    r = session.get(url)\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "    \n",
    "    # it's called \"soup\" because the python webscraping library is called \"beautiful soup\"\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    h2 = soup.find('h2')\n",
    "    if h2 is None:\n",
    "        # not a table schema page, this is another page\n",
    "        # ignore it\n",
    "        # e.g. https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_2.htm#1\n",
    "        continue\n",
    "    h2_text = h2.text.strip()\n",
    "    if not h2_text.startswith(\"Table: \"):\n",
    "        if not h2_text.strip().startswith('Package') and not h2_text.strip().startswith('Diagram'):\n",
    "            print(f\"Ignoring page with h2: {h2_text}: {url}\")\n",
    "    else:\n",
    "        # watch out, sometimes the documentation has an incorrect space inside the table name\n",
    "        # e.g. SET_ APC_COMPENSATION instead of SET_APC_COMPENSATION\n",
    "        table_name = re.match(r\"Table: (.*)\", h2_text).group(1).replace(' ','')\n",
    "        \n",
    "        schemas[table_name] = {\n",
    "            'indexes': [],\n",
    "            'columns': {}\n",
    "        }\n",
    "        for (i, h3) in enumerate(soup.find_all('h3')):\n",
    "            h3_name = h3.find('a').get('name').strip()\n",
    "            h3_text = h3.text.strip()\n",
    "            if i == 0:\n",
    "                assert table_name == h3.text.strip().replace(' ',''), f\"Table name from top h2 was {table_name}, from first h3 was {h3.text.strip()} in {url}\"\n",
    "                table = h3.find_next('table')\n",
    "                if table is None:\n",
    "                    print(f\"No first table on {url}\")\n",
    "                schemas[table_name]['description'] = table.find_all('p')[-1].text.strip()\n",
    "                \n",
    "            elif h3_text == \"Primary Key Columns\":\n",
    "                # the next element is a table.\n",
    "                # grab each cell of table, except the first one\n",
    "                # that's the primary key name\n",
    "                schemas[table_name]['primary_keys'] = [td.text.strip() for td in h3.find_next('table').find_all('td')[1:]]\n",
    "            elif h3_text == \"Index Columns\":\n",
    "                # same again, but for an index\n",
    "                schemas[table_name]['indexes'].append([td.text.strip() for td in h3.find_next('table').find_all('td')[1:]])\n",
    "            elif h3_text == \"Content\":\n",
    "                # the list of all column names\n",
    "                for row in h3.find_next('table').find_all('tr')[1:]:\n",
    "                    cells = row.find_all('td')\n",
    "                    column_name = cells[0].text.strip()\n",
    "                    column_type = cells[1].text.strip()\n",
    "                    column_desc = cells[-1].text.strip()\n",
    "                    schemas[table_name]['columns'][column_name] = {\n",
    "                        'AEMO_type': column_type,\n",
    "                        'comment': column_desc\n",
    "                    }\n",
    "            elif h3_text == \"Description\":\n",
    "                # ignore\n",
    "                pass\n",
    "            else:\n",
    "                print(f\"Unknown h3: {h3.text.strip()} on {url}\")\n",
    "        \n",
    "        # sometimes there's so many columns that there is a second page, just of the \"Content\" table\n",
    "        # example: https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_128_1.htm\n",
    "        for a in soup.find_all('a'):\n",
    "            if a.get('href') and re.match(r\"MMS_(\\d+)_(\\d+).htm\", a['href']):\n",
    "                try:\n",
    "                    int(a.text.strip())\n",
    "                except ValueError:\n",
    "                    # this is not a subsequent page, it's another link\n",
    "                    # e.g. to the list of tables\n",
    "                    # https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report_files/MMS_358_1.htm\n",
    "                    continue\n",
    "                else:\n",
    "                    url_prefix, slash, url_end = url.rpartition('/')\n",
    "                    new_url = url_prefix + '/' + a['href']\n",
    "                    r = session.get(new_url)\n",
    "                    r.raise_for_status()\n",
    "                    html = r.text\n",
    "                    soup = BeautifulSoup(html)\n",
    "                    h3 = soup.find('h3')\n",
    "                    assert h3.text.strip() == 'Content', f\"Unexpected first h3 in second page: {new_url} after {url}\"\n",
    "                    table = h3.find_next('table')\n",
    "                    for row in table.find_all('tr')[1:]:\n",
    "                        cells = row.find_all('td')\n",
    "                        try:\n",
    "                            column_name = cells[0].text.strip()\n",
    "                            column_type = cells[1].text.strip()\n",
    "                            column_desc = cells[-1].text.strip()\n",
    "                        except IndexError:\n",
    "                            print(f\"Error with {url}, {row}\")\n",
    "                            raise\n",
    "                        schemas[table_name]['columns'][column_name] = {\n",
    "                            'AEMO_type': column_type,\n",
    "                            'comment': column_desc\n",
    "                        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09453f3a-e462-4f17-948c-2ee407ba20f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89616c72-5ecd-4868-a31e-239265afb852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually add a schema for CO2EII_AVAILABLE_GENERATORS\n",
    "# This table doesn't appear in the nemweb schema documentation\n",
    "# and we might use it\n",
    "# Guess the types from columns in other tables with the same name\n",
    "co2eii_cols = [\n",
    "    'STATIONNAME',\n",
    "    'DUID',\n",
    "    'GENSETID',\n",
    "    'REGIONID',\n",
    "    'CO2E_EMISSIONS_FACTOR',\n",
    "    'CO2E_ENERGY_SOURCE',\n",
    "    'CO2E_DATA_SOURCE',\n",
    "]\n",
    "\n",
    "schemas['CO2EII_AVAILABLE_GENERATORS'] = {\n",
    "    'columns': {},\n",
    "    'primary_keys': ['DUID'],\n",
    "    'indexes': [] # none\n",
    "}\n",
    "for c in co2eii_cols:\n",
    "    for (other_table, other_schema) in schemas.items():\n",
    "        if c in other_schema['columns']:\n",
    "            schemas['CO2EII_AVAILABLE_GENERATORS']['columns'][c] = other_schema['columns'][c]\n",
    "            \n",
    "    if c not in schemas['CO2EII_AVAILABLE_GENERATORS']['columns']:\n",
    "        raise ValueError(f\"Missing {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf566fb-d6ce-4b0f-809f-bb3b847a320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are mistakes in the schema\n",
    "\n",
    "# change int to float\n",
    "# the actual digits don't matter. Later on we treat all `NUMBER(x,y)` the same\n",
    "schemas['DUDETAIL']['columns']['REGISTEREDCAPACITY']['AEMO_type'] = 'NUMBER(3,2)'\n",
    "schemas['DUDETAIL']['columns']['MAXCAPACITY']['AEMO_type'] = 'NUMBER(3,2)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8fa7c-b062-4496-9a6f-4877ff17d159",
   "metadata": {},
   "source": [
    "For the largest tables, which are each larger than the smallest 200 tables combined, let's discard the columns we know we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84f42a-be08-4ece-9e27-055b51fdb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_tables = ['DISPATCHLOAD', 'DISPATCHREGIONSUM']\n",
    "for (table, schema) in schemas.items():\n",
    "    if table in large_tables:\n",
    "        to_drop = []\n",
    "        cols = list(schema['columns'].keys())\n",
    "        for c in cols:\n",
    "            if any(c.upper().startswith(prefix) for prefix in ['RAISE', 'LOWER', 'VIOLATION', 'MARGINAL']) \\\n",
    "               and c not in schema['primary_keys']:\n",
    "                to_drop.append(c)\n",
    "        if to_drop:\n",
    "            schema['columns_to_drop'] = to_drop\n",
    "            print(f\"Deleting columns {to_drop} from {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0579aff-7cf7-4717-8917-3e77ec45fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, 'w') as f:\n",
    "    json.dump(schemas, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d109e-119d-4ce0-b973-f14e03dea8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
