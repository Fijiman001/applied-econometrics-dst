{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfc77c51-01c0-4027-9fb9-c46b76caae3a",
   "metadata": {},
   "source": [
    "# Webscrape AEMO data from nemweb\n",
    "\n",
    "This script downloads data files from https://www.nemweb.com.au/REPORTS/ and https://www.nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/ .\n",
    "\n",
    "1. Crawl those pages to find a list of URLs for CSV and .zip files\n",
    "2. Filter that list. e.g. delete large files we don't need\n",
    "3. Download those files. (Suprisingly hard. The server is unreliable. Throttles us, gives corrupt files etc)\n",
    "\n",
    "Note that this playbook takes about 1 day to run, and produces about 140GB of data, even though the dataset we care about is only a few hundred MB. This is because AEMO mixes up the data we want with lots of data we don't want. It's quite difficult to map their SQL 'tables' to the original files on their website. For most tables we can guess which source files have the data. But for some (in particular, CO2 emissions intensity per generator) I can't find which particular file has that data. So we download them all, process them, then choose just the ones we want (we choose in the next script, not this playbook).  (Actually, we don't download all. We skip files we definitely know don't have the data we want, e.g. gas market data. But that still leaves a lot we're unsure about.)\n",
    "\n",
    "The code may seem more complex than you expected. That's because :\n",
    "\n",
    "* I want to do multiprocessing to speed it up.\n",
    "* our downloads get throttled, so we need to sleep and retry\n",
    "* The whole thing takes so long that we want to be able to stop the kernel and restart, without the state ending up broken (e.g. writing the same data 1.5 times to and output file.)\n",
    "* Sometimes you can download 1000 files, and then file 1001 fails. I want to keep processing the remainder, gather up all the success/failures, and then compare them. So there's some try/catch stuff to handle that.\n",
    " \n",
    "When faced with a decision between making this code fast vs understandable, I tried to focus on understandable. (Except for multiprocessing)\n",
    "\n",
    "To run this on your laptop, you'll want to change `base_data_dir` to point to a directory with >140GB of space. It does not have to be inside this git repo.\n",
    "\n",
    "If you have been given a copy of files already downloaded, you can change `base_data_dir` such that `raw_files_path` points to the directory of already-downloaded files. Then this script will avoid re-downloading those files, and will only download new/missing/corrupt files. If you are marking this script based on whether it 'just runs', grab the hard drive with 140GB of already-downloaded stuff, and point the folder variables at that hard drive, then run. Alternatively, if you want to assess that this code 'just runs', set `max_files_per_page=2` to only download a small subset of files.\n",
    "\n",
    "There are many logs generated while running, to see what's happening. That's in `log_file`. (If this script printed them, it would be overwhelming and hard to scroll.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609abb78-d83d-4ed5-abc1-364c0252ebbd",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "If you don't have these libraries installed, run `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a050fd1-0264-4b95-a491-64c4368d70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import urllib3\n",
    "import shutil\n",
    "from urllib.parse import  urljoin, urlparse\n",
    "from time import sleep, time\n",
    "from random import randrange, shuffle\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "# type annotations\n",
    "# they don't do anything in python. It's just a concise way of documenting function input/output\n",
    "from typing import Set, List, Dict, Tuple, Optional\n",
    "\n",
    "from tqdm import tqdm # progress bar animation\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # webscraping\n",
    "\n",
    "# utils is our local utility module\n",
    "# if we change utils.py, and re-run a normal 'import'\n",
    "# python won't reload it by default. (Since it's already loaded.)\n",
    "# So we force a reload\n",
    "import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46927f8e-2e66-4f42-bd02-dbc157da6846",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069dcb9-ff87-4da2-a748-0dce757882aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = '/media/matthew/nemweb/AppliedEconometrics/data'\n",
    "\n",
    "# the list of URLs and file sizes will be saved here\n",
    "# relative to the repo (because it's small and we want to save this.)\n",
    "urls_file_path = os.path.join(base_data_dir, '01-urls.json')\n",
    "\n",
    "# the downlaoded files will be saved here\n",
    "raw_files_path = os.path.join(base_data_dir, '01-A-raw/')\n",
    "\n",
    "# logs will be written here\n",
    "# on linux, view this live with `tail -f logs.txt`\n",
    "log_file = os.path.join(base_data_dir, 'logs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73b070-1c9a-48ef-a8e1-3b63afcd5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to an int >0 to download only a representative sample of files\n",
    "# useful for debugging\n",
    "max_files_per_page = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51f76b-9013-4c91-a6b6-7a1b29a05b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscrape recursively from each of these pages\n",
    "start_urls = [\n",
    "    'https://www.nemweb.com.au/REPORTS/CURRENT/', # last year\n",
    "    'https://www.nemweb.com.au/REPORTS/ARCHIVE/', # last year\n",
    "    #'https://www.nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/2023/MMSDM_2023_10/', # one particular month\n",
    "    'https://nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/' # 1-10 years old, in a different folder and zip structure\n",
    "]\n",
    "# ignore files other than these\n",
    "# (case insensitive)\n",
    "file_suffixes = ['.zip', '.csv']\n",
    "\n",
    "# security check - don't follow links to other domains\n",
    "expected_domains = ('www.nemweb.com.au', 'nemweb.com.au')\n",
    "\n",
    "# ignore any URL containing one of these\n",
    "# because we definitely don't need them.\n",
    "# Skipping them will save time and disk space.\n",
    "# Most of these are about gas instead of electricity,\n",
    "# or bidding (financial stuff, not relevant for our research).\n",
    "url_substrings_to_skip = [\n",
    "    '/NEMDE/', \n",
    "    '/Gas_Supply_Guarantee/',\n",
    "    '/VicGas/',\n",
    "    '_GAS_'\n",
    "    '/DWGM/',\n",
    "    'Predispatch', # predictions (very large)\n",
    "    'FCAS', # unrelated to our purposes, and quite large\n",
    "    'BIDPEROFFER', # large, unrelated\n",
    "    'BIDDAYOFFER', # large, unrelated\n",
    "    'BIDOFFER',\n",
    "    'BID_OFFER',\n",
    "    'BIDMOVE',\n",
    "    'BIDTYPES',\n",
    "    'BIDDUIDDETAILS',\n",
    "    'RESIDUE_PRICE_FUNDS_BID',\n",
    "    'DISPATCHCONSTRAINT',\n",
    "    'DAYOFFER',\n",
    "    'MTPASA_OFFER',\n",
    "    'MTPASA',\n",
    "    'PDPASA',\n",
    "    'STPASA',\n",
    "    'PRUDENTIAL',\n",
    "    'MMSDM_CLI_', # not data\n",
    "    'MMSDM_GUI_', # not data\n",
    "    'CONSTRAINTSOLUTION', # detailed electrical constraints\n",
    "]\n",
    "\n",
    "\n",
    "# process files with multiprocessing, to be faster.\n",
    "# If set to False, will use a for loop, which gives clearer traceback error messages.\n",
    "use_multiprocessing = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff380b0-3576-4410-9bc6-c2e82be6876b",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7514c1-3dd4-4ab4-8a1e-b6f9e37abce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decrease the priority of this script given by the OS\n",
    "# so you can keep using your laptop for other stuff\n",
    "utils.renice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951c93a-96a7-4731-9ec2-425421529495",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = utils.Logger(log_file)\n",
    "logger.info(\"Initialising Logger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68597489-e528-4079-8047-69417287cae7",
   "metadata": {},
   "source": [
    "## Discover which files to download\n",
    "\n",
    "We don't actually download the .zip or .csv files yet.\n",
    "We just get a list of the URLs of the .zip and .csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba34003-7899-4447-a367-7a1644f2aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session object to re-use between requests\n",
    "# to hopefully speed up downloads by not re-doing the TLS handshake for each HTTP request\n",
    "# (unsure if this actually speeds things up)\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512b2e5-76cd-4569-91eb-947cd7d15d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "progress_bar = tqdm(leave=False)\n",
    "\n",
    "# nemweb sometimes redirects from https to http if you forget a traling slash\n",
    "# undo that.\n",
    "def force_https(url):\n",
    "    p = urlparse(url)\n",
    "    if p.scheme != 'https':\n",
    "        assert url.count('http://') == 1, f\"Strange URL: {url}\" # abort, for security reasons\n",
    "        url = url.replace('http://', 'https://')\n",
    "    return url\n",
    "\n",
    "def is_child_of(parent, child):\n",
    "    p = urlparse(parent)\n",
    "    c = urlparse(child)\n",
    "    return (p.hostname in expected_domains) and (c.hostname in expected_domains) \\\n",
    "        and c.path.rstrip('/').startswith(p.path.rstrip('/')) \\\n",
    "        and c.path.rstrip('/') != p.path.rstrip('/')\n",
    "\n",
    "# unit tests\n",
    "assert is_child_of('https://www.nemweb.com.au/REPORTS/', 'https://www.nemweb.com.au/REPORTS/CURRENT/')\n",
    "assert not is_child_of('https://www.nemweb.com.au/REPORTS/CURRENT', 'https://www.nemweb.com.au/REPORTS/')\n",
    "assert not is_child_of('https://www.nemweb.com.au/REPORTS/CURRENT/', 'https://www.nemweb.com.au/REPORTS/CURRENT/')\n",
    "assert not is_child_of('https://www.nemweb.com.au/REPORTS/CURRENT/', 'https://www.nemweb.com.au/REPORTS/ARCHIVE/')\n",
    "\n",
    "\n",
    "# returns a tuple of (url, size)\n",
    "# where url is a string\n",
    "# size is number of bytes, as an int. But None if unknown\n",
    "def extract_links(url) -> List[Tuple[str, Optional[int]]]:\n",
    "    logger.info(f\"Checking for links in {url}\")\n",
    "    if any(ss in url for ss in url_substrings_to_skip):\n",
    "        # skip this. Don't bother looking inside it.\n",
    "        logger.info(f\"Skipping {url} because of substrings\")\n",
    "        return []\n",
    "    # this is a web page listing other files or pages of other files\n",
    "    logger.info(f\"Downloading {url}\")\n",
    "    r = session.get(url)\n",
    "    if r.status_code >= 300:\n",
    "        sleep(5)\n",
    "        r = session.get(url)\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "\n",
    "    # it's called \"soup\" because the python webscraping library is called \"beautiful soup\"\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    links = []\n",
    "    files_this_page = 0\n",
    "    for a in soup.find_all('a'):\n",
    "        u = a['href']\n",
    "\n",
    "        # convert potentially relative URLs to absolute\n",
    "        u = urljoin(start_url, u)\n",
    "\n",
    "        # ignore links that aren't files on this page or subpages\n",
    "        # watch out, the domain can sometimes change (www. removed or added)\n",
    "        if not is_child_of(parent=url, child=u):\n",
    "            continue # skip the rest of this for loop iteration, go to the next one\n",
    "\n",
    "        elif u.endswith('/'):\n",
    "            # this is not a link to a file. This is a link to another page of files.\n",
    "\n",
    "            # call this function recursively\n",
    "            links_of_children = extract_links(u)\n",
    "            links.extend(links_of_children)\n",
    "\n",
    "        else:\n",
    "            # this is a link to a file\n",
    "\n",
    "            if not any(u.lower().endswith(ext.lower()) for ext in file_suffixes):\n",
    "                logger.debug(f\"Ignoring {u} because of file extension\")\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # iognore any links which contain the blacklisted substrings\n",
    "            if any(ss in u for ss in url_substrings_to_skip):\n",
    "                for ss in url_substrings_to_skip:\n",
    "                    logger.debug(f\"Ignoring {u} because of blacklisted substring {ss}\")\n",
    "                # watch out, we want to 'continue' the outer for loop, not the inner one, which is only for debugging\n",
    "                continue\n",
    "            \n",
    "            # Each link should have a string that precedes URL itself\n",
    "            # e.g. Tuesday, January 2, 2024  8:00 AM       168243\n",
    "            # let's get this size\n",
    "            # in this example, 168243\n",
    "            # use None if we can't get the size successfully\n",
    "            text = a.previous_sibling\n",
    "            if not isinstance(text, str):\n",
    "                logger.error(f\"No suitable text preceding URL. Previous sibling not text, is {a.previous_sibling} of type {type(text)}, for url {u} found on page {url}\")\n",
    "                size = None\n",
    "            else:\n",
    "                text = text.strip()\n",
    "                match = re.match(r\"\\w+, \\w+ \\d{1,2}, \\d{4} +\\d{1,2}:\\d{1,2} [AP]M\\s+(\\d+)\", text)\n",
    "                if match:\n",
    "                    size = match.group(1)\n",
    "                    try:\n",
    "                        size = int(size)\n",
    "                    except ValueError:\n",
    "                        logger.error(f\"Unable to extract file size from text: {text}, for url {u} found on page {url}\")\n",
    "                        size = None\n",
    "                else:\n",
    "                    size = None\n",
    "\n",
    "            # only a few files per page\n",
    "            # except MMSDM pages, which have one file of each type\n",
    "            if max_files_per_page and (max_files_per_page > 0) and ('/MMSDM/' not in url) and (files_this_page >= max_files_per_page):\n",
    "                logger.info(f\"Ignoring {u} because of max_files_per_page\")\n",
    "            else:\n",
    "                links.append((u, size))\n",
    "                files_this_page += 1\n",
    "\n",
    "    progress_bar.update()\n",
    "    logger.info(f\"Found {len(links)} links at/under {url}\")\n",
    "    return links\n",
    "\n",
    "urls = []\n",
    "for start_url in start_urls:\n",
    "    urls.extend(extract_links(start_url))\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c4cda-8178-42b3-a1f3-899bbe81331b",
   "metadata": {},
   "source": [
    "Now we filter the list of URLs, to discard the ones we don't care about. (Large ones, gas ones etc)\n",
    "\n",
    "Just delete URLs with string from `url_substrings_to_skip` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770f9ae-e6b0-423c-8f71-9bfeaa5d3baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the older files, each year is available as one big file, or lots of little files\n",
    "# e.g. https://nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/2017/\n",
    "# We don't want to download both\n",
    "# drop the few large files, keep the little ones\n",
    "urls = [(u, size) for (u, size) in  urls if not re.search(r\"MMSDM_\\d{4}_\\d{2}.zip\", u)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a34aa5-78d3-439d-a245-69d45f162e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should have been filtered already, but just in case.\n",
    "# (Usefull if you want to add a substring to skip without re-crawling nemweb)\n",
    "urls = [(u, size) for (u, size) in urls if not any(s.lower() in u.lower() for s in url_substrings_to_skip)]\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bacd0-4d98-49f1-9955-e3328e49c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there are no duplicate files in different locations on the website\n",
    "\n",
    "# deduplicate based on everything after the last slash\n",
    "urls = list({u.split('/')[-1]:(u, sz) for (u, sz) in urls}.values())\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b5da12-df89-47f6-92c2-43840569602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in descending order of size before writing\n",
    "# it makes it easier to analyse whether there's any large files we can skip\n",
    "# later on we'll shuffle the list again\n",
    "urls.sort(key=lambda u: u[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d1943-9db3-4910-b8ec-d6a3972c8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Writing {len(urls)} urls to {urls_file_path}\")\n",
    "\n",
    "with open(urls_file_path, 'w') as f:\n",
    "    json.dump(urls, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4189d3-318a-4675-8bb4-21b2d8876532",
   "metadata": {},
   "source": [
    "## Download the files\n",
    "\n",
    "\n",
    "Since we're downloading a lot of files, from the other side of the world, over slow wifi, we can get an error.\n",
    "We don't want to redownload all the files when one has an error. So let's just carry on, then retry the whole lot.\n",
    "When retrying, this code checks if the file already exists on disk. If not, dowload it.  If yes, it might be an incomplete download. To check, we compare the expected size of the file (obtained earlier) to the size of the file on disk. If it's the same, do not re-download the file.\n",
    "\n",
    "We get throttled a lot, which appears as 403 errors. Sleeping and retrying fixes that. \n",
    "\n",
    "This takes hours to run. You can stop the playbook, and then just re-run from here onwards. The code will not re-download files it already has. You can even restart the kernel (e.g. if you shut off your laptop) and re-run. (But then you'll have to re-run the imports up the top of the playbook, and the subsequent constant definitions. But you don't need to rerun the URL detection section. The next cell gets the saved list of URLs from `urls.txt`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff78907-9261-4e97-a262-8b8ec7472991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to skip re-indexing, run the playbook from here\n",
    "with open(urls_file_path, 'r') as f:\n",
    "    urls = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07df976-e521-4714-9021-3dde56d24720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create raw folder if it doesn't exist\n",
    "utils.create_dir(raw_files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be123fa4-41e7-411e-b640-c9b1c63be16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle, so that large files aren't clumped together.\n",
    "# this makes the progress bar estimate more accurate (compared to downloading all the big files last/first)\n",
    "# and if we don't download all the small files consecutively, we're less likely to be throttled\n",
    "shuffle(urls) # mutates list in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc8905-b8b1-4291-8baa-85d6e433eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using urllib3 instead of requests\n",
    "# because it's better for streaming large files to disk\n",
    "# https://stackoverflow.com/a/62075390/5443120\n",
    "http = urllib3.PoolManager(retries=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b80bd4-5322-42a7-a111-8ac44d4f1d35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform an HTTP HEAD request to ask the server for the size of the file\n",
    "# without downloading the whole thing\n",
    "def get_remote_size(url, retries=6):\n",
    "    r = http.request('HEAD', url)\n",
    "    if r.status >= 300:\n",
    "        if retries > 0:\n",
    "            logger.warning(f\"Retrying after bad status ({r.status}) for HEAD {url}\")\n",
    "            sleep(randrange(5))\n",
    "            return get_remote_size(url, retries=retries-1)\n",
    "        else:\n",
    "            logger.error(f\"Not retrying after bad status ({r.status}) for HEAD {url}\")\n",
    "            raise ValueError(f\"bad status ({r.status}) for HEAD {url}\")\n",
    "    return int(r.headers['Content-Length'])\n",
    "\n",
    "# have we already downloaded this url?\n",
    "# don't just check based on a file with that name existing locally,\n",
    "# it may be an incomplete download.\n",
    "# So also check based on size.\n",
    "# expected_size is the number from the nemweb page, just prior to the URL itself\n",
    "# it may be None, if we couldn't extract it\n",
    "# We can send a quick HEAD HTTP request to check file size without downloading a file\n",
    "def already_downloaded(url, expected_size, local):\n",
    "    if not os.path.exists(local):\n",
    "        return False\n",
    "    else:\n",
    "        local_size = os.path.getsize(local)\n",
    "        if (expected_size is not None) and (local_size == expected_size):\n",
    "            return True\n",
    "        else:\n",
    "            return get_remote_size(url) == local_size\n",
    "\n",
    "# url_and_size is a tuple, like ('https://...', 123)\n",
    "# a bit awkward to combine it into one, but passing multiple arguments with multiprocessing is tricky\n",
    "def download(url_and_size, retries=6):\n",
    "    (url, size) = url_and_size\n",
    "    fname = url.split('/')[-1]\n",
    "    local_path = os.path.join(raw_files_path, fname)\n",
    "    try:\n",
    "        if already_downloaded(url, size, local_path):\n",
    "            logger.info(f\"Skipping already downloaded {url}\")\n",
    "            return 'skipped'\n",
    "        else:\n",
    "            r = http.request('GET', url, preload_content=False)\n",
    "            if (r.status == 404) and (\"nemweb.com.au/REPORTS/CURRENT\" in url):\n",
    "                # this whole playbook takes so long\n",
    "                # that some daily files get moved by AEMO into monthly files\n",
    "                # re-run the playbook to grab those new monthly files.\n",
    "                # But if this happens, it's probably for data that's only 1 week old\n",
    "                # which is outside the scope of our study.\n",
    "                # So continus on for now.\n",
    "                logger.warning(f\"File {url} not found. Probably daily file expired\")\n",
    "                return 'skipped'\n",
    "            elif r.status >= 300:\n",
    "                logger.warning(f\"Retrying after bad status ({r.status}) for GET {url}\")\n",
    "                sleep(randrange(5))\n",
    "                return download(url_and_size, retries=retries-1)\n",
    "            logger.info(f\"Downloading {url} to {local_path}\")\n",
    "            with open(local_path, 'wb') as f:\n",
    "                shutil.copyfileobj(r, f)\n",
    "            logger.info(f\"Downloaded {url} to {local_path}\")\n",
    "    except (urllib3.exceptions.HTTPError, ValueError) as ex:\n",
    "        # tidy up partial download\n",
    "        try:\n",
    "            os.remove(local_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "        if retries <= 0:\n",
    "            logger.error(f\"Not retrying after error with {url}: {ex}\")\n",
    "            return ex\n",
    "        else:\n",
    "            logger.warning(f\"Retrying after error with {url}: {ex}\")\n",
    "            sleep(randrange(3))\n",
    "            return download(url_and_size, retries=retries-1)\n",
    "    return 'ok'\n",
    "                \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # use multiprocessing to process the files concurrently\n",
    "    # tqdm for a progress bar\n",
    "    # list(tqdm(imap())) thing explained here: https://stackoverflow.com/a/41921948/5443120\n",
    "\n",
    "    # remember that urls is a list of tuples of url and size\n",
    "    if use_multiprocessing:\n",
    "        with Pool(utils.num_cpu()) as p:\n",
    "            statuses = list(tqdm(p.imap(download, urls), total=len(urls)))\n",
    "    else:\n",
    "        statuses = [download(url) for url in tqdm(urls)]\n",
    "\n",
    "    # if these are only HEAD 404s for a few files, that's ok. They've probably moved from /CURRENT to /ARCHIVE (under a different file name)\n",
    "    # Re-run the notebook from the start up to this point once more, and you'll get new files which contain that data\n",
    "    # to debug this, use the next cell\n",
    "    assert all(s in ['ok', 'skipped'] for s in statuses), \"some files failed to be downloaded. Re-run whole playbook once.\"\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10553088-f3da-4cf7-a192-b370e0695902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
