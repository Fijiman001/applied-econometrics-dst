\addcontentsline{toc}{subsection}{Appendix C: Data Wrangling Explanation}
\subsection*{Appendix C: Data Wrangling Explanation}

The dataset we downloaded is 300GB of compressed CSV files, totalling 1.4TB when uncompressed. Handling datasets larger than the size of your hard drive, with individual files larger than memory, is quite a technical challenge. As an example of the challenge, note that when processing this data with a new big-data library (Polars, `the new Pandas'), the tool threw segmentation fault errors, because the dataset is too large for this big-data tool. This is noteworthy because whilst Polars is used from Python, it is written in Rust, and therefore segmentation faults are theoretically impossible. (The bug has since been fixed, after we reported it \href{https://github.com/pola-rs/polars/issues/13915}{on GitHub}.)

The main dataset comes from AEMO. Their target audience are electricity industry participants (e.g. coal generators), who generally want to know everything about the market. So the data is somewhat mixed together. e.g. individual files telling you total energy (MWh) for a region also tell you the total price, and the marginal cost of electrical transmission constraints, and FCAS ancillary service charges and so on. We were able to identify about one third of files as being definitely not needed (e.g. ones about gas) prior to downloading them. Of the remainder we need to download them, unzip them, "split" them (e.g. split rows of energy data from price data within the same file), and only then can we figure out which of the final 300 ``tables" they belong to. At that point we can discard most data. 

Market participants use proprietary software from \ac{AEMO} to download the data, process it and write it into an Oracle database. This proprietary software is no longer publicly available.\footnote{The public binary was taken down when the log4j vulnerability (\texttt{CVE-2021-44832}) became known. The subsequent version from AEMO which was patched was not released publicly.} The infrastructure cost of running such a large database is in the region of â‚¬10,000 per year. Such transactional row-based databases are optimised for operational queries, not analytical queries. For these reasons a new pipeline was written for this project from scratch, to process the CSV files into parquet files.

\ac{AEMO}  publishes the files publicly on \href{https://www.nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/}{their website}. Downloading the files is surprisingly complicated due to throttling, corrupt files and other issues.

The raw files are zips of zips of CSV files. However the CSV files are not standard tabular files. They are a concatenation of tables into one file. This bespoke format is unique to AEMO. The documentation is no longer on their website, but can be found in \href{https://web.archive.org/web/20230414160249/https://aemo.com.au/-/media/files/market-it-systems/guide-to-csv-data-format-standard.pdf?la=en}{the Internet Archive}. After decompressing and filtering, what remains is a large number of small files. Due to the schema changing over time, files which are part of the same dataset do not have the same columns. Note that the python library Pandas cannot handle empty null values for integer cells. However more sophisticated tools such as Arrow cannot implicitly merge integers and floats into a single datatype.
Metadata about the dataset (the names and data types of each column, in each ``table") was webscraped from \href{https://nemweb.com.au/Reports/Current/MMSDataModelReport/Electricity/MMS%20Data%20Model%20Report.htm}{\ac{AEMO}'s online documentation}, to allow for explicit schema handling across the hundreds of tables. Two errors in the schema documentation were identified. \ac{AEMO} has not responded to our reports about the errors.

For each of the hundreds of ``tables'' in the AEMO dataset, a single parquet file is produced. Parquet is an alternative format to CSV. The main motivation for using it was as a technical solution to keep the dataset small and fast. (e.g. it allows us to use predicate pushdown in subsequent scripts.) However the largest of the files relevant to us is a 5GB parquet file. When loaded into memory (e.g. in R) this would take up about 20GB. None of our laptops have that much memory. The processing we want to do is to take 5-minute data per generator, multiply it by the constant CO2 emissions factor, sum within each region, aggregate to half hour intervals. Then it's small enough to join with some other data, e.g. to account for inter-region import-export. One challenge is that AEMO's files contain duplicate data. (e.g. they have 5-minute files, and daily summaries of those files, and monthly summaries of those, etc.) Deduplicating data generally requires loading the whole thing in memory. So this is a really hard big data task. What we do is use \href{https://arrow.apache.org/}{Apache Arrow} to lazy-load the parquet files, such that we can use filter and predicate pushdowns into the storage layer, along with physical repartitioning, to end up with something small. From then onwards normal R joins can be used to add additional datasets.

For data about \ac{DST} transitions itself, we need to know what days the clocks move. We also want some enriched data about this. e.g. for each calendar day, is the nearest clock change in the future, or past? How many days away? We did not download this data from anywhere. Python itself has a copy inside it, which it uses for timezone conversions of datetimes. We use that instead of downloading, because it's easier and less likely to have mistakes than manually downloading and combining many CSVs.

\addcontentsline{toc}{subsection}{Appendix D: Inter-region flow emission adjustment calculation}
\subsection*{Appendix D: Inter-region flow emission adjustment calculation}
\label{sec:interconnector calc}
In the data section, we explained that we might have inter-region flows between Queensland and the treated regions. We adjust for this by calculating adjusted emissions taking into account between-region import and export data. 

A substantial fraction (7\%) of energy in the \ac{NEM} is exported across region borders. For some interconnectors, the flows do not average to zero. As one of the data preparation steps mentioned in Section \ref{sec:data}, these flows are adjusted for. For example if Queensland generates 3 GW of power and 3000 tonnes of CO2e, and exports 1 GW to New-South-Wales, we subtract 1000 tonnes of CO2 from Queensland's emissions and add it to New South Wales' emissions. 

Note that whilst the generation data is available with 5 minute granularity, interconnector data is only available with a 30 minute granularity (for most of the time period studied). That is one reason why we downsample to 30 minute frequency.
A 30 minute frequency also aligns with the literature \parencite{kellogg_daylight_2008}, make the dataset set manageable, reduces the impact of serial correlation, and align to market trading periods.